{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9968fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78a654c6-f5ed-4da6-96e9-6a415baa0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MLP\n",
    "class Multi_Layer_Perceptron(nn.Sequential):\n",
    "    def __init__(self, input_dim, intern_dim, output_dim, depth = 2, isBiased = False):\n",
    "        \n",
    "        dict = OrderedDict([(\"input\",nn.Linear(input_dim,intern_dim, bias=isBiased))])\n",
    "        for i in range(depth):\n",
    "            dict.update({str(i) : nn.Linear(intern_dim,intern_dim,bias=isBiased)})\n",
    "        dict.update({\"output\" : nn.Linear(intern_dim,output_dim,bias=isBiased)})\n",
    "\n",
    "        super().__init__(dict)\n",
    "\n",
    "        self.reset_init_weights_biases(0) # so that we do not use a default initialization\n",
    "\n",
    "    def reset_init_weights_biases(self, norm = None):\n",
    "        for layer in self.children():\n",
    "            if norm == None:\n",
    "                stdv = 1. / math.sqrt(layer.weight.size(1))\n",
    "            else :\n",
    "                stdv = norm\n",
    "            \n",
    "            layer.weight.data.uniform_(-stdv, stdv)\n",
    "            if layer.bias is not None:\n",
    "                layer.bias.data.uniform_(-stdv, stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "388bf0ae-b403-4a5f-a87e-1431b33b1405",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CustomLoss DQN\n",
    "class DQN_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN_Loss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        loss = (target - input)**2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fdb52ffe-7290-4c94-8850-db3c480348f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def select_action(self, state):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, episodes, debug_mode=False):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, id, env):\n",
    "        self.id = id\n",
    "        self.env = env\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48ce199a-cc03-418a-bcf2-c205d954304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        pass\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        return random.randint(0,1)\n",
    "        \n",
    "        \n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, episodes, debug_mode=False):\n",
    "        episodesHistory = np.zeros((episodes))\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            if(debug_mode) : print(\"Episode: \"+str(i+1)+\" starts\")\n",
    "            newSeed = random.randint(0,100000)\n",
    "            state,_ = self.env.reset(seed = newSeed)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                                        \n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                self.observe(state,action,next_state,reward)\n",
    "                self.update()\n",
    "\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "\n",
    "            episodesHistory[i] = episode_reward\n",
    "            if(debug_mode) : print(\"Episode \"+str(i+1)+ \" , Reward: \"+str(episode_reward))\n",
    "        print(episodesHistory[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "228a8b98-8bed-4b33-9097-fa858886d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(Agent):\n",
    "\n",
    "    def __init__(self, id, epsilon, Q = Multi_Layer_Perceptron(input_dim = 3,intern_dim = 64, output_dim = 1, depth = 2, isBiased = True), env = gym.make('MountainCar-v0'), gamma = 0.99, replay_buffer_SIZE = 10000, batch_size = 64, observation_SIZE = 6, optimizer = torch.optim.AdamW):\n",
    "        Agent.__init__(self,id,env)\n",
    "        self.Q = Q\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.replay_buffer = np.zeros((replay_buffer_SIZE,observation_SIZE))\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer(self.Q.parameters())\n",
    "        self.replay_buffer_SIZE = replay_buffer_SIZE\n",
    "        self.observation_SIZE = observation_SIZE\n",
    "        \n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        state = np.array(state)\n",
    "        action = np.array([action])\n",
    "        next_state = np.array(next_state)\n",
    "        reward = np.array([reward])\n",
    "\n",
    "        concatenatation = np.concatenate((state, action, next_state, reward))\n",
    "        return concatenatation\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        return random.randint(0,1)\n",
    "        \n",
    "        \n",
    "    def update(self):\n",
    "\n",
    "        LossFct = DQN_Loss()\n",
    "        \n",
    "        batch = np.random.choice(self.replay_buffer.shape[0], self.batch_size)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "\n",
    "            A0 = torch.from_numpy(np.concatenate((self.replay_buffer[batch[i]][3:5], np.array([0])))).to(torch.float32)\n",
    "            A1 = torch.from_numpy(np.concatenate((self.replay_buffer[batch[i]][3:5], np.array([1])))).to(torch.float32)\n",
    "            target = self.replay_buffer[batch[i]][5] + self.gamma*max(self.Q(A0),self.Q(A1))\n",
    "            input = self.Q(torch.from_numpy(self.replay_buffer[batch[i]][:3]).to(torch.float32))\n",
    "\n",
    "            loss = LossFct(input, target)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            #print(loss.item())\n",
    "            self.optimizer.step()\n",
    "        \n",
    "\n",
    "    def train(self, episodes, debug_mode=False):\n",
    "        episodesHistory = np.zeros((episodes))\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            if debug_mode: print(\"Episode: \"+str(i+1)+\" starts\")\n",
    "            newSeed = random.randint(0,100000)\n",
    "            state,_ = self.env.reset(seed = newSeed)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            self.replay_buffer = np.zeros((self.replay_buffer_SIZE, self.observation_SIZE))\n",
    "            k=0\n",
    "            \n",
    "            while not done:\n",
    "                                        \n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                observe = self.observe(state,action,next_state,reward)\n",
    "                self.replay_buffer[k] = observe\n",
    "                k+=1\n",
    "\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "\n",
    "            if debug_mode: print(\"Episode \"+str(i+1)+ \" , Reward: \"+str(episode_reward))\n",
    "            self.update()\n",
    "            episodesHistory[i] = episode_reward\n",
    "        if debug_mode: print(episodesHistory[:])\n",
    "        return episodesHistory[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "59b87236-f076-4348-b990-d2b0d4999b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 starts\n",
      "Episode 1 , Reward: -200.0\n",
      "Episode: 2 starts\n",
      "Episode 2 , Reward: -200.0\n",
      "Episode: 3 starts\n",
      "Episode 3 , Reward: -200.0\n",
      "Episode: 4 starts\n",
      "Episode 4 , Reward: -200.0\n",
      "Episode: 5 starts\n",
      "Episode 5 , Reward: -200.0\n",
      "Episode: 6 starts\n",
      "Episode 6 , Reward: -200.0\n",
      "Episode: 7 starts\n",
      "Episode 7 , Reward: -200.0\n",
      "Episode: 8 starts\n",
      "Episode 8 , Reward: -200.0\n",
      "Episode: 9 starts\n",
      "Episode 9 , Reward: -200.0\n",
      "Episode: 10 starts\n",
      "Episode 10 , Reward: -200.0\n",
      "Episode: 11 starts\n",
      "Episode 11 , Reward: -200.0\n",
      "Episode: 12 starts\n",
      "Episode 12 , Reward: -200.0\n",
      "Episode: 13 starts\n",
      "Episode 13 , Reward: -200.0\n",
      "Episode: 14 starts\n",
      "Episode 14 , Reward: -200.0\n",
      "Episode: 15 starts\n",
      "Episode 15 , Reward: -200.0\n",
      "Episode: 16 starts\n",
      "Episode 16 , Reward: -200.0\n",
      "Episode: 17 starts\n",
      "Episode 17 , Reward: -200.0\n",
      "Episode: 18 starts\n",
      "Episode 18 , Reward: -200.0\n",
      "Episode: 19 starts\n",
      "Episode 19 , Reward: -200.0\n",
      "Episode: 20 starts\n",
      "Episode 20 , Reward: -200.0\n",
      "Episode: 21 starts\n",
      "Episode 21 , Reward: -200.0\n",
      "Episode: 22 starts\n",
      "Episode 22 , Reward: -200.0\n",
      "Episode: 23 starts\n",
      "Episode 23 , Reward: -200.0\n",
      "Episode: 24 starts\n",
      "Episode 24 , Reward: -200.0\n",
      "Episode: 25 starts\n",
      "Episode 25 , Reward: -200.0\n",
      "Episode: 26 starts\n",
      "Episode 26 , Reward: -200.0\n",
      "Episode: 27 starts\n",
      "Episode 27 , Reward: -200.0\n",
      "Episode: 28 starts\n",
      "Episode 28 , Reward: -200.0\n",
      "Episode: 29 starts\n",
      "Episode 29 , Reward: -200.0\n",
      "Episode: 30 starts\n",
      "Episode 30 , Reward: -200.0\n",
      "Episode: 31 starts\n",
      "Episode 31 , Reward: -200.0\n",
      "Episode: 32 starts\n",
      "Episode 32 , Reward: -200.0\n",
      "Episode: 33 starts\n",
      "Episode 33 , Reward: -200.0\n",
      "Episode: 34 starts\n",
      "Episode 34 , Reward: -200.0\n",
      "Episode: 35 starts\n",
      "Episode 35 , Reward: -200.0\n",
      "Episode: 36 starts\n",
      "Episode 36 , Reward: -200.0\n",
      "Episode: 37 starts\n",
      "Episode 37 , Reward: -200.0\n",
      "Episode: 38 starts\n",
      "Episode 38 , Reward: -200.0\n",
      "Episode: 39 starts\n",
      "Episode 39 , Reward: -200.0\n",
      "Episode: 40 starts\n",
      "Episode 40 , Reward: -200.0\n",
      "Episode: 41 starts\n",
      "Episode 41 , Reward: -200.0\n",
      "Episode: 42 starts\n",
      "Episode 42 , Reward: -200.0\n",
      "Episode: 43 starts\n",
      "Episode 43 , Reward: -200.0\n",
      "Episode: 44 starts\n",
      "Episode 44 , Reward: -200.0\n",
      "Episode: 45 starts\n",
      "Episode 45 , Reward: -200.0\n",
      "Episode: 46 starts\n",
      "Episode 46 , Reward: -200.0\n",
      "Episode: 47 starts\n",
      "Episode 47 , Reward: -200.0\n",
      "Episode: 48 starts\n",
      "Episode 48 , Reward: -200.0\n",
      "Episode: 49 starts\n",
      "Episode 49 , Reward: -200.0\n",
      "Episode: 50 starts\n",
      "Episode 50 , Reward: -200.0\n",
      "Episode: 51 starts\n",
      "Episode 51 , Reward: -200.0\n",
      "Episode: 52 starts\n",
      "Episode 52 , Reward: -200.0\n",
      "Episode: 53 starts\n",
      "Episode 53 , Reward: -200.0\n",
      "Episode: 54 starts\n",
      "Episode 54 , Reward: -200.0\n",
      "Episode: 55 starts\n",
      "Episode 55 , Reward: -200.0\n",
      "Episode: 56 starts\n",
      "Episode 56 , Reward: -200.0\n",
      "Episode: 57 starts\n",
      "Episode 57 , Reward: -200.0\n",
      "Episode: 58 starts\n",
      "Episode 58 , Reward: -200.0\n",
      "Episode: 59 starts\n",
      "Episode 59 , Reward: -200.0\n",
      "Episode: 60 starts\n",
      "Episode 60 , Reward: -200.0\n",
      "Episode: 61 starts\n",
      "Episode 61 , Reward: -200.0\n",
      "Episode: 62 starts\n",
      "Episode 62 , Reward: -200.0\n",
      "Episode: 63 starts\n",
      "Episode 63 , Reward: -200.0\n",
      "Episode: 64 starts\n",
      "Episode 64 , Reward: -200.0\n",
      "Episode: 65 starts\n",
      "Episode 65 , Reward: -200.0\n",
      "Episode: 66 starts\n",
      "Episode 66 , Reward: -200.0\n",
      "Episode: 67 starts\n",
      "Episode 67 , Reward: -200.0\n",
      "Episode: 68 starts\n",
      "Episode 68 , Reward: -200.0\n",
      "Episode: 69 starts\n",
      "Episode 69 , Reward: -200.0\n",
      "Episode: 70 starts\n",
      "Episode 70 , Reward: -200.0\n",
      "Episode: 71 starts\n",
      "Episode 71 , Reward: -200.0\n",
      "Episode: 72 starts\n",
      "Episode 72 , Reward: -200.0\n",
      "Episode: 73 starts\n",
      "Episode 73 , Reward: -200.0\n",
      "Episode: 74 starts\n",
      "Episode 74 , Reward: -200.0\n",
      "Episode: 75 starts\n",
      "Episode 75 , Reward: -200.0\n",
      "Episode: 76 starts\n",
      "Episode 76 , Reward: -200.0\n",
      "Episode: 77 starts\n",
      "Episode 77 , Reward: -200.0\n",
      "Episode: 78 starts\n",
      "Episode 78 , Reward: -200.0\n",
      "Episode: 79 starts\n",
      "Episode 79 , Reward: -200.0\n",
      "Episode: 80 starts\n",
      "Episode 80 , Reward: -200.0\n",
      "Episode: 81 starts\n",
      "Episode 81 , Reward: -200.0\n",
      "Episode: 82 starts\n",
      "Episode 82 , Reward: -200.0\n",
      "Episode: 83 starts\n",
      "Episode 83 , Reward: -200.0\n",
      "Episode: 84 starts\n",
      "Episode 84 , Reward: -200.0\n",
      "Episode: 85 starts\n",
      "Episode 85 , Reward: -200.0\n",
      "Episode: 86 starts\n",
      "Episode 86 , Reward: -200.0\n",
      "Episode: 87 starts\n",
      "Episode 87 , Reward: -200.0\n",
      "Episode: 88 starts\n",
      "Episode 88 , Reward: -200.0\n",
      "Episode: 89 starts\n",
      "Episode 89 , Reward: -200.0\n",
      "Episode: 90 starts\n",
      "Episode 90 , Reward: -200.0\n",
      "Episode: 91 starts\n",
      "Episode 91 , Reward: -200.0\n",
      "Episode: 92 starts\n",
      "Episode 92 , Reward: -200.0\n",
      "Episode: 93 starts\n",
      "Episode 93 , Reward: -200.0\n",
      "Episode: 94 starts\n",
      "Episode 94 , Reward: -200.0\n",
      "Episode: 95 starts\n",
      "Episode 95 , Reward: -200.0\n",
      "Episode: 96 starts\n",
      "Episode 96 , Reward: -200.0\n",
      "Episode: 97 starts\n",
      "Episode 97 , Reward: -200.0\n",
      "Episode: 98 starts\n",
      "Episode 98 , Reward: -200.0\n",
      "Episode: 99 starts\n",
      "Episode 99 , Reward: -200.0\n",
      "Episode: 100 starts\n",
      "Episode 100 , Reward: -200.0\n",
      "[-200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "       -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "       -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "       -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "       -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "       -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "       -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "       -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "       -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "       -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "       -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "       -200.])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DQN = DQNAgent(0,0.9)\n",
    "DQN.train(100, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a93ea22-7ebd-4ce1-bcd3-077dbe1a2991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
