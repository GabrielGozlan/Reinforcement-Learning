{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9968fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78a654c6-f5ed-4da6-96e9-6a415baa0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MLP\n",
    "class Multi_Layer_Perceptron(nn.Sequential):\n",
    "    def __init__(self, input_dim, intern_dim, output_dim, depth = 2, isBiased = False):\n",
    "        \n",
    "        dict = OrderedDict([(\"input\",nn.Linear(input_dim,intern_dim, bias=isBiased))])\n",
    "        for i in range(depth):\n",
    "            dict.update({str(i) : nn.Linear(intern_dim,intern_dim,bias=isBiased)})\n",
    "        dict.update({\"output\" : nn.Linear(intern_dim,output_dim,bias=isBiased)})\n",
    "\n",
    "        super().__init__(dict)\n",
    "\n",
    "        self.reset_init_weights_biases(0) # so that we do not use a default initialization\n",
    "\n",
    "    def reset_init_weights_biases(self, norm = None):\n",
    "        for layer in self.children():\n",
    "            if norm == None:\n",
    "                stdv = 1. / math.sqrt(layer.weight.size(1))\n",
    "            else :\n",
    "                stdv = norm\n",
    "            \n",
    "            layer.weight.data.uniform_(-stdv, stdv)\n",
    "            if layer.bias is not None:\n",
    "                layer.bias.data.uniform_(-stdv, stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "388bf0ae-b403-4a5f-a87e-1431b33b1405",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CustomLoss DQN\n",
    "class DQN_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN_Loss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        loss = (target - input)**2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdb52ffe-7290-4c94-8850-db3c480348f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def select_action(self, state):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, episodes, debug_mode=False):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, id, env):\n",
    "        self.id = id\n",
    "        self.env = env\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48ce199a-cc03-418a-bcf2-c205d954304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        pass\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        return random.randint(0,1)\n",
    "        \n",
    "        \n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, episodes, debug_mode=False):\n",
    "        episodesHistory = np.zeros((episodes))\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            if(debug_mode) : print(\"Episode: \"+str(i+1)+\" starts\")\n",
    "            newSeed = random.randint(0,100000)\n",
    "            state,_ = self.env.reset(seed = newSeed)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                                        \n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                self.observe(state,action,next_state,reward)\n",
    "                self.update()\n",
    "\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "\n",
    "            episodesHistory[i] = episode_reward\n",
    "            if(debug_mode) : print(\"Episode \"+str(i+1)+ \" , Reward: \"+str(episode_reward))\n",
    "        print(episodesHistory[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "228a8b98-8bed-4b33-9097-fa858886d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(Agent):\n",
    "\n",
    "    def __init__(self, id, epsilon, Q = Multi_Layer_Perceptron(input_dim = 3,intern_dim = 64, output_dim = 1, depth = 2, isBiased = True), env = gym.make('MountainCar-v0'), arrayNewReward = None, gamma = 0.99, replay_buffer_SIZE = 10000, batch_size = 64, observation_SIZE = 6, optimizer = torch.optim.AdamW):\n",
    "        Agent.__init__(self,id,env)\n",
    "        self.Q = Q\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.replay_buffer = np.zeros((replay_buffer_SIZE,observation_SIZE))\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer(self.Q.parameters())\n",
    "        self.replay_buffer_SIZE = replay_buffer_SIZE\n",
    "        self.observation_SIZE = observation_SIZE\n",
    "        self.arrayNewReward = arrayNewReward\n",
    "        \n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        state = np.array(state)\n",
    "        action = np.array([action])\n",
    "        next_state = np.array(next_state)\n",
    "        reward = np.array([reward])\n",
    "\n",
    "        concatenatation = np.concatenate((state, action, next_state, reward))\n",
    "        return concatenatation\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        P = random.uniform(0,1)\n",
    "        a=0\n",
    "        if P < 1-self.epsilon :\n",
    "            print(self.env.action_space.shape)\n",
    "            A=np.zeros(3)\n",
    "            print(A)\n",
    "            for k in range(A.shape[0]):\n",
    "                A[k] = self.Q(torch.from_numpy(np.concatenate((np.array(state), np.array([k])))).to(torch.float32))\n",
    "            a = np.argmax(A)\n",
    "        else:\n",
    "            a = random.randint(0,3)\n",
    "            \n",
    "        return a\n",
    "        \n",
    "        \n",
    "    def update(self):\n",
    "\n",
    "        LossFct = DQN_Loss()\n",
    "        \n",
    "        batch = np.random.choice(self.replay_buffer.shape[0], self.batch_size)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "\n",
    "            A0 = torch.from_numpy(np.concatenate((self.replay_buffer[batch[i]][3:5], np.array([0])))).to(torch.float32)\n",
    "            A1 = torch.from_numpy(np.concatenate((self.replay_buffer[batch[i]][3:5], np.array([1])))).to(torch.float32)\n",
    "            target = self.replay_buffer[batch[i]][5] + self.gamma*max(self.Q(A0),self.Q(A1))\n",
    "            input = self.Q(torch.from_numpy(self.replay_buffer[batch[i]][:3]).to(torch.float32))\n",
    "\n",
    "            loss = LossFct(input, target)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            #print(loss.item())\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def customReward(currentReward):\n",
    "        reward = currentReward\n",
    "        if self.arrayNewReward == None:\n",
    "            return reward\n",
    "            \n",
    "        for i in self.arrayNewReward[:][0]:\n",
    "            if i == next_state[0]:\n",
    "                reward = self.arrayNewReward[i][1]\n",
    "                break\n",
    "        return reward\n",
    "        \n",
    "    def train(self, episodes, debug_mode=False):\n",
    "        episodesHistory = np.zeros((episodes))\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            if debug_mode: print(\"Episode: \"+str(i+1)+\" starts\")\n",
    "            newSeed = random.randint(0,100000)\n",
    "            state,_ = self.env.reset(seed = newSeed)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            self.replay_buffer = np.zeros((self.replay_buffer_SIZE, self.observation_SIZE))\n",
    "            k=0\n",
    "            \n",
    "            while done == False:\n",
    "                                        \n",
    "                action = self.select_action(state)\n",
    "                if debug_mode: print(\"Action \"+str(k)+\" selected: \"+str(action))\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                reward = self.customReward(currentReward = reward)\n",
    "                observe = self.observe(state,action,next_state,reward)\n",
    "                self.replay_buffer[k] = observe\n",
    "                k+=1\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "\n",
    "            if debug_mode: print(\"Episode \"+str(i+1)+ \" , Reward: \"+str(episode_reward))\n",
    "            self.update()\n",
    "            episodesHistory[i] = episode_reward\n",
    "        return episodesHistory[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8b9b53e-568d-42f7-90a7-3003d3866ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, arrayNewReward, render = None, **kwargs):\n",
    "        super(CustomEnv, self).__init__(**kwargs)\n",
    "        self.basicEnv = gym.make('MountainCar-v0', render_mode=render)\n",
    "        self.arrayNewReward = arrayNewReward\n",
    "\n",
    "    def reset(self,**kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        for i in self.arrayNewReward[:][0]:\n",
    "            if i == next_state[0]:\n",
    "                reward = self.arrayNewReward[i][1]\n",
    "                break\n",
    "        return next_state, reward, terminated, truncated, info \n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59b87236-f076-4348-b990-d2b0d4999b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200. -200.\n",
      " -200. -200. -200. -200.]\n"
     ]
    }
   ],
   "source": [
    "DQN = DQNAgent(0,0.1, env= CustomEnv(arrayNewReward=np.array([(-0.3,10),(0.3,5)])))\n",
    "print(DQN.train(1000, debug_mode = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a93ea22-7ebd-4ce1-bcd3-077dbe1a2991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 starts\n",
      "()\n",
      "[0. 0. 0.]\n",
      "Action 0 selected: 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "customReward() got multiple values for argument 'currentReward'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m NewReward\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.3\u001b[39m,\u001b[38;5;241m10\u001b[39m),(\u001b[38;5;241m0.3\u001b[39m,\u001b[38;5;241m5\u001b[39m),(\u001b[38;5;241m0.4\u001b[39m,\u001b[38;5;241m20\u001b[39m),(\u001b[38;5;241m0.6\u001b[39m,\u001b[38;5;241m200\u001b[39m)])\n\u001b[0;32m      2\u001b[0m DQN \u001b[38;5;241m=\u001b[39m DQNAgent(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0.1\u001b[39m, arrayNewReward \u001b[38;5;241m=\u001b[39m NewReward)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mDQN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m DQN\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[34], line 88\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[1;34m(self, episodes, debug_mode)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m debug_mode: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(k)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m selected: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(action))\n\u001b[0;32m     87\u001b[0m next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m---> 88\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustomReward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrentReward\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m observe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobserve(state,action,next_state,reward)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer[k] \u001b[38;5;241m=\u001b[39m observe\n",
      "\u001b[1;31mTypeError\u001b[0m: customReward() got multiple values for argument 'currentReward'"
     ]
    }
   ],
   "source": [
    "NewReward=np.array([(-0.3,10),(0.3,5),(0.4,20),(0.6,200)])\n",
    "DQN = DQNAgent(0,0.1, arrayNewReward = NewReward)\n",
    "DQN.train(1, debug_mode = True)\n",
    "DQN.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4cd15d7-4c99-4ea6-8dc0-a6b57044168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3606f5-081e-461b-bdda-0684f0acaf37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
