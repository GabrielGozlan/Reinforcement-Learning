{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9968fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a654c6-f5ed-4da6-96e9-6a415baa0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MLP\n",
    "class Multi_Layer_Perceptron(nn.Sequential):\n",
    "    def __init__(self, input_dim, intern_dim, output_dim, depth = 2, isBiased = False):\n",
    "        \n",
    "        dict = OrderedDict([(\"input\",nn.Linear(input_dim,intern_dim, bias=isBiased))])\n",
    "        for i in range(depth):\n",
    "            dict.update({str(i) : nn.Linear(intern_dim,intern_dim,bias=isBiased)})\n",
    "        dict.update({\"output\" : nn.Linear(intern_dim,output_dim,bias=isBiased)})\n",
    "\n",
    "        super().__init__(dict)\n",
    "\n",
    "        self.reset_init_weights_biases(0) # so that we do not use a default initialization\n",
    "\n",
    "    def reset_init_weights_biases(self, norm = -1):\n",
    "        for layer in self.children():\n",
    "            stdv = 1. / math.sqrt(layer.weight.size(1))\n",
    "            if norm != -1 :\n",
    "                stdv = norm\n",
    "            layer.weight.data.uniform_(-stdv,stdv)\n",
    "            if layer.bias is not None:\n",
    "                layer.bias.data.uniform_(-stdv,stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb52ffe-7290-4c94-8850-db3c480348f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def select_action(self, state):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, episodes, debug_mode=False):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, id, env):\n",
    "        self.id = id\n",
    "        self.env = env\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ce199a-cc03-418a-bcf2-c205d954304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        pass\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        return random.randint(0,1)\n",
    "        \n",
    "        \n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, episodes, debug_mode=False):\n",
    "        episodesHistory = np.zeros((episodes))\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            if(debug_mode) : print(\"Episode: \"+str(i+1)+\" starts\")\n",
    "            newSeed = random.randint(0,100000)\n",
    "            state,_ = self.env.reset(seed = newSeed)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                                        \n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                self.observe(state,action,next_state,reward)\n",
    "                self.update()\n",
    "\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "\n",
    "            episodesHistory[i] = episode_reward\n",
    "            if(debug_mode) : print(\"Episode \"+str(i+1)+ \" , Reward: \"+str(episode_reward))\n",
    "        print(episodesHistory[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a8b98-8bed-4b33-9097-fa858886d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(Agent):\n",
    "\n",
    "    def __init__(self, id, epsilonMax, epsilonMin = 0.05, LossFct = torch.nn.MSELoss(reduction='mean'), reward_factor = 0.5,PredList_batch = 200, Pred=Multi_Layer_Perceptron(input_dim = 2,intern_dim = 64, output_dim = 1, depth = 2, isBiased = False), Q = Multi_Layer_Perceptron(input_dim = 3,intern_dim = 64, output_dim = 1, depth = 2, isBiased = False), env = gym.make('MountainCar-v0'), arrayNewPosReward = None, arrayNewVelReward = None, contReward = False, gamma = 0.99, replay_buffer_SIZE = 10000, batch_size = 64, observation_SIZE = 6, optimizer = torch.optim.AdamW):\n",
    "        Agent.__init__(self,id,env)\n",
    "        self.Q = Q\n",
    "        self.QTarget = copy.deepcopy(Q)\n",
    "        self.Pred = Pred\n",
    "        self.PredList = []\n",
    "        self.PredTarget = copy.deepcopy(Pred)\n",
    "        self.PredList_batch = PredList_batch\n",
    "        self.reward_factor = reward_factor\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilonMax\n",
    "        self.epsilonMax = epsilonMax\n",
    "        self.epsilonMin = epsilonMin\n",
    "        self.replay_buffer = np.zeros((replay_buffer_SIZE,observation_SIZE))\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer(self.Q.parameters())\n",
    "        self.optimizer2 = optimizer(self.Pred.parameters())\n",
    "        self.replay_buffer_SIZE = replay_buffer_SIZE\n",
    "        self.observation_SIZE = observation_SIZE\n",
    "        self.arrayNewPosReward = arrayNewPosReward\n",
    "        self.arrayNewVelReward = arrayNewVelReward\n",
    "        self.contReward = contReward\n",
    "        self.LossFct = LossFct\n",
    "        \n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        state = np.array(state)\n",
    "        action = np.array([action])\n",
    "        next_state = np.array(next_state)\n",
    "        reward = np.array([reward])\n",
    "\n",
    "        concatenatation = np.concatenate((state, action, next_state, reward))\n",
    "        return concatenatation\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        P = random.uniform(0,1)\n",
    "        a=0\n",
    "        if P <= 1-self.epsilon :\n",
    "            A=np.zeros(3)\n",
    "            o = []\n",
    "            ArgQmax = 0\n",
    "            for k in range(3):\n",
    "                A[k] = self.Q(torch.from_numpy(np.concatenate((np.array(state), np.array([k])))).to(torch.float32))\n",
    "            a = np.argmax(A)\n",
    "            for k in range(3):\n",
    "                if A[k] == A[a]:\n",
    "                    o.append(k)\n",
    "            #print(A)\n",
    "            a =random.choice(o)\n",
    "                \n",
    "        else:\n",
    "            a = random.randint(0,2)\n",
    "        return a\n",
    "        \n",
    "        \n",
    "    def update(self,j):\n",
    "        #print(min(j,self.replay_buffer.shape[0]))\n",
    "        batchIndex = np.random.choice(min(j,self.replay_buffer.shape[0]), self.batch_size)\n",
    "        batch = self.replay_buffer[batchIndex,:]\n",
    "        target = torch.zeros((self.batch_size))\n",
    "        input = torch.zeros((self.batch_size))\n",
    "\n",
    "        A0= np.copy(batch[:,3:6])\n",
    "        A0[:,2] = 0.\n",
    "        A1= np.copy(batch[:,3:6])\n",
    "        A1[:,2] = 1.\n",
    "        A2= np.copy(batch[:,3:6])\n",
    "        A2[:,2] = 2.\n",
    "        A = np.concatenate((A0,A1,A2), axis=0)\n",
    "        #print(A)\n",
    "        A = torch.from_numpy(A).to(torch.float32)\n",
    "        Qtarget = self.QTarget(A)\n",
    "        QMax = np.array([max(float(Qtarget[i]),float(Qtarget[i+self.batch_size]),float(Qtarget[i+2*self.batch_size])) for i in range(self.batch_size)])\n",
    "        target = torch.from_numpy(batch[:,5] + self.gamma*QMax).to(torch.float32)\n",
    "        #print(\"****\")\n",
    "        #print(target.mean().item())    \n",
    "        input = self.Q(torch.from_numpy(batch[:,:3]).to(torch.float32)).squeeze()\n",
    "        #print(input.mean().item())\n",
    "        loss = self.LossFct(input, target)\n",
    "        for param in self.Q.parameters():\n",
    "            param.grad = None\n",
    "            \n",
    "        loss.backward()\n",
    "\n",
    "        grad = 0\n",
    "        for layer in self.Q.children():\n",
    "            grad += layer.weight.grad.mean()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        #print(time.time() - start_time)\n",
    "        return loss.item(), abs(grad)\n",
    "\n",
    "    def update_Pred(self, j, next_state):\n",
    "        \n",
    "        m = min(j,self.replay_buffer.shape[0])\n",
    "        next_states = self.replay_buffer[:m,3:5]\n",
    "        next_state_mean = np.mean(next_states, axis=0)\n",
    "        next_state_mean_square = np.mean(next_states**2, axis=0)\n",
    "        next_state_std = (next_state_mean_square - next_state_mean**2)**(1/2)\n",
    "\n",
    "        if len(self.PredList) >= self.PredList_batch:\n",
    "            next_state = (next_state - next_state_mean)/next_state_std\n",
    "            \n",
    "        target = self.PredTarget(torch.from_numpy(next_state).to(torch.float32))\n",
    "        input = self.Pred(torch.from_numpy(next_state).to(torch.float32))\n",
    "        loss = self.LossFct(input, target)\n",
    "        RND = loss.detach().numpy().item()\n",
    "        self.PredList.append(RND)\n",
    "        #if len(self.PredList) <= self.PredList_batch*100: self.PredList.append(RND)\n",
    "        if len(self.PredList) >= self.PredList_batch:\n",
    "            RND_mean = sum(self.PredList) / float(len(self.PredList))\n",
    "            RND_mean_square = sum([ x**2 for x in self.PredList ]) / float(len(self.PredList))\n",
    "            RND_std = math.sqrt(RND_mean_square - RND_mean**2)\n",
    "            RND = (RND - RND_mean)/RND_std\n",
    "        \n",
    "        for param in self.Pred.parameters():\n",
    "            param.grad = None\n",
    "        loss.backward()\n",
    "        \n",
    "        self.optimizer2.step()\n",
    "        return RND, loss.item()\n",
    "        \n",
    "    def customReward(self, state, action, currentReward, next_state, uniqueReward, excludePassivity, RND):\n",
    "        reward = -1\n",
    "\n",
    "        if excludePassivity and action == 1:\n",
    "            reward -= 2\n",
    "\n",
    "        if self.contReward:\n",
    "            reward = (abs(next_state[0] + 0.5)**2)/200\n",
    "        \n",
    "        if self.arrayNewPosReward.all() != None:\n",
    "            for k in range(self.arrayNewPosReward.shape[0]):\n",
    "                #print(i)\n",
    "                i = self.arrayNewPosReward[k,0]\n",
    "                if (i <= next_state[0] and i +0.5 >=0) or (i >= next_state[0] and i +0.5 <=0):\n",
    "                    if self.arrayNewPosReward[k,2] == 0:\n",
    "                        if self.arrayNewPosReward[k,1] > reward:\n",
    "                            reward = self.arrayNewPosReward[k,1]\n",
    "                            if uniqueReward: self.arrayNewPosReward[k,2] = 1\n",
    "                        \n",
    "        if self.arrayNewVelReward.all() != None:\n",
    "            for k in range(self.arrayNewVelReward.shape[0]):\n",
    "                i = self.arrayNewVelReward[k,0]\n",
    "                if (i < state[1] and i>0 and action == 2) or (i > state[1] and i<0 and action == 0):\n",
    "                    reward += self.arrayNewVelReward[k,1]\n",
    "                    #print(state[1])\n",
    "                    #print(action)\n",
    "        return reward + self.reward_factor*RND\n",
    "\n",
    "    def play(self):\n",
    "        self.epsilon = 0\n",
    "        newSeed = random.randint(0,100000)\n",
    "        state,_ = self.env.reset(seed = newSeed)\n",
    "        done = False\n",
    "                    \n",
    "        while done == False:\n",
    "                                        \n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "        self.env.close()\n",
    "    \n",
    "    def train(self, episodes,lossLim = 0, limitStep = 200, refreshPredTarget = 2000, refreshQTarget = 10000, refreshQ = 1, buffer_fill = True, epsilonDecreasing =100, debug_mode=False, recap_mode=False, reset_init = False, epsilon_decrease = True, uniqueReward = False, excludePassivity = True):\n",
    "        episodesHistory = np.zeros((episodes))\n",
    "        rewardHistory = np.zeros((episodes*limitStep))\n",
    "        lossHistory = np.zeros((int(episodes*limitStep/refreshQ)))\n",
    "        gradHistory = np.zeros((int(episodes*limitStep/refreshQ)))\n",
    "        cumulativeHistory = np.zeros((episodes))\n",
    "        self.QTarget = copy.deepcopy(self.Q)\n",
    "        self.PredTarget = copy.deepcopy(self.Pred)\n",
    "        if reset_init != False:\n",
    "            self.Q.reset_init_weights_biases(reset_init)\n",
    "            self.QTarget.reset_init_weights_biases(reset_init)\n",
    "            self.Pred.reset_init_weights_biases(reset_init)\n",
    "            self.PredTarget.reset_init_weights_biases(reset_init)\n",
    "        j=0\n",
    "        self.replay_buffer = np.zeros((self.replay_buffer_SIZE, self.observation_SIZE))\n",
    "        k=0\n",
    "        for e in range (episodes):\n",
    "            l=0\n",
    "            terminated = False\n",
    "            if debug_mode: print(\"Episode: \"+str(e+1)+\" starts\")\n",
    "                \n",
    "            if epsilon_decrease: \n",
    "                if self.epsilon > self.epsilonMin:\n",
    "                    self.epsilon = self.epsilonMax*math.exp(-e/epsilonDecreasing)\n",
    "            else:\n",
    "                self.epsilon = self.epsilonMax\n",
    "                \n",
    "            newSeed = random.randint(0,100000)\n",
    "            state,_ = self.env.reset(seed = newSeed)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            if self.arrayNewPosReward.all() != None:\n",
    "                for i in range(self.arrayNewPosReward.shape[0]):\n",
    "                    self.arrayNewPosReward[i,2] = 0\n",
    "\n",
    "            s=1\n",
    "            while done == False:\n",
    "                j+=1\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                RND = 0\n",
    "                if self.reward_factor != 0: RND, loss_RND = self.update_Pred(j,next_state)\n",
    "                    \n",
    "                if s >= limitStep:\n",
    "                    truncated = True\n",
    "                else:\n",
    "                    truncated = False\n",
    "\n",
    "                s+=1\n",
    "                reward = self.customReward(state, action,reward, next_state, uniqueReward, excludePassivity,RND)\n",
    "                if debug_mode: print(\"Action \"+str(k)+\" selected: \"+str(action)+\" Reward: \"+ str(reward))\n",
    "                observe = self.observe(state,action,next_state,reward)\n",
    "                \n",
    "                if k < self.replay_buffer_SIZE: self.replay_buffer[k] = observe\n",
    "                elif buffer_fill: k=-1\n",
    "                k+=1\n",
    "                \n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "                if j%refreshQ == 0:\n",
    "                    start_time = time.time()\n",
    "                    l,g = self.update(j)\n",
    "                    print(time.time() - start_time)\n",
    "                    lossHistory[int(j/refreshQ)-1] = l\n",
    "                    gradHistory[int(j/refreshQ)-1] = g\n",
    "                if j%refreshPredTarget == 0 and self.reward_factor != 0:\n",
    "                    self.PredTarget = copy.deepcopy(self.Pred)\n",
    "                    print(\"Loss_RND :\" +str(loss_RND))\n",
    "                if j%refreshQTarget == 0:\n",
    "                    print(\"Loss: \"+str(l))\n",
    "                    self.QTarget = copy.deepcopy(self.Q)\n",
    "                    #for param in self.Q.parameters():\n",
    "                        #print(param.data)\n",
    "                rewardHistory[j-1] = reward\n",
    "                \n",
    "            if terminated:\n",
    "                if e > 0:\n",
    "                    cumulativeHistory[e] = cumulativeHistory[e-1] +1\n",
    "                else:\n",
    "                    cumulativeHistory[e] = 1\n",
    "            else:\n",
    "                if e > 0:\n",
    "                    cumulativeHistory[e] = cumulativeHistory[e-1]\n",
    "                else:\n",
    "                    cumulativeHistory[e] = 0\n",
    "                                    \n",
    "            if debug_mode or recap_mode: print(\"Episode \"+str(e+1)+ \" , Reward: \"+str(episode_reward)+\" Epsilon: \"+str(self.epsilon))\n",
    "            episodesHistory[e] = episode_reward\n",
    "            \n",
    "            if l <= lossLim:\n",
    "                print(\"Loss reaches limit\")\n",
    "                break\n",
    "                \n",
    "        print(time.time() - start_time)\n",
    "        return episodesHistory, rewardHistory, lossHistory, gradHistory, cumulativeHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a93ea22-7ebd-4ce1-bcd3-077dbe1a2991",
   "metadata": {},
   "outputs": [],
   "source": [
    "u=0.01\n",
    "episodes = 400\n",
    "lossLimTrain = -1\n",
    "batch_size = 64\n",
    "refreshQ = 1\n",
    "refreshQTarget = 5000\n",
    "refreshPredTarget = 5000\n",
    "reset_init = 0.1\n",
    "replay_buffer_SIZE = 10000\n",
    "epsilonDecreasing = 100\n",
    "reward_factor = 0\n",
    "\n",
    "NewPosReward=np.array([(-1.1,u*7,0),(-1,u*6,0),(-0.9,u*5,0),(-0.8,u*4,0),(-0.7,u*3,0),(-0.6,u*2,0),(-0.4,u*2,0),(-0.3,u*3,0),(-0.2,u*4,0),(-0.1,u*5,0),(0,u*6,0),(0.1,u*7,0),(0.2,u*8,0),(0.3,u*9,0),(0.4,u*10,0),(0.5,20*u,0)])\n",
    "NewVelReward=np.array([(0.001,1),(-0.001,1)])\n",
    "No = np.array([None])\n",
    "DQN = DQNAgent(\"id0\",epsilonMax = 1,reward_factor = reward_factor, epsilonMin = 0.05, batch_size = batch_size, contReward = False, arrayNewPosReward = No, arrayNewVelReward = No, replay_buffer_SIZE = replay_buffer_SIZE)\n",
    "episodesHistory, rewardHistory, lossHistory, gradHistory, cumulativeHistory= DQN.train(episodes,lossLim=lossLimTrain,limitStep = 200,buffer_fill = True, refreshPredTarget = refreshPredTarget, refreshQTarget = refreshQTarget, refreshQ = refreshQ, debug_mode = False, recap_mode=True, reset_init = reset_init, uniqueReward = False, excludePassivity = False, epsilonDecreasing =epsilonDecreasing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dde4285-4eca-4908-80d1-b635bfc70b49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig1,ax1 = plt.subplots(1,1)\n",
    "ax1.plot(range(episodesHistory.shape[0]), episodesHistory, marker='.')\n",
    "ax1.set_xlabel(r'episodes $t$')\n",
    "ax1.set_ylabel(r'$Total Reward$')\n",
    "ax1.set_xscale('linear')\n",
    "ax1.set_yscale('linear')\n",
    "plt.grid(color='black', which=\"both\", linestyle='-', linewidth=0.2)\n",
    "plt.savefig(f'figures/TotalReward_epochs{str(episodes)}_u{str(u)}_bS{str(batch_size)}_rQ{str(refreshQ)}_rT{str(refreshTarget)}_rI{str(reset_init)}_rB{str(replay_buffer_SIZE)}_ep{str(epsilonDecreasing)}.png', format='png')\n",
    "\n",
    "fig2,ax2 = plt.subplots(1,1)\n",
    "ax2.plot(range(rewardHistory.shape[0]), rewardHistory, marker='.')\n",
    "ax2.set_xlabel(r'Step $t$')\n",
    "ax2.set_ylabel(r'$Reward$')\n",
    "ax2.set_xscale('linear')\n",
    "ax2.set_yscale('linear')\n",
    "plt.grid(color='black', which=\"both\", linestyle='-', linewidth=0.2)\n",
    "plt.savefig(f'figures/Reward_epochs{episodes}_u{u}_bS{batch_size}_rQ{refreshQ}_rT{refreshTarget}_rI{reset_init}_rB{replay_buffer_SIZE}_ep{epsilonDecreasing}.png', format='png')\n",
    "\n",
    "\n",
    "fig3,ax3 = plt.subplots(1,1)\n",
    "ax3.plot(range(lossHistory.shape[0]), lossHistory, marker='.')\n",
    "ax3.set_xlabel(r'Update $t$')\n",
    "ax3.set_ylabel(r'$Loss$')\n",
    "ax3.set_xscale('linear')\n",
    "ax3.set_yscale('log')\n",
    "#plt.ylim(top=0.15)\n",
    "plt.grid(color='black', which=\"both\", linestyle='-', linewidth=0.2)\n",
    "plt.savefig(f'figures/Loss_epochs{episodes}_u{u}_bS{batch_size}_rQ{refreshQ}_rT{refreshTarget}_rI{reset_init}_rB{replay_buffer_SIZE}_ep{epsilonDecreasing}.png', format='png')\n",
    "\n",
    "fig4,ax4 = plt.subplots(1,1)\n",
    "ax4.plot(range(gradHistory.shape[0]), gradHistory, marker='.')\n",
    "ax4.set_xlabel(r'Update $t$')\n",
    "ax4.set_ylabel(r'$Grad$')\n",
    "ax4.set_xscale('linear')\n",
    "ax4.set_yscale('log')\n",
    "plt.grid(color='black', which=\"both\", linestyle='-', linewidth=0.2)\n",
    "plt.savefig(f'figures/Grad_epochs{episodes}_u{u}_bS{batch_size}_rQ{refreshQ}_rT{refreshTarget}_rI{reset_init}_rB{replay_buffer_SIZE}_ep{epsilonDecreasing}.png', format='png')\n",
    "\n",
    "\n",
    "s=0\n",
    "for e in range(cumulativeHistory.shape[0]):\n",
    "    if cumulativeHistory[e] == 1:\n",
    "        s = e\n",
    "        break\n",
    "fig5,ax5 = plt.subplots(1,1)\n",
    "ax5.plot(range(cumulativeHistory.shape[0] - s), cumulativeHistory[s:], marker='.')\n",
    "ax5.set_xlabel(r'episodes: first_success: ep '+str(s))\n",
    "ax5.set_ylabel(r'$CumulativeSucess$')\n",
    "ax5.set_xscale('linear')\n",
    "ax5.set_yscale('linear')\n",
    "plt.grid(color='black', which=\"both\", linestyle='-', linewidth=0.2)\n",
    "plt.savefig(f'figures/CumulativeSucess_epochs{episodes}_u{u}_bS{batch_size}_rQ{refreshQ}_rT{refreshTarget}_rI{reset_init}_rB{replay_buffer_SIZE}_ep{epsilonDecreasing}.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cd15d7-4c99-4ea6-8dc0-a6b57044168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN.env = gym.make('MountainCar-v0', render_mode='human')\n",
    "DQN.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac77d547-9e22-4e27-b569-d8835df67752",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0.5\n",
    "MatVel = np.arange(-1*100,1*100,step*100, dtype = int)\n",
    "step2 = 0.01\n",
    "MatPos = np.arange(-1.2*100,(0.5+step2)*100,step2*100, dtype = int)\n",
    "print(MatVel.shape[0])\n",
    "print(MatPos.shape[0])\n",
    "\n",
    "MatState = np.array((MatVel.shape[0]*MatPos.shape[0]))\n",
    "MatState[:171]\n",
    "MatState[171:2*171]\n",
    "vel = 0.11\n",
    "indexV = 0\n",
    "for i in range(MatVel.shape[0]):\n",
    "    if MatVel[i] >= vel*100:\n",
    "        indexV = i\n",
    "        break\n",
    "print(vel/100)\n",
    "\n",
    "indexState = iV*MatPos.shape[0] + iP\n",
    "\n",
    "iV = iS//MatPos.shape[0]\n",
    "iP = iS%MatPos.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c3e60-bf01-42b8-968f-e8cf05543aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def update(self,j):\n",
    "        #print(min(j,self.replay_buffer.shape[0]))\n",
    "        batch = np.random.choice(min(j,self.replay_buffer.shape[0]), self.batch_size)\n",
    "        target = torch.zeros((self.batch_size))\n",
    "        input = torch.zeros((self.batch_size))\n",
    "        start_time = time.time()\n",
    "        for i in range(self.batch_size):\n",
    "            if self.replay_buffer[batch[i],3] <0.5:\n",
    "                A0 = torch.from_numpy(np.concatenate((self.replay_buffer[batch[i],3:5], np.array([0])))).to(torch.float32)\n",
    "                A1 = torch.from_numpy(np.concatenate((self.replay_buffer[batch[i],3:5], np.array([1])))).to(torch.float32)\n",
    "                A2 = torch.from_numpy(np.concatenate((self.replay_buffer[batch[i],3:5], np.array([2])))).to(torch.float32)\n",
    "                target[i] = self.replay_buffer[batch[i],5] + self.gamma*max(self.QTarget(A0),self.QTarget(A1),self.QTarget(A2))\n",
    "            else:\n",
    "                target[i] = self.replay_buffer[batch[i],5]\n",
    "                \n",
    "            input[i] = self.Q(torch.from_numpy(self.replay_buffer[batch[i],:3]).to(torch.float32))\n",
    "        print(time.time() - start_time)\n",
    "        loss = self.LossFct(input, target)\n",
    "        for param in self.Q.parameters():\n",
    "            param.grad = None\n",
    "            \n",
    "        loss.backward()\n",
    "\n",
    "        grad = 0\n",
    "        for layer in self.Q.children():\n",
    "            grad += layer.weight.grad.mean()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        return loss.item(), abs(grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
