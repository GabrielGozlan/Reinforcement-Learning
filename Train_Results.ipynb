{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8989c07b-8e50-4ce3-9e9b-7588ac8000a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Agent_DQN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf1b0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 168/990 [02:54<15:11,  1.11s/it]"
     ]
    }
   ],
   "source": [
    "u=0\n",
    "episodes = 990\n",
    "lossLimTrain = -1\n",
    "batch_size = 64\n",
    "refreshQ = 1\n",
    "refreshQTarget = 50000\n",
    "reset_init = 1\n",
    "replay_buffer_SIZE = 10000\n",
    "epsilonDecreasing = 100\n",
    "reward_factor = 4\n",
    "PredList_batch = 10000\n",
    "PredFirstUpdate = 1000\n",
    "\n",
    "NewPosReward=np.array([(-1.1,u*7,0),(-1,u*6,0),(-0.9,u*5,0),(-0.8,u*4,0),(-0.7,u*3,0),(-0.6,u*2,0),(-0.4,u*2,0),(-0.3,u*3,0),(-0.2,u*4,0),(-0.1,u*5,0),(0,u*6,0),(0.1,u*7,0),(0.2,u*8,0),(0.3,u*9,0),(0.4,u*10,0),(0.5,20*u,0)])\n",
    "NewVelReward=np.array([(0.001,1),(-0.001,1)])\n",
    "No = np.array([None])\n",
    "DQN = DQNAgent(\"id0\",epsilonMax = 1, PredFirstUpdate = PredFirstUpdate, PredList_batch = PredList_batch, reward_factor = reward_factor, epsilonMin = 0.05, batch_size = batch_size, contReward = False, arrayNewPosReward = No, arrayNewVelReward = No, replay_buffer_SIZE = replay_buffer_SIZE)\n",
    "episodesHistory, rewardHistory, lossHistory, gradHistory, cumulativeHistory, lossRNDHistory, Q_Trained = DQN.train(episodes,lossLim=lossLimTrain, limitStep = 200,buffer_fill = True, refreshQTarget = refreshQTarget, refreshQ = refreshQ, debug_mode = False, recap_mode=False, reset_init = reset_init, uniqueReward = False, excludePassivity = False, epsilonDecreasing =epsilonDecreasing)\n",
    "\n",
    "plot_all_results(episodesHistory, rewardHistory, lossHistory, gradHistory, cumulativeHistory, lossRNDHistory, episodes, u, batch_size, reward_factor, refreshQTarget, reset_init, replay_buffer_SIZE, epsilonDecreasing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f20965-73c5-4268-8a38-b3ef1a4b61e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = Multi_Layer_Perceptron(input_dim = 2,intern_dim = 64, output_dim = 3, depth = 2, isBiased = False)\n",
    "Q.load_state_dict(Q_Trained)\n",
    "DQN_Test = DQNAgent(\"idTest\", env=gym.make('MountainCar-v0', render_mode='human'))\n",
    "DQN_Test.Q = Q\n",
    "DQN_Test.play(seed = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b37eca-fc4c-4bb2-b9ce-6bd8d9d3922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_results(episodesHistory, rewardHistory, lossHistory, gradHistory, cumulativeHistory, lossRNDHistory, episodes, u, batch_size, reward_factor, refreshQTarget, reset_init, replay_buffer_SIZE, epsilonDecreasing):\n",
    "\n",
    "    fig1,ax1 = plt.subplots(1,1)\n",
    "    ax1.plot(range(episodesHistory.shape[0]), episodesHistory, marker='.')\n",
    "    ax1.set_title(f'TotalReward QTargetRefresh: {refreshQTarget}')\n",
    "    if u>0: ax1.set_title(f'TotalReward with heuristic reward QTargetRefresh: {refreshQTarget}')\n",
    "    if reward_factor>0:ax1.set_title(f'TotalReward with RND RewardFactor: {reward_factor} QTargetRefresh: {refreshQTarget}')\n",
    "    ax1.set_xlabel(r'episodes $t$')\n",
    "    ax1.set_ylabel(r'$Total Reward$')\n",
    "    ax1.set_xscale('linear')\n",
    "    ax1.set_yscale('linear')\n",
    "    plt.grid(color='black', which=\"both\", linestyle='-', linewidth=0.2)\n",
    "    plt.savefig(f'figures/TotalReward_ep{episodes}_u{u}_bS{batch_size}_rF{reward_factor}_rT{refreshQTarget}_iN{reset_init}_rB{replay_buffer_SIZE}_eD{epsilonDecreasing}.png', format='png')\n",
    "    \n",
    "    fig2,ax2 = plt.subplots(1,1)\n",
    "    ax2.plot(range(rewardHistory.shape[0]), rewardHistory, marker='.')\n",
    "    ax2.set_title(f'Reward QTargetRefresh: {refreshQTarget}')\n",
    "    if u>0: ax2.set_title(f'Reward with heuristic reward QTargetRefresh: {refreshQTarget}')\n",
    "    if reward_factor>0:ax2.set_title(f'Reward with RND RewardFactor: {reward_factor} QTargetRefresh: {refreshQTarget}')\n",
    "    ax2.set_xlabel(r'Step $t$')\n",
    "    ax2.set_ylabel(r'$Reward$')\n",
    "    ax2.set_xscale('linear')\n",
    "    ax2.set_yscale('linear')\n",
    "    plt.grid(color='black', which=\"both\", linestyle='-', linewidth=0.2)\n",
    "    plt.savefig(f'figures/Reward_ep{episodes}_u{u}_bS{batch_size}_rF{reward_factor}_rT{refreshQTarget}_iN{reset_init}_rB{replay_buffer_SIZE}_eD{epsilonDecreasing}.png', format='png')\n",
    "    \n",
    "    \n",
    "    fig3,ax3 = plt.subplots(1,1)\n",
    "    ax3.plot(range(lossHistory.shape[0]), lossHistory, marker='.')\n",
    "    ax3.set_title(f'Loss QTargetRefresh: {refreshQTarget}')\n",
    "    if u>0: ax3.set_title(f'Loss with heuristic reward QTargetRefresh: {refreshQTarget}')\n",
    "    if reward_factor>0:ax3.set_title(f'Loss with RND RewardFactor: {reward_factor} QTargetRefresh: {refreshQTarget}')\n",
    "    ax3.set_xlabel(r'Update $t$')\n",
    "    ax3.set_ylabel(r'$Loss$')\n",
    "    ax3.set_xscale('linear')\n",
    "    ax3.set_yscale('log')\n",
    "    #plt.ylim(top=0.15)\n",
    "    plt.grid(color='black', which=\"both\", linestyle='-', linewidth=0.2)\n",
    "    plt.savefig(f'figures/Loss_ep{episodes}_u{u}_bS{batch_size}_rF{reward_factor}_rT{refreshQTarget}_iN{reset_init}_rB{replay_buffer_SIZE}_eD{epsilonDecreasing}.png', format='png')\n",
    "    \n",
    "    fig3b,ax3b = plt.subplots(1,1)\n",
    "    ax3b.plot(range(lossRNDHistory.shape[0]), lossRNDHistory, marker='.')\n",
    "    ax3b.set_title(f'RNDLoss QTargetRefresh: {refreshQTarget}')\n",
    "    if u>0: ax3b.set_title(f'RNDLoss with heuristic reward QTargetRefresh: {refreshQTarget}')\n",
    "    if reward_factor>0:ax3b.set_title(f'RNDLoss with RND RewardFactor: {reward_factor} QTargetRefresh: {refreshQTarget}')\n",
    "    ax3b.set_xlabel(r'Update $t$')\n",
    "    ax3b.set_ylabel(r'$RNDLoss$')\n",
    "    ax3b.set_xscale('linear')\n",
    "    ax3b.set_yscale('log')\n",
    "    plt.grid(color='black', which=\"both\", linestyle='-', linewidth=0.2)\n",
    "    plt.savefig(f'figures/LossRND_ep{episodes}_u{u}_bS{batch_size}_rF{reward_factor}_rT{refreshQTarget}_iN{reset_init}_rB{replay_buffer_SIZE}_eD{epsilonDecreasing}.png', format='png')\n",
    "    \n",
    "    fig4,ax4 = plt.subplots(1,1)\n",
    "    ax4.plot(range(gradHistory.shape[0]), gradHistory, marker='.')\n",
    "    ax4.set_title(f'Gradient QTargetRefresh: {refreshQTarget}')\n",
    "    if u>0: ax4.set_title(f'Gradient with heuristic reward QTargetRefresh: {refreshQTarget}')\n",
    "    if reward_factor>0:ax4.set_title(f'Gradient with RND RewardFactor: {reward_factor} QTargetRefresh: {refreshQTarget}')\n",
    "    ax4.set_xlabel(r'Update $t$')\n",
    "    ax4.set_ylabel(r'$Gradient$')\n",
    "    ax4.set_xscale('linear')\n",
    "    ax4.set_yscale('log')\n",
    "    plt.grid(color='black', which=\"both\", linestyle='-', linewidth=0.2)\n",
    "    plt.savefig(f'figures/Grad_ep{episodes}_u{u}_bS{batch_size}_rF{reward_factor}_rT{refreshQTarget}_iN{reset_init}_rB{replay_buffer_SIZE}_eD{epsilonDecreasing}.png', format='png')\n",
    "    \n",
    "    \n",
    "    s=0\n",
    "    for e in range(cumulativeHistory.shape[0]):\n",
    "        if cumulativeHistory[e] == 1:\n",
    "            s = e\n",
    "            break\n",
    "    fig5,ax5 = plt.subplots(1,1)\n",
    "    ax5.plot(range(cumulativeHistory.shape[0]), cumulativeHistory[:], marker='.')\n",
    "    ax5.set_title(f'CumulativeSucess QTargetRefresh: {refreshQTarget}')\n",
    "    if u>0: ax5.set_title(f'CumulativeSucess with heuristic reward QTargetRefresh: {refreshQTarget}')\n",
    "    if reward_factor>0:ax5.set_title(f'CumulativeSucess with RND RewardFactor: {reward_factor} QTargetRefresh: {refreshQTarget}')\n",
    "    ax5.set_xlabel(r'episodes: first_success: ep '+str(s))\n",
    "    ax5.set_ylabel(r'$CumulativeSucess$')\n",
    "    ax5.set_xscale('linear')\n",
    "    ax5.set_yscale('linear')\n",
    "    plt.grid(color='black', which=\"both\", linestyle='-', linewidth=0.2)\n",
    "    plt.savefig(f'figures/CumulativeSucess_ep{episodes}_u{u}_bS{batch_size}_rF{reward_factor}_rT{refreshQTarget}_iN{reset_init}_rB{replay_buffer_SIZE}_eD{epsilonDecreasing}.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55804b1-41f4-458d-b31b-db349d5d976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "u=0.05\n",
    "episodes = 990\n",
    "lossLimTrain = -1\n",
    "batch_size = 64\n",
    "refreshQ = 1\n",
    "refreshQTarget = 50000\n",
    "reset_init = 1\n",
    "replay_buffer_SIZE = 10000\n",
    "epsilonDecreasing = 100\n",
    "reward_factor = 0\n",
    "PredList_batch = 0\n",
    "PredFirstUpdate = 0\n",
    "\n",
    "NewPosReward=np.array([(-1.1,u*7,0),(-1,u*6,0),(-0.9,u*5,0),(-0.8,u*4,0),(-0.7,u*3,0),(-0.6,u*2,0),(-0.4,u*2,0),(-0.3,u*3,0),(-0.2,u*4,0),(-0.1,u*5,0),(0,u*6,0),(0.1,u*7,0),(0.2,u*8,0),(0.3,u*9,0),(0.4,u*10,0),(0.5,20*u,0)])\n",
    "No = np.array([None])\n",
    "\n",
    "DQN = DQNAgent(\"id0\",epsilonMax = 1, PredFirstUpdate = PredFirstUpdate, PredList_batch = PredList_batch, reward_factor = reward_factor, epsilonMin = 0.05, batch_size = batch_size, contReward = False, arrayNewPosReward = NewPosReward, arrayNewVelReward = No, replay_buffer_SIZE = replay_buffer_SIZE)\n",
    "episodesHistory, rewardHistory, lossHistory, gradHistory, cumulativeHistory, lossRNDHistory, Q_Trained = DQN.train(episodes,lossLim=lossLimTrain, limitStep = 200,buffer_fill = True, refreshQTarget = refreshQTarget, refreshQ = refreshQ, debug_mode = False, recap_mode=False, reset_init = reset_init, uniqueReward = False, excludePassivity = False, epsilonDecreasing =epsilonDecreasing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194af984-ab73-4f94-8977-5d8e917440c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(Agent):\n",
    "\n",
    "    def __init__(self, id, epsilonMax=1, epsilonMin = 0.05, reward_factor = 0.5, PredFirstUpdate = 200, PredList_batch = 200, env = gym.make('MountainCar-v0'), arrayNewPosReward = None, gamma = 0.99, replay_buffer_SIZE = 10000, batch_size = 64, observation_SIZE = 6):\n",
    "        Agent.__init__(self,id,env)\n",
    "        '''\n",
    "        Definition of MLP for the Policy + the RND\n",
    "        '''\n",
    "        self.Q = Multi_Layer_Perceptron(input_dim = 2,intern_dim = 64, output_dim = 3, depth = 2, isBiased = False) # For the policy\n",
    "        self.QTarget = Multi_Layer_Perceptron(input_dim = 2,intern_dim = 64, output_dim = 3, depth = 2, isBiased = False)\n",
    "        self.Pred = Multi_Layer_Perceptron(input_dim = 2,intern_dim = 64, output_dim = 1, depth = 2, isBiased = False) # For the RND\n",
    "        self.PredTarget = Multi_Layer_Perceptron(input_dim = 2,intern_dim = 64, output_dim = 1, depth = 2, isBiased = False)\n",
    "        '''\n",
    "        Definition of the hyperparameters for the training\n",
    "        '''\n",
    "        self.arrayNewPosReward = arrayNewPosReward # Heuristic Rewards\n",
    "        self.reward_factor = reward_factor # For the RND\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilonMax\n",
    "        self.epsilonMax = epsilonMax\n",
    "        self.epsilonMin = epsilonMin\n",
    "        '''\n",
    "        Definition of the replay buffer\n",
    "        '''\n",
    "        self.replay_buffer_SIZE = replay_buffer_SIZE\n",
    "        self.observation_SIZE = observation_SIZE\n",
    "        self.replay_buffer = np.zeros((replay_buffer_SIZE,observation_SIZE))\n",
    "        self.batch_size = batch_size\n",
    "        '''\n",
    "        Definition of the optimizer + loss function (there are 2 for the Q + RND)\n",
    "        '''        \n",
    "        self.optimizer = torch.optim.AdamW(self.Q.parameters())\n",
    "        self.optimizer2 = torch.optim.AdamW(self.Pred.parameters())     \n",
    "        self.LossFct = torch.nn.MSELoss()\n",
    "        self.LossFct2 = torch.nn.MSELoss()\n",
    "        '''\n",
    "        Definition of parameters for the computing of the normalization (for the RND)\n",
    "        '''        \n",
    "        self.next_state_mean = 0\n",
    "        self.next_state_std = 1\n",
    "        self.RND_std = 1\n",
    "        self.RND_mean = 0\n",
    "        self.PredList = [] #History to compute the normalization\n",
    "        self.PredList_batch = PredList_batch #When do we start to compute normalization\n",
    "        self.PredFirstUpdate = PredFirstUpdate #When do we start to update the Q network (to be sure to have enough data for the normalization and avoid big value)\n",
    "        '''\n",
    "        Definition of extra informations\n",
    "        '''   \n",
    "        self.last_success_seed = 0 # To facilitate the test after training\n",
    "        \n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        state = np.array(state)\n",
    "        action = np.array([action])\n",
    "        next_state = np.array(next_state)\n",
    "        reward = np.array([reward])\n",
    "\n",
    "        concatenatation = np.concatenate((state, action, next_state, reward))\n",
    "        return concatenatation\n",
    "    '''\n",
    "    Select actions with exploration (for the training)\n",
    "    '''          \n",
    "    def select_action(self, state):\n",
    "        P = random.uniform(0,1)\n",
    "        a=0\n",
    "        if P <= 1-self.epsilon :\n",
    "            arg = self.Q(torch.from_numpy(state).to(torch.float32)).max(axis = 0)[1].numpy()\n",
    "            if arg.size == 1:\n",
    "                a = arg\n",
    "            else:\n",
    "                a = np.random.choice(arg, size = 1)[0]\n",
    "        else:\n",
    "            a = random.randint(0,2)\n",
    "        return a\n",
    "    '''\n",
    "    Select actions without exploration (for the tests)\n",
    "    '''  \n",
    "    def select_best_action(self, state):\n",
    "        a = 0\n",
    "        arg = self.Q(torch.from_numpy(state).to(torch.float32)).max(axis = 0)[1].numpy()\n",
    "        if arg.size == 1:\n",
    "            a = arg\n",
    "        else:\n",
    "            a = np.random.choice(arg, size = 1)[0]\n",
    "        return a\n",
    "    '''\n",
    "    Update the Q network (j: nb of epochs)\n",
    "    '''  \n",
    "    def update(self,j):\n",
    "        #print(min(j,self.replay_buffer.shape[0]))\n",
    "        batchIndex = np.random.choice(min(j,self.replay_buffer.shape[0]), self.batch_size)\n",
    "        batch = self.replay_buffer[batchIndex,:]\n",
    "        target = torch.zeros((self.batch_size))\n",
    "        input = torch.zeros((self.batch_size))\n",
    "\n",
    "        A = torch.from_numpy(batch[:,3:5]).to(torch.float32)\n",
    "        \n",
    "        QMax = self.QTarget(A).max(axis = 1)[0].detach().numpy()\n",
    "        target = torch.from_numpy(batch[:,5] + self.gamma*QMax).to(torch.float32)\n",
    "        \n",
    "        for i in range(A.shape[0]):\n",
    "            if A[i,0] >= 0.5:\n",
    "                target[i] = batch[i,5]\n",
    "                \n",
    "        #print(\"****\")\n",
    "        #print(target.mean().item()+1)    \n",
    "        input = self.Q(torch.from_numpy(batch[:,:2]).to(torch.float32))[np.arange(batch.shape[0]),batch[:,2]]\n",
    "        #print(QMax - input.detach().numpy())\n",
    "        #print(input.mean().item())\n",
    "        loss = self.LossFct(input, target)\n",
    "        self.optimizer.zero_grad()\n",
    "            \n",
    "        loss.backward()\n",
    "\n",
    "        grad = 0\n",
    "        for layer in self.Q.children():\n",
    "            grad += layer.weight.grad.mean()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        #print(time.time() - start_time)\n",
    "        return loss.item(), abs(grad)\n",
    "    '''\n",
    "    Update the predictor of RND (j: nb of epochs)\n",
    "    '''          \n",
    "    def update_Pred(self, j, next_state):\n",
    "\n",
    "        L = 0\n",
    "        if j <= self.PredList_batch:\n",
    "            next_states = self.replay_buffer[:j,3:5]\n",
    "            self.next_state_mean = np.mean(next_states, axis=0)\n",
    "            next_state_mean_square = np.mean(next_states**2, axis=0)\n",
    "            self.next_state_std = 2*(next_state_mean_square - self.next_state_mean**2)**(1/2)\n",
    "\n",
    "        if j >= self.PredFirstUpdate:\n",
    "            next_state = (next_state - self.next_state_mean)/self.next_state_std\n",
    "            \n",
    "            target = self.PredTarget(torch.from_numpy(next_state).to(torch.float32))\n",
    "            input = self.Pred(torch.from_numpy(next_state).to(torch.float32))\n",
    "            loss = self.LossFct2(input, target)\n",
    "            RND = loss.detach().numpy().item()\n",
    "    \n",
    "            if RND > 5: RND = 5\n",
    "            if RND < -5: RND = -5\n",
    "    \n",
    "            self.optimizer2.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer2.step()\n",
    "            \n",
    "            if len(self.PredList) <= self.PredList_batch:\n",
    "                self.PredList.append(RND)\n",
    "                self.RND_mean = sum(self.PredList) / float(len(self.PredList))\n",
    "                RND_mean_square = sum([ x**2 for x in self.PredList ]) / float(len(self.PredList))\n",
    "                self.RND_std = math.sqrt(RND_mean_square - self.RND_mean**2)\n",
    "                \n",
    "            if len(self.PredList) >= self.PredList_batch:\n",
    "                    \n",
    "                RND = (RND - self.RND_mean)/self.RND_std\n",
    "                \n",
    "                if RND > 5: RND = 5\n",
    "                if RND < -5: RND = -5\n",
    "                    \n",
    "            else: RND = 0\n",
    "                \n",
    "            L = loss.item()\n",
    "        else: RND = 0\n",
    "            \n",
    "        return RND, L\n",
    "    '''\n",
    "    customReward: Heuristic or/and RND (uniqueReward: heuristic reward can be taken only one time during an episode)\n",
    "    '''        \n",
    "    def customReward(self, state, action, currentReward, next_state, uniqueReward, RND):\n",
    "        reward = -1\n",
    "     \n",
    "        if self.arrayNewPosReward.all() != None:\n",
    "            for k in range(self.arrayNewPosReward.shape[0]):\n",
    "                #print(i)\n",
    "                i = self.arrayNewPosReward[k,0]\n",
    "                if (i <= next_state[0] and i +0.5 >=0) or (i >= next_state[0] and i +0.5 <=0):\n",
    "                    if self.arrayNewPosReward[k,2] == 0:\n",
    "                        if self.arrayNewPosReward[k,1] > reward:\n",
    "                            reward = self.arrayNewPosReward[k,1]\n",
    "                            if uniqueReward: self.arrayNewPosReward[k,2] = 1\n",
    "                                \n",
    "        return reward + self.reward_factor*RND\n",
    "    '''\n",
    "    Test the agent on a seed (random or not) after the training\n",
    "    '''  \n",
    "    def play(self, seed = False):\n",
    "        newSeed = random.randint(0,100000)\n",
    "        \n",
    "        if seed != False:\n",
    "            newSeed = seed\n",
    "            \n",
    "        state,_ = self.env.reset(seed = newSeed)\n",
    "        done = False\n",
    "                    \n",
    "        while done == False:\n",
    "                                        \n",
    "            action = self.select_best_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "        self.env.close()\n",
    "    '''\n",
    "    All the training process   (lossLim: to do an early stopping (avoid training collaps) ; \n",
    "                                limitStep: to increase length of an episode ; \n",
    "                                refreshQ: to update Q (should be each epochs/actions) ; \n",
    "                                buffer_fill: to continue to fill the replay_buffer if it's full ;\n",
    "                                epsilonDecreasing: how fast epsilon decrease during the training (exponentially) ;\n",
    "                                debug_mode: print each action information ;\n",
    "                                recap_mode: print each episode information ;\n",
    "                                reset_init: to reset the weight at the start of the training (norm can be choosen by passing a float) ;\n",
    "    '''  \n",
    "    def train(self, episodes,lossLim = 0, limitStep = 200, refreshQTarget = 10000, refreshQ = 1, buffer_fill = True, epsilonDecreasing =100, debug_mode=False, recap_mode=False, reset_init = False, epsilon_decrease = True, uniqueReward = False):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        episodesHistory = np.zeros((episodes))\n",
    "        rewardHistory = np.zeros((episodes*limitStep))\n",
    "        lossHistory = np.zeros((int(episodes*limitStep/refreshQ)))\n",
    "        lossRNDHistory = np.zeros((int(episodes*limitStep)))\n",
    "        gradHistory = np.zeros((int(episodes*limitStep/refreshQ)))\n",
    "        cumulativeHistory = np.zeros((episodes))\n",
    "        if reset_init != False:\n",
    "            self.Q.reset_init_weights_biases(reset_init)\n",
    "            self.Pred.reset_init_weights_biases(reset_init)\n",
    "            self.PredTarget.reset_init_weights_biases(reset_init)\n",
    "\n",
    "        self.QTarget.load_state_dict(self.Q.state_dict())\n",
    "        j=0\n",
    "        self.replay_buffer = np.zeros((self.replay_buffer_SIZE, self.observation_SIZE))\n",
    "        k=0\n",
    "        \n",
    "        for e in tqdm(range(episodes)):\n",
    "            l=0\n",
    "            terminated = False\n",
    "            if debug_mode: print(\"Episode: \"+str(e+1)+\" starts\")\n",
    "                \n",
    "            if epsilon_decrease: \n",
    "                if self.epsilon > self.epsilonMin:\n",
    "                    self.epsilon = self.epsilonMax*math.exp(-e/epsilonDecreasing)\n",
    "            else:\n",
    "                self.epsilon = self.epsilonMax\n",
    "                \n",
    "            newSeed = random.randint(0,100000)\n",
    "            state,_ = self.env.reset(seed = newSeed)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            if self.arrayNewPosReward.all() != None:\n",
    "                for i in range(self.arrayNewPosReward.shape[0]):\n",
    "                    self.arrayNewPosReward[i,2] = 0\n",
    "\n",
    "            s=1\n",
    "            while done == False:\n",
    "                j+=1\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                RND = 0\n",
    "                loss_RND = 0\n",
    "                if self.reward_factor != 0: RND, loss_RND = self.update_Pred(j,next_state)\n",
    "                lossRNDHistory[j-1] = loss_RND\n",
    "                if s >= limitStep:\n",
    "                    truncated = True\n",
    "                else:\n",
    "                    truncated = False\n",
    "\n",
    "                s+=1\n",
    "                reward = self.customReward(state, action,reward, next_state, uniqueReward, excludePassivity,RND)\n",
    "                if debug_mode: print(\"Action \"+str(k)+\" selected: \"+str(action)+\" Reward: \"+ str(reward))\n",
    "                observe = self.observe(state,action,next_state,reward)\n",
    "                \n",
    "                if k < self.replay_buffer_SIZE: self.replay_buffer[k] = observe\n",
    "                elif buffer_fill: k=-1\n",
    "                k+=1\n",
    "                \n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "                if j%refreshQ == 0 and j >= self.PredFirstUpdate:\n",
    "                    #start_time = time.time()\n",
    "                    l,g = self.update(j)\n",
    "                    #print(time.time() - start_time)\n",
    "                    lossHistory[int(j/refreshQ)-1] = l\n",
    "                    gradHistory[int(j/refreshQ)-1] = g\n",
    "                if j%refreshQTarget == 0:\n",
    "                    if recap_mode: print(\"Loss: \"+str(l))\n",
    "                    self.QTarget.load_state_dict(self.Q.state_dict())\n",
    "                    #for param in self.Q.parameters():\n",
    "                        #print(param.data)\n",
    "                rewardHistory[j-1] = reward\n",
    "                \n",
    "                if terminated:\n",
    "                    self.last_success_seed = newSeed\n",
    "                    if e > 0:\n",
    "                        cumulativeHistory[e] = cumulativeHistory[e-1] +1\n",
    "                    else:\n",
    "                        cumulativeHistory[e] = 1\n",
    "                else:\n",
    "                    if e > 0:\n",
    "                        cumulativeHistory[e] = cumulativeHistory[e-1]\n",
    "                    else:\n",
    "                        cumulativeHistory[e] = 0\n",
    "                                    \n",
    "            if debug_mode or recap_mode: print(\"Episode \"+str(e+1)+ \" , Reward: \"+str(episode_reward)+\" Epsilon: \"+str(self.epsilon))\n",
    "            episodesHistory[e] = episode_reward\n",
    "            \n",
    "            if l <= lossLim:\n",
    "                print(\"Loss reaches limit\")\n",
    "                break\n",
    "        print(self.last_success_seed)\n",
    "        print(time.time()-start_time)\n",
    "        return episodesHistory, rewardHistory, lossHistory, gradHistory, cumulativeHistory, lossRNDHistory, self.Q.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
