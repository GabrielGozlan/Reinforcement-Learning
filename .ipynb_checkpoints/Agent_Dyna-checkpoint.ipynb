{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9968fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb52ffe-7290-4c94-8850-db3c480348f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def select_action(self, state):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, episodes, debug_mode=False):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, id, env):\n",
    "        self.id = id\n",
    "        self.env = env\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da8dc4c5-d0f8-4e79-b8ed-217f095a6532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([72., 28.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discr_step = np.array([0.025, 0.005])\n",
    "env = gym.make('MountainCar-v0')\n",
    "(env.observation_space.high - env.observation_space.low)//discr_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "345c3e60-bf01-42b8-968f-e8cf05543aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cf code cours, après avoir obtenu un modèle de l'environnement, résoudre le porblème d'optimisation avec le dynamic programming\n",
    "\n",
    "class DynaAgent(Agent):\n",
    "    \n",
    "    def __init__(self, id, env=gym.make('MountainCar-v0'), epsilonMax = 0.9, epsilonMin = 0.05, discr_step = np.array([0.025, 0.02]), gamma = 0.99, k=0, k_fixed = True, alpha=0.2, observation_SIZE = 6, replay_buffer_SIZE = 10000):\n",
    "        Agent.__init__(self,id,env)\n",
    "        self.discr_step = discr_step\n",
    "        self.n_xbins = np.round(((self.env.observation_space.high - self.env.observation_space.low)/self.discr_step)[0]).astype(np.int32)\n",
    "        print(self.n_xbins)\n",
    "        print(self.env.observation_space.high)\n",
    "        print(self.env.observation_space.low)\n",
    "        self.n_vbins = np.round(((self.env.observation_space.high - self.env.observation_space.low)/self.discr_step)[1]).astype(np.int32)\n",
    "        print(self.n_vbins)\n",
    "        self.n_states = self.n_xbins*self.n_vbins\n",
    "        self.n_actions = 3\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilonMax\n",
    "        self.epsilonMax = epsilonMax\n",
    "        self.epsilonMin = epsilonMin\n",
    "        self.k = k\n",
    "        self.k_fixed = k_fixed\n",
    "        self.alpha = alpha\n",
    "        '''\n",
    "        Definition of the replay buffer\n",
    "        '''\n",
    "        self.replay_buffer_SIZE = replay_buffer_SIZE\n",
    "        self.observation_SIZE = observation_SIZE\n",
    "        self.visited_state_action_Array = np.zeros((replay_buffer_SIZE,observation_SIZE))\n",
    "        self.visited_state_action = set()\n",
    "        \n",
    "        self.N = np.zeros((self.n_states, self.n_actions, self.n_states))\n",
    "        \n",
    "        self.P = np.zeros(shape=(self.n_states, self.n_actions, self.n_states))\n",
    "        for i in range(self.n_states):\n",
    "            for j in range(self.n_actions):\n",
    "                random = np.random.rand(self.n_states)\n",
    "                self.P[i, j, :] = random/random.sum()\n",
    "        \n",
    "        self.R = - np.ones(shape=(self.n_states, self.n_actions))\n",
    "        self.terminal_x_bin = self.discretize_x(0.5)*self.n_vbins\n",
    "        print(self.terminal_x_bin)\n",
    "        \n",
    "        self.Q = np.zeros(shape=(self.n_states, self.n_actions))\n",
    "\n",
    "        self.lossHistory = 0\n",
    "        self.up_count = 0\n",
    "    \n",
    "\n",
    "    # On obtient les s du (s, a, s') en discrétisant (cf plus haut), puis pour les s' on utilise une loi uniforme pour chaque paire (s, a)\n",
    "\n",
    "    def discretize_x(self, x):\n",
    "        x_bin = np.round(((x - self.env.observation_space.low)/self.discr_step)[0]).astype(np.int32)\n",
    "        return x_bin\n",
    "\n",
    "    def discretize_v(self, v):\n",
    "        v_bin = np.round(((v - self.env.observation_space.low)/self.discr_step)[1]).astype(np.int32)\n",
    "        return v_bin \n",
    "\n",
    "    def discretize(self, state):\n",
    "        x_bin = self.discretize_x(state[0])\n",
    "        v_bin = self.discretize_v(state[1])\n",
    "        return x_bin*self.n_vbins + v_bin\n",
    "\n",
    "    \n",
    "   \n",
    "    def update(self, state, action, next_state, reward):\n",
    "        discr_state, discr_next_state = self.discretize(state), self.discretize(next_state)\n",
    "\n",
    "        self.visited_state_action.add((discr_state,action))\n",
    "        self.N[discr_state,action, discr_next_state] += 1\n",
    "\n",
    "        \n",
    "        total_visited = self.N[discr_state,action,:].sum()\n",
    "\n",
    "        if total_visited > 0:\n",
    "            self.P[discr_state, action, :] = self.N[discr_state, action,  :] / total_visited\n",
    "            self.R[discr_state, action] = (self.R[discr_state, action]*(total_visited-1) + reward) / total_visited\n",
    "            \n",
    "        start = time.time()\n",
    "\n",
    "        if discr_state < self.terminal_x_bin:\n",
    "            self.Q[discr_state, action] = reward + (self.gamma)*(self.P[discr_state, action,:]*np.max(self.Q, axis = 1)[:]).sum()\n",
    "        else:\n",
    "            self.Q[discr_state, action] = reward\n",
    "\n",
    "        self.lossHistory[self.up_count]= self.Q[discr_state, action]\n",
    "        self.up_count += 1\n",
    "        \n",
    "        if not self.k_fixed:\n",
    "            self.k = len(self.visited_state_action) // 10\n",
    "            print(\"K changes\")\n",
    "            \n",
    "        sampled_states = []\n",
    "        if self.k >= 1:\n",
    "            sampled_states = random.choices(list(self.visited_state_action), k = self.k)\n",
    "\n",
    "            for (random_state, random_action) in sampled_states:\n",
    "                if random_state < self.terminal_x_bin:\n",
    "                    self.Q[random_state, random_action] = self.R[random_state, random_action] + (self.gamma)*(self.P[random_state, random_action,:]*np.max(self.Q, axis = 1)[:]).sum()\n",
    "                else:\n",
    "                    self.Q[random_state, random_action] = self.R[random_state, random_action]\n",
    "\n",
    "                self.lossHistory[self.up_count]= self.Q[random_state, random_action]\n",
    "                self.up_count += 1\n",
    "        #print(time.time() - start)\n",
    "\n",
    "    def observe(self):\n",
    "        pass\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state_bin = self.discretize(state)\n",
    "        p = random.uniform(0,1)\n",
    "        a=0\n",
    "        if p < 1-self.epsilon :\n",
    "            a = np.argmax(self.Q[state_bin,:])\n",
    "        else:\n",
    "            a = random.randint(0,2)\n",
    "            \n",
    "        return a\n",
    "    '''\n",
    "    Select actions without exploration (for the tests)\n",
    "    '''  \n",
    "    def select_best_action(self, state):\n",
    "        state_bin = self.discretize(state)\n",
    "        return np.argmax(self.Q[state_bin,:])\n",
    "    '''\n",
    "    Test the agent on a seed (random or not) after the training\n",
    "    '''  \n",
    "    def play(self, seed = False):\n",
    "        newSeed = random.randint(0,100000)\n",
    "        \n",
    "        if seed != False:\n",
    "            newSeed = seed\n",
    "            \n",
    "        state,_ = self.env.reset(seed = newSeed)\n",
    "        done = False\n",
    "                    \n",
    "        while done == False:\n",
    "                                        \n",
    "            action = self.select_best_action(state)\n",
    "            print(action)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "        self.env.close()\n",
    "\n",
    "    \"\"\"\n",
    "    # Reinitialisation de R nécessaire ?\n",
    "    def reset_for_episode(self):\n",
    "        self.N_episode = np.zeros(shape=(self.n_states, self.n_actions, self.n_states))\n",
    "        \n",
    "        self.W_episode = - np.ones(shape=(self.n_states, self.n_actions))\n",
    "        terminal_x_bin = self.discretize_x(0.5)\n",
    "        for state in range(terminal_x_bin, self.n_states):\n",
    "            self.W_episode[state, :] = 0\n",
    "\n",
    "    \"\"\"  \n",
    "\n",
    "    \n",
    "    def train(self, episodes, debug_mode=True, epsilon_decrease=True, epsilonDecreasing=100):\n",
    "        episodesHistory = np.zeros((episodes))\n",
    "        self.lossHistory = np.zeros((int(episodes*200*(self.k+1))))\n",
    "        self.up_count = 0\n",
    "        for i in range(episodes):\n",
    "\n",
    "            if epsilon_decrease: \n",
    "                if self.epsilon > self.epsilonMin:\n",
    "                    self.epsilon = self.epsilonMax*math.exp(-i/epsilonDecreasing)\n",
    "            else:\n",
    "                self.epsilon = self.epsilonMax\n",
    "                \n",
    "            newSeed = random.randint(0,100000)\n",
    "            state,_ = self.env.reset(seed = newSeed)\n",
    "            \n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "            \n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "\n",
    "                self.update(state, action, next_state, reward)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "                if terminated: print(\"Terminated\")\n",
    "\n",
    "            #self.reset_for_episode()\n",
    "\n",
    "             \n",
    "            if debug_mode: print(\"Episode \"+str(i+1)+ \" , Reward: \"+str(episode_reward)+\" Epsilon: \"+str(self.epsilon))\n",
    "            episodesHistory[i] = episode_reward\n",
    "        return episodesHistory, self.Q.mean(), self.R.mean(), np.max(A.P, axis=2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab3a1265-b585-4fea-bd39-bc8652333bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "[0.6  0.07]\n",
      "[-1.2  -0.07]\n",
      "7\n",
      "476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:00<00:44,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 , Reward: -200.0 Epsilon: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:01<00:41,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2 , Reward: -200.0 Epsilon: 0.8910448503742513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:02<00:41,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3 , Reward: -200.0 Epsilon: 0.8821788059760798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:03<00:38,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4 , Reward: -200.0 Epsilon: 0.8734009801936573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:04<00:35,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5 , Reward: -200.0 Epsilon: 0.8647104952370909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:04<00:34,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6 , Reward: -200.0 Epsilon: 0.8561064820506427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [00:05<00:32,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7 , Reward: -200.0 Epsilon: 0.8475880802258239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [00:06<00:31,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8 , Reward: -200.0 Epsilon: 0.8391544379153535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [00:07<00:30,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9 , Reward: -200.0 Epsilon: 0.8308047117479722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [00:07<00:29,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 , Reward: -200.0 Epsilon: 0.8225380667441053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:08<00:28,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11 , Reward: -200.0 Epsilon: 0.8143536762323635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [00:09<00:27,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12 , Reward: -200.0 Epsilon: 0.8062507217668754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [00:10<00:27,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13 , Reward: -200.0 Epsilon: 0.7982283930454418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [00:10<00:26,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14 , Reward: -200.0 Epsilon: 0.7902858878285052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [00:11<00:26,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15 , Reward: -200.0 Epsilon: 0.7824224118589252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [00:12<00:25,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16 , Reward: -200.0 Epsilon: 0.774637178782552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [00:12<00:24,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17 , Reward: -200.0 Epsilon: 0.7669294100695903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [00:13<00:23,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18 , Reward: -200.0 Epsilon: 0.7592983349367454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [00:14<00:22,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19 , Reward: -200.0 Epsilon: 0.7517431902701448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [00:15<00:22,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20 , Reward: -200.0 Epsilon: 0.744263220549026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [00:15<00:21,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 21 , Reward: -200.0 Epsilon: 0.7368576777701836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [00:16<00:22,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 22 , Reward: -200.0 Epsilon: 0.7295258213731683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [00:17<00:22,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 23 , Reward: -200.0 Epsilon: 0.7222669181662307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [00:18<00:21,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 24 , Reward: -200.0 Epsilon: 0.7150802422530007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [00:19<00:19,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 25 , Reward: -200.0 Epsilon: 0.7079650749598981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [00:20<00:18,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 26 , Reward: -200.0 Epsilon: 0.7009207047642644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [00:20<00:17,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 27 , Reward: -200.0 Epsilon: 0.6939464272232096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [00:21<00:16,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 28 , Reward: -200.0 Epsilon: 0.6870415449031678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [00:22<00:15,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 29 , Reward: -200.0 Epsilon: 0.6802053673101529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [00:22<00:14,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 30 , Reward: -200.0 Epsilon: 0.6734372108207087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [00:23<00:13,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 31 , Reward: -200.0 Epsilon: 0.666736398613546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [00:24<00:13,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 32 , Reward: -200.0 Epsilon: 0.6601022606018603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [00:25<00:12,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 33 , Reward: -200.0 Epsilon: 0.6535341333663218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [00:25<00:11,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 34 , Reward: -200.0 Epsilon: 0.6470313600887335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [00:26<00:11,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 35 , Reward: -200.0 Epsilon: 0.6405932904863487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [00:27<00:10,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 36 , Reward: -200.0 Epsilon: 0.6342192807468421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [00:28<00:09,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 37 , Reward: -200.0 Epsilon: 0.627908693463928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [00:28<00:09,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 38 , Reward: -200.0 Epsilon: 0.6216608975736192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [00:29<00:08,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 39 , Reward: -200.0 Epsilon: 0.6154752682911203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [00:30<00:07,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 40 , Reward: -200.0 Epsilon: 0.6093511870483482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [00:31<00:06,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 41 , Reward: -200.0 Epsilon: 0.6032880414320754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [00:32<00:06,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 42 , Reward: -200.0 Epsilon: 0.5972852251226874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [00:33<00:05,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 43 , Reward: -200.0 Epsilon: 0.591342137833551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [00:34<00:05,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 44 , Reward: -200.0 Epsilon: 0.5854581852509849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [00:34<00:04,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 45 , Reward: -200.0 Epsilon: 0.5796327789748272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [00:35<00:03,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 46 , Reward: -200.0 Epsilon: 0.573865336459596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [00:36<00:02,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 47 , Reward: -200.0 Epsilon: 0.5681552809562334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [00:37<00:01,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 48 , Reward: -200.0 Epsilon: 0.5625020414544307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [00:37<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 49 , Reward: -200.0 Epsilon: 0.5569050526255268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:38<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50 , Reward: -200.0 Epsilon: 0.5513637547659745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "        -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "        -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "        -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "        -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "        -200., -200., -200., -200., -200.]),\n",
       " -8.38010683879843,\n",
       " -1.0,\n",
       " 0.2340591788909888)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = DynaAgent(\"id0\", k = 200)\n",
    "A.train(50, debug_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08fd16f2-9498-4806-873f-c0ca4969bc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "7\n",
      "476\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "Dyna_Test = DynaAgent(\"idTest\", env=gym.make('MountainCar-v0', render_mode='human'))\n",
    "Dyna_Test.Q = A.Q\n",
    "Dyna_Test.play(seed = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bd389a-7f50-449b-85aa-00527071c7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a59d81-bb3c-4b7d-8a07-62b8c25dcbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Prof\n",
    "\n",
    "def value_iteration(self, theta=0.001):\n",
    "    \"\"\"\n",
    "    P : 3D array representing transition probabilities, P[s,a,s'] is the probability of transitioning from s to s' under action a.\n",
    "    R : 2D array representing rewards for each state-action pair\n",
    "    gamma : discount factor\n",
    "    theta : stopping criterion\n",
    "    \"\"\"\n",
    "    V = np.zeros(self.n_states)\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(self.n_states):\n",
    "            v = V[s]\n",
    "            # Calculate the value of each action in the current state\n",
    "            action_values = np.zeros(self.n_actions)\n",
    "            for a in range(self.n_actions):\n",
    "                action_values[a] = np.sum(self.P[s, a] * (self.R[s, a] + self.gamma * v))\n",
    "            # Update the value function\n",
    "            V[s] = np.max(action_values)\n",
    "            # Update the change in value function\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "\n",
    "        # If the change in value function is smaller than theta, stop\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    policy = np.zeros(self.n_states, dtype=int)\n",
    "    for s in range(self.n_states):\n",
    "        # Calculate the value of each action in the current state\n",
    "        action_values = np.zeros(self.n_actions)\n",
    "        for a in range(self.n_actions):\n",
    "            action_values[a] = np.sum(self.P[s, a] * (self.R[s, a] + self.gamma * V))\n",
    "        # Choose the action with the maximum value\n",
    "        policy[s] = np.argmax(action_values)\n",
    "    \n",
    "    return policy, V"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
