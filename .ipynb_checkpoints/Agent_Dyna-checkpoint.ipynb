{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9968fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb52ffe-7290-4c94-8850-db3c480348f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def select_action(self, state):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, episodes, debug_mode=False):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, id, env):\n",
    "        self.id = id\n",
    "        self.env = env\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da8dc4c5-d0f8-4e79-b8ed-217f095a6532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([72., 28.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discr_step = np.array([0.025, 0.005])\n",
    "env = gym.make('MountainCar-v0')\n",
    "(env.observation_space.high - env.observation_space.low)//discr_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "345c3e60-bf01-42b8-968f-e8cf05543aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cf code cours, après avoir obtenu un modèle de l'environnement, résoudre le porblème d'optimisation avec le dynamic programming\n",
    "\n",
    "class DynaAgent(Agent):\n",
    "    \n",
    "    def __init__(self, id, env=gym.make('MountainCar-v0'), epsilonMax = 0.9, epsilonMin = 0.05, discr_step = np.array([0.025, 0.02]), gamma = 0.99, k=0, k_fixed = True, alpha=0.2, observation_SIZE = 6, replay_buffer_SIZE = 10000):\n",
    "        Agent.__init__(self,id,env)\n",
    "        self.discr_step = discr_step\n",
    "        self.n_xbins = np.round(((self.env.observation_space.high - self.env.observation_space.low)/self.discr_step)[0]).astype(np.int32)\n",
    "        print(self.n_xbins)\n",
    "        self.n_vbins = np.round(((self.env.observation_space.high - self.env.observation_space.low)/self.discr_step)[1]).astype(np.int32)\n",
    "        print(self.n_vbins)\n",
    "        self.n_states = self.n_xbins*self.n_vbins\n",
    "        self.n_actions = 3\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilonMax\n",
    "        self.epsilonMax = epsilonMax\n",
    "        self.epsilonMin = epsilonMin\n",
    "        self.k = k\n",
    "        self.k_fixed = k_fixed\n",
    "        self.alpha = alpha\n",
    "        '''\n",
    "        Definition for the plots\n",
    "        '''        \n",
    "        self.max_x = -2 \n",
    "        self.min_x = 1 \n",
    "        self.max_v = -0.1\n",
    "        self.min_v = 0.1\n",
    "        self.time_episodes = []\n",
    "        self.visited_states = []\n",
    "        self.important_episodes = []\n",
    "        '''\n",
    "        Definition of the replay buffer\n",
    "        '''\n",
    "        self.replay_buffer_SIZE = replay_buffer_SIZE\n",
    "        self.observation_SIZE = observation_SIZE\n",
    "        self.visited_state_action_Array = np.zeros((replay_buffer_SIZE,observation_SIZE))\n",
    "        self.visited_state_action = set()\n",
    "        \n",
    "        self.N = np.zeros((self.n_states, self.n_actions, self.n_states))\n",
    "        \n",
    "        self.P = np.zeros(shape=(self.n_states, self.n_actions, self.n_states))\n",
    "        for i in range(self.n_states):\n",
    "            for j in range(self.n_actions):\n",
    "                random = np.random.rand(self.n_states)\n",
    "                self.P[i, j, :] = random/random.sum()\n",
    "        \n",
    "        self.R = - np.ones(shape=(self.n_states, self.n_actions))\n",
    "        self.terminal_x_bin = self.discretize_x(0.5)*self.n_vbins\n",
    "        print(self.terminal_x_bin)\n",
    "        \n",
    "        self.Q = np.zeros(shape=(self.n_states, self.n_actions))\n",
    "\n",
    "    # On obtient les s du (s, a, s') en discrétisant (cf plus haut), puis pour les s' on utilise une loi uniforme pour chaque paire (s, a)\n",
    "\n",
    "    def discretize_x(self, x):\n",
    "        x_bin = np.round(((x - self.env.observation_space.low)/self.discr_step)[0]).astype(np.int32)\n",
    "        return x_bin\n",
    "\n",
    "    def discretize_v(self, v):\n",
    "        v_bin = np.round(((v - self.env.observation_space.low)/self.discr_step)[1]).astype(np.int32)\n",
    "        return v_bin \n",
    "\n",
    "    def discretize(self, state):\n",
    "        x_bin = self.discretize_x(state[0])\n",
    "        v_bin = self.discretize_v(state[1])\n",
    "        return x_bin*self.n_vbins + v_bin\n",
    "\n",
    "    \n",
    "   \n",
    "    def update(self, state, action, next_state, reward):\n",
    "        discr_state, discr_next_state = self.discretize(state), self.discretize(next_state)\n",
    "\n",
    "        self.visited_states.append(state)\n",
    "        self.visited_state_action.add((discr_state,action))\n",
    "        self.N[discr_state,action, discr_next_state] += 1\n",
    "\n",
    "        \n",
    "        total_visited = self.N[discr_state,action,:].sum()\n",
    "\n",
    "        if total_visited > 0:\n",
    "            self.P[discr_state, action, :] = self.N[discr_state, action,  :] / total_visited\n",
    "            self.R[discr_state, action] = (self.R[discr_state, action]*(total_visited-1) + reward) / total_visited\n",
    "            \n",
    "        start = time.time()\n",
    "\n",
    "        if discr_state < self.terminal_x_bin:\n",
    "            self.Q[discr_state, action] = reward + (self.gamma)*(self.P[discr_state, action,:]*np.max(self.Q, axis = 1)[:]).sum()\n",
    "        else:\n",
    "            self.Q[discr_state, action] = reward\n",
    "        \n",
    "        if not self.k_fixed:\n",
    "            self.k = len(self.visited_state_action) // 10\n",
    "            print(\"K changes\")\n",
    "            \n",
    "        sampled_states = []\n",
    "        if self.k >= 1:\n",
    "            sampled_states = random.choices(list(self.visited_state_action), k = self.k)\n",
    "\n",
    "            for (random_state, random_action) in sampled_states:\n",
    "                if random_state < self.terminal_x_bin:\n",
    "                    self.Q[random_state, random_action] = self.R[random_state, random_action] + (self.gamma)*(self.P[random_state, random_action,:]*np.max(self.Q, axis = 1)[:]).sum()\n",
    "                else:\n",
    "                    self.Q[random_state, random_action] = self.R[random_state, random_action]\n",
    "\n",
    "        #print(time.time() - start)\n",
    "\n",
    "    def observe(self):\n",
    "        pass\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state_bin = self.discretize(state)\n",
    "        p = random.uniform(0,1)\n",
    "        a=0\n",
    "        if p < 1-self.epsilon :\n",
    "            a = np.argmax(self.Q[state_bin,:])\n",
    "        else:\n",
    "            a = random.randint(0,2)\n",
    "            \n",
    "        return a\n",
    "    '''\n",
    "    Select actions without exploration (for the tests)\n",
    "    '''  \n",
    "    def select_best_action(self, state):\n",
    "        state_bin = self.discretize(state)\n",
    "        return np.argmax(self.Q[state_bin,:])\n",
    "    '''\n",
    "    Test the agent on a seed (random or not) after the training\n",
    "    '''  \n",
    "    def play(self, seed = False):\n",
    "        newSeed = random.randint(0,100000)\n",
    "        \n",
    "        if seed != False:\n",
    "            newSeed = seed\n",
    "            \n",
    "        state,_ = self.env.reset(seed = newSeed)\n",
    "        done = False\n",
    "                    \n",
    "        while done == False:\n",
    "                                        \n",
    "            action = self.select_best_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "        self.env.close()\n",
    "\n",
    "    \"\"\"\n",
    "    # Reinitialisation de R nécessaire ?\n",
    "    def reset_for_episode(self):\n",
    "        self.N_episode = np.zeros(shape=(self.n_states, self.n_actions, self.n_states))\n",
    "        \n",
    "        self.W_episode = - np.ones(shape=(self.n_states, self.n_actions))\n",
    "        terminal_x_bin = self.discretize_x(0.5)\n",
    "        for state in range(terminal_x_bin, self.n_states):\n",
    "            self.W_episode[state, :] = 0\n",
    "\n",
    "    \"\"\"  \n",
    "\n",
    "    \n",
    "    def train(self, episodes, debug_mode=True, epsilon_decrease=True, epsilonDecreasing=100):\n",
    "        episodesHistory = np.zeros((episodes))\n",
    "        for i in range(episodes):\n",
    "            start_time = time.time()\n",
    "            if epsilon_decrease: \n",
    "                if self.epsilon > self.epsilonMin:\n",
    "                    self.epsilon = self.epsilonMax*math.exp(-i/epsilonDecreasing)\n",
    "            else:\n",
    "                self.epsilon = self.epsilonMax\n",
    "                \n",
    "            newSeed = random.randint(0,100000)\n",
    "            state,_ = self.env.reset(seed = newSeed)\n",
    "            \n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            actual_episode_for_Q = []\n",
    "            \n",
    "            while not done:\n",
    "                \n",
    "                if self.max_x < state[0]:\n",
    "                    self.max_x = state[0]\n",
    "                if self.min_x > state[0]:\n",
    "                    self.min_x = state[0]\n",
    "\n",
    "                if self.max_v < state[1]:\n",
    "                    self.max_v = state[1]\n",
    "                if self.min_v > state[1]:\n",
    "                    self.min_v = state[1]\n",
    "                    \n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "\n",
    "                self.update(state, action, next_state, reward)\n",
    "                \n",
    "                discr_state = self.discretize(state)\n",
    "                max_value = np.max(self.Q[discr_state, :])\n",
    "                actual_episode_for_Q.append((state, max_value))\n",
    "                                             \n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "                if terminated: print(\"Terminated\")\n",
    "\n",
    "            stop_time = time.time()\n",
    "            episode_time = stop_time-start_time\n",
    "            \n",
    "            self.time_episodes.append(episode_time)\n",
    "            if (i%5 == 0) or episode_reward > -200:  # episodes marquants considérés : tous les 100 eps, ou quand ça se termine\n",
    "                self.important_episodes.append((actual_episode_for_Q, episode_time, episode_reward))\n",
    "                \n",
    "            if debug_mode: print(\"Episode \"+str(i+1)+ \" , Reward: \"+str(episode_reward)+\" Epsilon: \"+str(self.epsilon))\n",
    "            episodesHistory[i] = episode_reward\n",
    "        return episodesHistory\n",
    "\n",
    "    def three_dimensions_Q_plot(self, with_important_episodes = False):\n",
    "    \n",
    "        list_states = self.visited_states\n",
    "        x = np.array([state[0] for state in list_states])\n",
    "        v = np.array([state[1] for state in list_states])\n",
    "\n",
    "        discretized_states = [self.discretize(state) for state in list_states]\n",
    "        max_q_values = [max(self.Q[discr_state, :]) for discr_state in discretized_states]\n",
    "\n",
    "    \n",
    "        fig = go.Figure(data=[go.Scatter3d(\n",
    "            x=x,\n",
    "            y=v,\n",
    "            z=max_q_values,\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=5,\n",
    "                color=max_q_values,  \n",
    "                colorscale='Viridis',\n",
    "                colorbar=dict(\n",
    "                title='Q_max Values'  \n",
    "                ),\n",
    "            opacity=0.8\n",
    "            )\n",
    "        )])\n",
    "\n",
    "\n",
    "        if with_important_episodes:\n",
    "\n",
    "            states_ep = [value[0] for value in self.important_episodes]\n",
    "\n",
    "            for state_ep in states_ep:\n",
    "                x_ep = [state[0][0] for state in state_ep]\n",
    "                v_ep = [state[0][1] for state in state_ep]\n",
    "                max_q_ep = [state[1] for state in state_ep]\n",
    "            \n",
    "                fig.add_trace(go.Scatter3d(\n",
    "                    x=x_ep,\n",
    "                    y=v_ep,\n",
    "                    z=max_q_ep,\n",
    "                    mode='lines',  \n",
    "                    line=dict(\n",
    "                        color='black', \n",
    "                        width=6\n",
    "                    )\n",
    "                ))\n",
    "\n",
    "            \n",
    "\n",
    "        fig.update_layout(\n",
    "            title='Max Q(s,a) as a function of s = (x, v)',\n",
    "            autosize=False,\n",
    "            width=1000, \n",
    "            height=800,  \n",
    "            margin=dict(l=65, r=50, b=65, t=90),\n",
    "            scene=dict(\n",
    "                xaxis_title='Values of x',\n",
    "                yaxis_title='Values of v',\n",
    "                zaxis_title='Values of Q_max'\n",
    "            ),\n",
    "            legend_title_text='Trajectories'\n",
    "        )\n",
    "\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab3a1265-b585-4fea-bd39-bc8652333bfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "7\n",
      "476\n",
      "Episode 1 , Reward: -200.0 Epsilon: 0.9\n",
      "Episode 2 , Reward: -200.0 Epsilon: 0.8910448503742513\n",
      "Episode 3 , Reward: -200.0 Epsilon: 0.8821788059760798\n",
      "Episode 4 , Reward: -200.0 Epsilon: 0.8734009801936573\n",
      "Episode 5 , Reward: -200.0 Epsilon: 0.8647104952370909\n",
      "Episode 6 , Reward: -200.0 Epsilon: 0.8561064820506427\n",
      "Episode 7 , Reward: -200.0 Epsilon: 0.8475880802258239\n",
      "Episode 8 , Reward: -200.0 Epsilon: 0.8391544379153535\n",
      "Episode 9 , Reward: -200.0 Epsilon: 0.8308047117479722\n",
      "Episode 10 , Reward: -200.0 Epsilon: 0.8225380667441053\n",
      "Episode 11 , Reward: -200.0 Epsilon: 0.8143536762323635\n",
      "Episode 12 , Reward: -200.0 Epsilon: 0.8062507217668754\n",
      "Episode 13 , Reward: -200.0 Epsilon: 0.7982283930454418\n",
      "Episode 14 , Reward: -200.0 Epsilon: 0.7902858878285052\n",
      "Episode 15 , Reward: -200.0 Epsilon: 0.7824224118589252\n",
      "Episode 16 , Reward: -200.0 Epsilon: 0.774637178782552\n",
      "Episode 17 , Reward: -200.0 Epsilon: 0.7669294100695903\n",
      "Episode 18 , Reward: -200.0 Epsilon: 0.7592983349367454\n",
      "Episode 19 , Reward: -200.0 Epsilon: 0.7517431902701448\n",
      "Episode 20 , Reward: -200.0 Epsilon: 0.744263220549026\n",
      "Episode 21 , Reward: -200.0 Epsilon: 0.7368576777701836\n",
      "Episode 22 , Reward: -200.0 Epsilon: 0.7295258213731683\n",
      "Episode 23 , Reward: -200.0 Epsilon: 0.7222669181662307\n",
      "Episode 24 , Reward: -200.0 Epsilon: 0.7150802422530007\n",
      "Episode 25 , Reward: -200.0 Epsilon: 0.7079650749598981\n",
      "Episode 26 , Reward: -200.0 Epsilon: 0.7009207047642644\n",
      "Episode 27 , Reward: -200.0 Epsilon: 0.6939464272232096\n",
      "Episode 28 , Reward: -200.0 Epsilon: 0.6870415449031678\n",
      "Episode 29 , Reward: -200.0 Epsilon: 0.6802053673101529\n",
      "Episode 30 , Reward: -200.0 Epsilon: 0.6734372108207087\n",
      "Episode 31 , Reward: -200.0 Epsilon: 0.666736398613546\n",
      "Episode 32 , Reward: -200.0 Epsilon: 0.6601022606018603\n",
      "Episode 33 , Reward: -200.0 Epsilon: 0.6535341333663218\n",
      "Episode 34 , Reward: -200.0 Epsilon: 0.6470313600887335\n",
      "Episode 35 , Reward: -200.0 Epsilon: 0.6405932904863487\n",
      "Episode 36 , Reward: -200.0 Epsilon: 0.6342192807468421\n",
      "Episode 37 , Reward: -200.0 Epsilon: 0.627908693463928\n",
      "Episode 38 , Reward: -200.0 Epsilon: 0.6216608975736192\n",
      "Episode 39 , Reward: -200.0 Epsilon: 0.6154752682911203\n",
      "Episode 40 , Reward: -200.0 Epsilon: 0.6093511870483482\n",
      "Episode 41 , Reward: -200.0 Epsilon: 0.6032880414320754\n",
      "Episode 42 , Reward: -200.0 Epsilon: 0.5972852251226874\n",
      "Episode 43 , Reward: -200.0 Epsilon: 0.591342137833551\n",
      "Episode 44 , Reward: -200.0 Epsilon: 0.5854581852509849\n",
      "Episode 45 , Reward: -200.0 Epsilon: 0.5796327789748272\n",
      "Episode 46 , Reward: -200.0 Epsilon: 0.573865336459596\n",
      "Episode 47 , Reward: -200.0 Epsilon: 0.5681552809562334\n",
      "Episode 48 , Reward: -200.0 Epsilon: 0.5625020414544307\n",
      "Episode 49 , Reward: -200.0 Epsilon: 0.5569050526255268\n",
      "Episode 50 , Reward: -200.0 Epsilon: 0.5513637547659745\n",
      "Episode 51 , Reward: -200.0 Epsilon: 0.5458775937413701\n",
      "Episode 52 , Reward: -200.0 Epsilon: 0.5404460209310393\n",
      "Episode 53 , Reward: -200.0 Epsilon: 0.535068493173175\n",
      "Episode 54 , Reward: -200.0 Epsilon: 0.5297444727105197\n",
      "Episode 55 , Reward: -200.0 Epsilon: 0.5244734271365907\n",
      "Episode 56 , Reward: -200.0 Epsilon: 0.519254829342438\n",
      "Episode 57 , Reward: -200.0 Epsilon: 0.5140881574639334\n",
      "Episode 58 , Reward: -200.0 Epsilon: 0.5089728948295834\n",
      "Episode 59 , Reward: -200.0 Epsilon: 0.5039085299088618\n",
      "Episode 60 , Reward: -200.0 Epsilon: 0.4988945562610564\n",
      "Episode 61 , Reward: -200.0 Epsilon: 0.49393047248462385\n",
      "Episode 62 , Reward: -200.0 Epsilon: 0.48901578216704983\n",
      "Episode 63 , Reward: -200.0 Epsilon: 0.484149993835207\n",
      "Episode 64 , Reward: -200.0 Epsilon: 0.4793326209062075\n",
      "Episode 65 , Reward: -200.0 Epsilon: 0.4745631816387437\n",
      "Episode 66 , Reward: -200.0 Epsilon: 0.46984119908491445\n",
      "Episode 67 , Reward: -200.0 Epsilon: 0.46516620104252926\n",
      "Episode 68 , Reward: -200.0 Epsilon: 0.46053772000788823\n",
      "Episode 69 , Reward: -200.0 Epsilon: 0.4559552931290306\n",
      "Episode 70 , Reward: -200.0 Epsilon: 0.45141846215945003\n",
      "Episode 71 , Reward: -200.0 Epsilon: 0.4469267734122686\n",
      "Episode 72 , Reward: -200.0 Epsilon: 0.4424797777148686\n",
      "Episode 73 , Reward: -200.0 Epsilon: 0.4380770303639745\n",
      "Episode 74 , Reward: -200.0 Epsilon: 0.4337180910811822\n",
      "Episode 75 , Reward: -200.0 Epsilon: 0.42940252396893097\n",
      "Episode 76 , Reward: -200.0 Epsilon: 0.4251298974669132\n",
      "Episode 77 , Reward: -200.0 Epsilon: 0.42089978430891833\n",
      "Episode 78 , Reward: -200.0 Epsilon: 0.41671176148010525\n",
      "Episode 79 , Reward: -200.0 Epsilon: 0.41256541017470116\n",
      "Episode 80 , Reward: -200.0 Epsilon: 0.40846031575412023\n",
      "Episode 81 , Reward: -200.0 Epsilon: 0.4043960677054994\n",
      "Episode 82 , Reward: -200.0 Epsilon: 0.400372259600647\n",
      "Episode 83 , Reward: -200.0 Epsilon: 0.39638848905539936\n",
      "Episode 84 , Reward: -200.0 Epsilon: 0.3924443576893821\n",
      "Episode 85 , Reward: -200.0 Epsilon: 0.38853947108617176\n",
      "Episode 86 , Reward: -200.0 Epsilon: 0.38467343875385407\n",
      "Episode 87 , Reward: -200.0 Epsilon: 0.3808458740859739\n",
      "Episode 88 , Reward: -200.0 Epsilon: 0.37705639432287513\n",
      "Episode 89 , Reward: -200.0 Epsilon: 0.3733046205134232\n",
      "Episode 90 , Reward: -200.0 Epsilon: 0.369590177477111\n",
      "Episode 91 , Reward: -200.0 Epsilon: 0.36591269376653923\n",
      "Episode 92 , Reward: -200.0 Epsilon: 0.3622718016302724\n",
      "Episode 93 , Reward: -200.0 Epsilon: 0.35866713697606273\n",
      "Episode 94 , Reward: -200.0 Epsilon: 0.35509833933444096\n",
      "Episode 95 , Reward: -200.0 Epsilon: 0.351565051822669\n",
      "Episode 96 , Reward: -200.0 Epsilon: 0.34806692110905113\n",
      "Episode 97 , Reward: -200.0 Epsilon: 0.3446035973776009\n",
      "Episode 98 , Reward: -200.0 Epsilon: 0.34117473429305895\n",
      "Episode 99 , Reward: -200.0 Epsilon: 0.3377799889662596\n",
      "Episode 100 , Reward: -200.0 Epsilon: 0.33441902191984113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "       -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "       -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "       -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "       -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "       -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "       -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "       -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "       -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "       -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "       -200., -200., -200., -200., -200., -200., -200., -200., -200.,\n",
       "       -200.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = DynaAgent(\"id0\", k = 200)\n",
    "A.train(100, debug_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08fd16f2-9498-4806-873f-c0ca4969bc5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "[0.6  0.07]\n",
      "[-1.2  -0.07]\n",
      "7\n",
      "476\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "Dyna_Test = DynaAgent(\"idTest\", env=gym.make('MountainCar-v0', render_mode='human'))\n",
    "Dyna_Test.Q = A.Q\n",
    "episodesHistory = Dyna_Test.play(seed = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453bf34a-ae2a-4b9f-a37f-f5e4ae91fda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variation_size_bins_plot(coeff, number_points, number_train_for_each_point, change_for_x=False, change_for_v=False, mult=False, div=False):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Cette fonction trace les plots avec :\n",
    "    - en abscisse le nombre d'épisodes par pas de 50\n",
    "    - en ordonnée, soit le x, soit le v maximal (ou minimal) atteint tous les 50 épisodes \n",
    "    (pour un total de 4 plots donc)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    discr_step_ref = np.array([0.025, 0.005])\n",
    "\n",
    "    X_plot = np.arange(0, number_points*number_train_for_each_point, number_train_for_each_point)\n",
    "    \n",
    "    Y_max_x_episodes = []\n",
    "    Y_min_x_episodes = []\n",
    "\n",
    "    Y_max_v_episodes = []\n",
    "    Y_min_v_episodes = []\n",
    "\n",
    "    if change_for_x:\n",
    "        if mult:\n",
    "            discr_step_ref[0]*=coeff\n",
    "        else:\n",
    "            if div:\n",
    "                discr_step_ref[0]/=coeff\n",
    "            \n",
    "    if change_for_v:\n",
    "        if mult:\n",
    "            discr_step_ref[1]*=coeff\n",
    "        else :\n",
    "            if div:\n",
    "                discr_step_ref[1]/=coeff\n",
    "\n",
    "    # print(discr_step_ref)\n",
    "    Dyn = DynaAgent(\"idPlot\", discr_step = discr_step_ref)\n",
    "    print((Dyn.n_xbins, Dyn.n_vbins))\n",
    "\n",
    "    for _ in range(number_points):\n",
    "\n",
    "        Dyn.train(number_train_for_each_point) # nombre d'étapes avant de regarder l'état max atteint\n",
    "\n",
    "        print(Y_max_x_episodes)\n",
    "        Y_max_x_episodes.append(Dyn.max_x)\n",
    "        Y_min_x_episodes.append(Dyn.min_x)\n",
    "\n",
    "        Y_max_v_episodes.append(Dyn.max_v)\n",
    "        Y_min_v_episodes.append(Dyn.min_v)\n",
    "\n",
    "    return (X_plot,  Y_max_x_episodes, Y_max_v_episodes,  Y_min_x_episodes,  Y_min_v_episodes)\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91704eec-8780-42a7-8c02-713b30702889",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_plot,  Y_max_x_episodes, Y_max_v_episodes,  Y_min_x_episodes,  Y_min_v_episodes) = variation_size_bins_plot(2, 10, 5, change_for_x=True, change_for_v=True, mult=True)\n",
    "\n",
    "\n",
    "fig, ((ax1a, ax1b), (ax2a, ax2b)) = plt.subplots(2, 2)\n",
    "\n",
    "ax1a.plot(X_plot, Y_max_x_episodes)\n",
    "ax1a.set_xlabel(r'Number of episode')\n",
    "ax1a.set_ylabel(r'Max x reached')\n",
    "ax1a.set_title(r'Farthest x reached by number of episodes')  \n",
    "\n",
    "ax1b.plot(X_plot, Y_min_x_episodes)\n",
    "ax1b.set_xlabel(r'Number of episode')\n",
    "ax1b.set_ylabel(r'Min x reached')\n",
    "ax1b.set_title(r'Smallest x reached by number of episodes') \n",
    "\n",
    "ax2a.plot(X_plot, Y_max_v_episodes)\n",
    "ax2a.set_xlabel(r'Number of episode')\n",
    "ax2a.set_ylabel(r'Max v reached')\n",
    "ax2a.set_title(r'Farthest v reached by number of episodes')  \n",
    "\n",
    "\n",
    "ax2b.plot(X_plot, Y_min_v_episodes)\n",
    "ax2b.set_xlabel(r'Number of episode')\n",
    "ax2b.set_ylabel(r'Min v reached')\n",
    "ax2b.set_title(r'Smallest v reached by number of episodes') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=2, hspace=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972cc180-8915-473f-b7e1-60d618016fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_plot,  Y_max_x_episodes, Y_max_v_episodes,  Y_min_x_episodes,  Y_min_v_episodes) = variation_size_bins_plot(2, 10, 5, change_for_x=True, change_for_v=True, div=True)\n",
    "\n",
    "\n",
    "fig, ((ax1a, ax1b), (ax2a, ax2b)) = plt.subplots(2, 2)\n",
    "\n",
    "ax1a.plot(X_plot, Y_max_x_episodes)\n",
    "ax1a.set_xlabel(r'Number of episode')\n",
    "ax1a.set_ylabel(r'Max x reached')\n",
    "ax1a.set_title(r'Farthest x reached by number of episodes')  \n",
    "\n",
    "ax1b.plot(X_plot, Y_min_x_episodes)\n",
    "ax1b.set_xlabel(r'Number of episode')\n",
    "ax1b.set_ylabel(r'Min x reached')\n",
    "ax1b.set_title(r'Smallest x reached by number of episodes') \n",
    "\n",
    "ax2a.plot(X_plot, Y_max_v_episodes)\n",
    "ax2a.set_xlabel(r'Number of episode')\n",
    "ax2a.set_ylabel(r'Max v reached')\n",
    "ax2a.set_title(r'Farthest v reached by number of episodes')  \n",
    "\n",
    "\n",
    "ax2b.plot(X_plot, Y_min_v_episodes)\n",
    "ax2b.set_xlabel(r'Number of episode')\n",
    "ax2b.set_ylabel(r'Min v reached')\n",
    "ax2b.set_title(r'Smallest v reached by number of episodes') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=2, hspace=0.5)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
