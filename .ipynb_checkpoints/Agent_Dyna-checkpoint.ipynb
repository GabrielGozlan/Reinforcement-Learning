{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9968fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78a654c6-f5ed-4da6-96e9-6a415baa0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MLP\n",
    "class Multi_Layer_Perceptron(nn.Sequential):\n",
    "    def __init__(self, input_dim, intern_dim, output_dim, depth = 2, isBiased = False):\n",
    "        \n",
    "        dict = OrderedDict([(\"input\",nn.Linear(input_dim,intern_dim, bias=isBiased))])\n",
    "        for i in range(depth):\n",
    "            dict.update({str(i) : nn.Linear(intern_dim,intern_dim,bias=isBiased)})\n",
    "        dict.update({\"output\" : nn.Linear(intern_dim,output_dim,bias=isBiased)})\n",
    "\n",
    "        super().__init__(dict)\n",
    "\n",
    "        self.reset_init_weights_biases(0) # so that we do not use a default initialization\n",
    "\n",
    "    def reset_init_weights_biases(self, norm = -1):\n",
    "        for layer in self.children():\n",
    "            if norm == -1:\n",
    "                stdv = 1. / math.sqrt(layer.weight.size(1))\n",
    "            else :\n",
    "                stdv = norm\n",
    "            layer.weight.data.fill_(stdv)\n",
    "            if layer.bias is not None:\n",
    "                layer.bias.data.fill_(stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdb52ffe-7290-4c94-8850-db3c480348f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def select_action(self, state):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, episodes, debug_mode=False):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, id, env):\n",
    "        self.id = id\n",
    "        self.env = env\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48ce199a-cc03-418a-bcf2-c205d954304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        pass\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        return random.randint(0,1)\n",
    "        \n",
    "        \n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, episodes, debug_mode=False):\n",
    "        episodesHistory = np.zeros((episodes))\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            if(debug_mode) : print(\"Episode: \"+str(i+1)+\" starts\")\n",
    "            newSeed = random.randint(0,100000)\n",
    "            state,_ = self.env.reset(seed = newSeed)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                                        \n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                self.observe(state,action,next_state,reward)\n",
    "                self.update()\n",
    "\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "\n",
    "            episodesHistory[i] = episode_reward\n",
    "            if(debug_mode) : print(\"Episode \"+str(i+1)+ \" , Reward: \"+str(episode_reward))\n",
    "        print(episodesHistory[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "228a8b98-8bed-4b33-9097-fa858886d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(Agent):\n",
    "\n",
    "    def __init__(self, id, epsilonMax, epsilonMin = 0.05, Q = Multi_Layer_Perceptron(input_dim = 3,intern_dim = 64, output_dim = 1, depth = 2, isBiased = False), env = gym.make('MountainCar-v0'), arrayNewPosReward = None, arrayNewVelReward = None, contReward = False, gamma = 0.99, replay_buffer_SIZE = 10000, batch_size = 64, observation_SIZE = 6, optimizer = torch.optim.AdamW):\n",
    "        Agent.__init__(self,id,env)\n",
    "        self.Q = Q\n",
    "        self.QTarget = Q\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilonMax\n",
    "        self.epsilonMax = epsilonMax\n",
    "        self.epsilonMin = epsilonMin\n",
    "        self.replay_buffer = np.zeros((replay_buffer_SIZE,observation_SIZE))\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer(self.Q.parameters())\n",
    "        self.replay_buffer_SIZE = replay_buffer_SIZE\n",
    "        self.observation_SIZE = observation_SIZE\n",
    "        self.arrayNewPosReward = arrayNewPosReward\n",
    "        self.arrayNewVelReward = arrayNewVelReward\n",
    "        self.contReward = contReward\n",
    "        \n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        state = np.array(state)\n",
    "        action = np.array([action])\n",
    "        next_state = np.array(next_state)\n",
    "        reward = np.array([reward])\n",
    "\n",
    "        concatenatation = np.concatenate((state, action, next_state, reward))\n",
    "        return concatenatation\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        P = random.uniform(0,1)\n",
    "        a=0\n",
    "        if P <= 1-self.epsilon :\n",
    "            A=np.zeros(3)\n",
    "            o = []\n",
    "            ArgQmax = 0\n",
    "            for k in range(3):\n",
    "                A[k] = self.Q(torch.from_numpy(np.concatenate((np.array(state), np.array([k])))).to(torch.float32))\n",
    "            a = np.argmax(A)\n",
    "            for k in range(3):\n",
    "                if A[k] == A[a]:\n",
    "                    o.append(k)\n",
    "            a =random.choice(o)\n",
    "                \n",
    "        else:\n",
    "            a = random.randint(0,2)\n",
    "        return a\n",
    "        \n",
    "        \n",
    "    def update(self,j):\n",
    "\n",
    "        LossFct = torch.nn.MSELoss(reduction='mean')\n",
    "        #print(min(j,self.replay_buffer.shape[0]))\n",
    "        batch = np.random.choice(min(j,self.replay_buffer.shape[0]), self.batch_size)\n",
    "        target = torch.zeros((self.batch_size))\n",
    "        input = torch.zeros((self.batch_size))\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "\n",
    "            if self.replay_buffer[batch[i],3] <0.5:\n",
    "                A0 = torch.from_numpy(np.concatenate((self.replay_buffer[batch[i],3:5], np.array([0])))).to(torch.float32)\n",
    "                A1 = torch.from_numpy(np.concatenate((self.replay_buffer[batch[i],3:5], np.array([1])))).to(torch.float32)\n",
    "                A2 = torch.from_numpy(np.concatenate((self.replay_buffer[batch[i],3:5], np.array([2])))).to(torch.float32)\n",
    "                target[i] = self.replay_buffer[batch[i],5] + self.gamma*max(self.QTarget(A0),self.QTarget(A1),self.QTarget(A2))\n",
    "            else:\n",
    "                target[i] = self.replay_buffer[batch[i],5]\n",
    "                \n",
    "            input[i] = self.Q(torch.from_numpy(self.replay_buffer[batch[i],:3]).to(torch.float32))\n",
    "\n",
    "        loss = LossFct(input, target)\n",
    "        for param in self.Q.parameters():\n",
    "            param.grad = None\n",
    "            \n",
    "        loss.backward()\n",
    "\n",
    "        grad = 0\n",
    "        for layer in self.Q.children():\n",
    "            grad += layer.weight.grad.mean()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        return loss.item(), abs(grad)\n",
    "\n",
    "    def customReward(self, state, action, currentReward, next_state, uniqueReward, excludePassivity):\n",
    "        reward = -1\n",
    "\n",
    "        if excludePassivity and action == 1:\n",
    "            reward -= 2\n",
    "\n",
    "        if self.contReward:\n",
    "            reward = (abs(next_state[0] + 0.5)**2)/200\n",
    "        \n",
    "        if self.arrayNewPosReward.all() != None:\n",
    "            for k in range(self.arrayNewPosReward.shape[0]):\n",
    "                #print(i)\n",
    "                i = self.arrayNewPosReward[k,0]\n",
    "                if (i <= next_state[0] and i +0.5 >=0) or (i >= next_state[0] and i +0.5 <=0):\n",
    "                    if self.arrayNewPosReward[k,2] == 0:\n",
    "                        if self.arrayNewPosReward[k,1] > reward:\n",
    "                            reward = self.arrayNewPosReward[k,1]\n",
    "                            if uniqueReward: self.arrayNewPosReward[k,2] = 1\n",
    "                        \n",
    "        if self.arrayNewVelReward.all() != None:\n",
    "            for k in range(self.arrayNewVelReward.shape[0]):\n",
    "                i = self.arrayNewVelReward[k,0]\n",
    "                if (i < state[1] and i>0 and action == 2) or (i > state[1] and i<0 and action == 0):\n",
    "                    reward += self.arrayNewVelReward[k,1]\n",
    "                    #print(state[1])\n",
    "                    #print(action)\n",
    "                    \n",
    "        return reward\n",
    "\n",
    "    def play(self):\n",
    "        self.epsilon = 0\n",
    "        newSeed = random.randint(0,100000)\n",
    "        state,_ = self.env.reset(seed = newSeed)\n",
    "        done = False\n",
    "                    \n",
    "        while done == False:\n",
    "                                        \n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "        self.env.close()\n",
    "    \n",
    "    def train(self, episodes,lossLim = 0, limitStep = 200, refreshTarget = 200, refreshQ = 1, buffer_fill = True, epsilonDecreasing =100, debug_mode=False, recap_mode=False, reset_init = False, epsilon_decrease = True, uniqueReward = False, excludePassivity = True):\n",
    "        episodesHistory = np.zeros((episodes))\n",
    "        rewardHistory = np.zeros((episodes*limitStep))\n",
    "        lossHistory = np.zeros((int(episodes*limitStep/refreshQ)))\n",
    "        gradHistory = np.zeros((int(episodes*limitStep/refreshQ)))\n",
    "        cumulativeHistory = np.zeros((episodes))\n",
    "        self.QTarget = self.Q\n",
    "        if reset_init != False: self.Q.reset_init_weights_biases(reset_init)\n",
    "        j=0\n",
    "        self.replay_buffer = np.zeros((self.replay_buffer_SIZE, self.observation_SIZE))\n",
    "        k=0\n",
    "        for e in range(episodes):\n",
    "            l=0\n",
    "            terminated = False\n",
    "            if debug_mode: print(\"Episode: \"+str(e+1)+\" starts\")\n",
    "                \n",
    "            if epsilon_decrease: \n",
    "                if self.epsilon > self.epsilonMin:\n",
    "                    self.epsilon = self.epsilonMax*math.exp(-e/epsilonDecreasing)\n",
    "            else:\n",
    "                self.epsilon = self.epsilonMax\n",
    "                \n",
    "            newSeed = random.randint(0,100000)\n",
    "            state,_ = self.env.reset(seed = newSeed)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            if self.arrayNewPosReward.all() != None:\n",
    "                for i in range(self.arrayNewPosReward.shape[0]):\n",
    "                    self.arrayNewPosReward[i,2] = 0\n",
    "\n",
    "            s=1\n",
    "            while done == False:\n",
    "                j+=1               \n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                \n",
    "                if s >= limitStep:\n",
    "                    truncated = True\n",
    "                else:\n",
    "                    truncated = False\n",
    "\n",
    "                s+=1\n",
    "                reward = self.customReward(state, action,reward, next_state, uniqueReward, excludePassivity)\n",
    "                if debug_mode: print(\"Action \"+str(k)+\" selected: \"+str(action)+\" Reward: \"+ str(reward))\n",
    "                observe = self.observe(state,action,next_state,reward)\n",
    "                \n",
    "                if k < self.replay_buffer_SIZE: self.replay_buffer[k] = observe\n",
    "                elif buffer_fill: k=-1\n",
    "                k+=1\n",
    "                \n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "                if j%refreshQ == 0:\n",
    "                    l,g = self.update(j)\n",
    "                    lossHistory[int(j/refreshQ)-1] = l\n",
    "                    gradHistory[int(j/refreshQ)-1] = g\n",
    "                if j%refreshTarget == 0:\n",
    "                    print(\"Loss: \"+str(l))\n",
    "                    self.QTarget = self.Q\n",
    "                    #for param in self.Q.parameters():\n",
    "                        #print(param.data)\n",
    "\n",
    "                rewardHistory[j-1] = reward\n",
    "                \n",
    "            if terminated:\n",
    "                if e > 0:\n",
    "                    cumulativeHistory[e] = cumulativeHistory[e-1] +1\n",
    "                else:\n",
    "                    cumulativeHistory[e] = 1\n",
    "            else:\n",
    "                if e > 0:\n",
    "                    cumulativeHistory[e] = cumulativeHistory[e-1]\n",
    "                else:\n",
    "                    cumulativeHistory[e] = 0\n",
    "                                    \n",
    "            if debug_mode or recap_mode: print(\"Episode \"+str(e+1)+ \" , Reward: \"+str(episode_reward)+\" Epsilon: \"+str(self.epsilon))\n",
    "            episodesHistory[e] = episode_reward\n",
    "            \n",
    "            if l <= lossLim:\n",
    "                print(\"Loss reaches limit\")\n",
    "                break\n",
    "        return episodesHistory, rewardHistory, lossHistory, gradHistory, cumulativeHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac77d547-9e22-4e27-b569-d8835df67752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da8dc4c5-d0f8-4e79-b8ed-217f095a6532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([72., 28.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discr_step = np.array([0.025, 0.005])\n",
    "env = gym.make('MountainCar-v0')\n",
    "(env.observation_space.high - env.observation_space.low)//discr_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7520b9-d6a8-4719-bb1b-e3b674a32bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eda6fbdd-728f-44f0-b029-bd07efb6e823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample([1,2,5], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "565a7796-4f32-4502-b38b-88b8cbfb032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "345c3e60-bf01-42b8-968f-e8cf05543aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cf code cours, après avoir obtenu un modèle de l'environnement, résoudre le porblème d'optimisation avec le dynamic programming\n",
    "\n",
    "class DynaAgent(Agent):\n",
    "    \n",
    "    def __init__(self, id, env=gym.make('MountainCar-v0'), epsilonMax = 0.9, epsilonMin = 0.05, discr_step = np.array([0.025, 0.005]), gamma = 0.99, k=0, alpha=0.2):\n",
    "        Agent.__init__(self,id,env)\n",
    "        self.discr_step = discr_step\n",
    "        self.n_xbins = np.round(((self.env.observation_space.high - self.env.observation_space.low)/self.discr_step)[0]).astype(np.int32)\n",
    "        self.n_vbins = np.round(((self.env.observation_space.high - self.env.observation_space.low)/self.discr_step)[1]).astype(np.int32)\n",
    "        self.n_states = self.n_xbins*self.n_vbins\n",
    "        self.n_actions = 3\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilonMax\n",
    "        self.epsilonMax = epsilonMax\n",
    "        self.epsilonMin = epsilonMin\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.visited_state_action = set()\n",
    "        \n",
    "        self.N = [sp.lil_array((self.n_actions, self.n_states)) for _ in range(self.n_states)]\n",
    "        \n",
    "        self.P = np.zeros(shape=(self.n_states, self.n_actions, self.n_states))\n",
    "        for i in range(self.n_states):\n",
    "            for j in range(self.n_actions):\n",
    "                random = torch.rand(size=(self.n_states,))\n",
    "                self.P[i, j, :] = random/random.sum()\n",
    "\n",
    "        \n",
    "        self.R = - np.ones(shape=(self.n_states, self.n_actions))\n",
    "        terminal_x_bin = self.discretize_x(0.5)\n",
    "        for state in range(terminal_x_bin, self.n_states):\n",
    "            self.R[state, :] = 0\n",
    "\n",
    "        self.R = sp.lil_array(self.R)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.W = - np.ones(shape=(self.n_states, self.n_actions))\n",
    "        terminal_x_bin = self.discretize_x(0.5)\n",
    "        for state in range(terminal_x_bin, self.n_states):\n",
    "            self.W[state, :] = 0\n",
    "        \"\"\"\n",
    "        \n",
    "        self.Q = sp.lil_array(np.zeros(shape=(self.n_states, self.n_actions)))\n",
    "    \n",
    "\n",
    "    # On obtient les s du (s, a, s') en discrétisant (cf plus haut), puis pour les s' on utilise une loi uniforme pour chaque paire (s, a)\n",
    "\n",
    "    def discretize_x(self, x):\n",
    "        x_bin = np.round(((x - self.env.observation_space.low)/self.discr_step)[0]).astype(np.int32)\n",
    "        return x_bin*(self.n_vbins) \n",
    "\n",
    "    def discretize_v(self, v):\n",
    "        v_bin = np.round(((v - self.env.observation_space.low)/self.discr_step)[1]).astype(np.int32)\n",
    "        return v_bin \n",
    "\n",
    "    def discretize(self, state):\n",
    "        x_bin = self.discretize_x(state[0])\n",
    "        v_bin = self.discretize_v(state[1])\n",
    "        return x_bin + v_bin\n",
    "\n",
    "    \n",
    "   \n",
    "    def update(self, state, action, next_state, reward):\n",
    "        \n",
    "        discr_state, discr_next_state = self.discretize(state), self.discretize(next_state)\n",
    "\n",
    "        self.N[discr_state][action, discr_next_state] += 1\n",
    "\n",
    "        \n",
    "        total_visited = self.N[discr_state].getrow(action).sum()\n",
    "        if total_visited > 0:\n",
    "            self.P[discr_state, action, discr_next_state] = self.N[discr_state][action,  discr_next_state] / total_visited\n",
    "\n",
    "        \n",
    "        #self.W[discr_state, action] += reward\n",
    "        self.R[discr_state, action] = (1-self.alpha)*self.R[discr_state, action] + self.alpha*reward\n",
    "        \n",
    "        Q_dense = self.Q.toarray()\n",
    "        Q_dense[discr_state, action] = self.R[discr_state, action] + (self.gamma)*np.array([(self.P[discr_state, action, discr_next_local])*(max(Q_dense[discr_next_local, :])) for discr_next_local in range(self.n_states)]).sum()\n",
    "        self.Q = sp.lil_matrix(Q_dense)\n",
    "\n",
    "        self.k = len(self.visited_state_action) // 10\n",
    "        sampled_states = []\n",
    "        if self.k >= 1:\n",
    "            sampled_states = random.sample(list(self.visited_state_action), min(self.k, len(self.visited_state_action)))\n",
    "\n",
    "            for (random_state, random_action) in sampled_states:\n",
    "            \n",
    "                Q_dense = self.Q.toarray()\n",
    "                Q_dense[random_state, random_action] = self.R[random_state, random_action] + (self.gamma)*np.array([(self.P[random_state, random_action, discr_next_local])*(max(Q_dense[discr_next_local, :])) for discr_next_local in range(self.n_states)]).sum()\n",
    "                self.Q = sp.lil_matrix(Q_dense)\n",
    "               \n",
    "\n",
    "      \n",
    "\n",
    "    def observe(self):\n",
    "        pass\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state_bin = self.discretize(state)\n",
    "        p = random.uniform(0,1)\n",
    "        a=0\n",
    "        if p < 1-self.epsilon :\n",
    "            a = np.argmax(self.Q.getrow(state_bin))\n",
    "        else:\n",
    "            a = random.randint(0,2)\n",
    "            \n",
    "        return a\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # Reinitialisation de R nécessaire ?\n",
    "    def reset_for_episode(self):\n",
    "        self.N_episode = np.zeros(shape=(self.n_states, self.n_actions, self.n_states))\n",
    "        \n",
    "        self.W_episode = - np.ones(shape=(self.n_states, self.n_actions))\n",
    "        terminal_x_bin = self.discretize_x(0.5)\n",
    "        for state in range(terminal_x_bin, self.n_states):\n",
    "            self.W_episode[state, :] = 0\n",
    "\n",
    "    \"\"\"  \n",
    "\n",
    "    \n",
    "    def train(self, episodes, debug_mode=True, epsilon_decrease=True, epsilonDecreasing=100):\n",
    "        episodesHistory = np.zeros((episodes))\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            if debug_mode: print(\"Episode: \"+str(i+1)+\" starts\")\n",
    "\n",
    "\n",
    "            if epsilon_decrease: \n",
    "                if self.epsilon > self.epsilonMin:\n",
    "                    self.epsilon = self.epsilonMax*math.exp(-np.e/epsilonDecreasing)\n",
    "            else:\n",
    "                self.epsilon = self.epsilonMax\n",
    "                \n",
    "            newSeed = random.randint(0,100000)\n",
    "            state,_ = self.env.reset(seed = newSeed)\n",
    "            \n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            k=0\n",
    "            \n",
    "            while not done:\n",
    "            \n",
    "                action = self.select_action(state)\n",
    "                #if debug_mode: print(\"Action :\"+str(k)+\" selected: \"+str(action))\n",
    "                #print(self.N_episode.sum())\n",
    "                k+=1\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "\n",
    "                self.update(state, action, next_state, reward)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "\n",
    "            #self.reset_for_episode()\n",
    "\n",
    "             \n",
    "            if debug_mode: print(\"Episode \"+str(i+1)+ \" , Reward: \"+str(episode_reward))\n",
    "            episodesHistory[i] = episode_reward\n",
    "        return episodesHistory[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ab3a1265-b585-4fea-bd39-bc8652333bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = DynaAgent(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "08fd16f2-9498-4806-873f-c0ca4969bc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "N = np.ones(shape=(10, 5))\n",
    "\n",
    "print(np.max(sp.csr_matrix(N)[0, :]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8e45d38c-0435-4325-861a-f4b61ca841c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (0, 1)\t1.0\n",
      "  (0, 2)\t1.0\n",
      "  (0, 3)\t1.0\n",
      "  (0, 4)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(sp.lil_array(N).getrow(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80cd3a1-7794-4c20-95fd-923c228c7b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 starts\n",
      "Episode 1 , Reward: -200.0\n",
      "Episode: 2 starts\n",
      "Episode 2 , Reward: -200.0\n",
      "Episode: 3 starts\n",
      "Episode 3 , Reward: -200.0\n",
      "Episode: 4 starts\n",
      "Episode 4 , Reward: -200.0\n",
      "Episode: 5 starts\n",
      "Episode 5 , Reward: -200.0\n",
      "Episode: 6 starts\n",
      "Episode 6 , Reward: -200.0\n",
      "Episode: 7 starts\n",
      "Episode 7 , Reward: -200.0\n",
      "Episode: 8 starts\n",
      "Episode 8 , Reward: -200.0\n",
      "Episode: 9 starts\n",
      "Episode 9 , Reward: -200.0\n",
      "Episode: 10 starts\n",
      "Episode 10 , Reward: -200.0\n",
      "Episode: 11 starts\n",
      "Episode 11 , Reward: -200.0\n",
      "Episode: 12 starts\n",
      "Episode 12 , Reward: -200.0\n",
      "Episode: 13 starts\n",
      "Episode 13 , Reward: -200.0\n",
      "Episode: 14 starts\n",
      "Episode 14 , Reward: -200.0\n",
      "Episode: 15 starts\n",
      "Episode 15 , Reward: -200.0\n",
      "Episode: 16 starts\n",
      "Episode 16 , Reward: -200.0\n",
      "Episode: 17 starts\n",
      "Episode 17 , Reward: -200.0\n",
      "Episode: 18 starts\n",
      "Episode 18 , Reward: -200.0\n",
      "Episode: 19 starts\n",
      "Episode 19 , Reward: -200.0\n",
      "Episode: 20 starts\n",
      "Episode 20 , Reward: -200.0\n",
      "Episode: 21 starts\n",
      "Episode 21 , Reward: -200.0\n",
      "Episode: 22 starts\n",
      "Episode 22 , Reward: -200.0\n",
      "Episode: 23 starts\n",
      "Episode 23 , Reward: -200.0\n",
      "Episode: 24 starts\n",
      "Episode 24 , Reward: -200.0\n",
      "Episode: 25 starts\n",
      "Episode 25 , Reward: -200.0\n",
      "Episode: 26 starts\n",
      "Episode 26 , Reward: -200.0\n",
      "Episode: 27 starts\n",
      "Episode 27 , Reward: -200.0\n",
      "Episode: 28 starts\n",
      "Episode 28 , Reward: -200.0\n",
      "Episode: 29 starts\n",
      "Episode 29 , Reward: -200.0\n",
      "Episode: 30 starts\n",
      "Episode 30 , Reward: -200.0\n",
      "Episode: 31 starts\n",
      "Episode 31 , Reward: -200.0\n",
      "Episode: 32 starts\n",
      "Episode 32 , Reward: -200.0\n",
      "Episode: 33 starts\n",
      "Episode 33 , Reward: -200.0\n",
      "Episode: 34 starts\n",
      "Episode 34 , Reward: -200.0\n",
      "Episode: 35 starts\n",
      "Episode 35 , Reward: -200.0\n",
      "Episode: 36 starts\n",
      "Episode 36 , Reward: -200.0\n",
      "Episode: 37 starts\n",
      "Episode 37 , Reward: -200.0\n",
      "Episode: 38 starts\n",
      "Episode 38 , Reward: -200.0\n",
      "Episode: 39 starts\n",
      "Episode 39 , Reward: -200.0\n",
      "Episode: 40 starts\n",
      "Episode 40 , Reward: -200.0\n",
      "Episode: 41 starts\n",
      "Episode 41 , Reward: -200.0\n",
      "Episode: 42 starts\n",
      "Episode 42 , Reward: -200.0\n",
      "Episode: 43 starts\n",
      "Episode 43 , Reward: -200.0\n",
      "Episode: 44 starts\n",
      "Episode 44 , Reward: -200.0\n",
      "Episode: 45 starts\n",
      "Episode 45 , Reward: -200.0\n",
      "Episode: 46 starts\n",
      "Episode 46 , Reward: -200.0\n",
      "Episode: 47 starts\n",
      "Episode 47 , Reward: -200.0\n",
      "Episode: 48 starts\n",
      "Episode 48 , Reward: -200.0\n",
      "Episode: 49 starts\n",
      "Episode 49 , Reward: -200.0\n",
      "Episode: 50 starts\n",
      "Episode 50 , Reward: -200.0\n",
      "Episode: 51 starts\n",
      "Episode 51 , Reward: -200.0\n",
      "Episode: 52 starts\n",
      "Episode 52 , Reward: -200.0\n",
      "Episode: 53 starts\n",
      "Episode 53 , Reward: -200.0\n",
      "Episode: 54 starts\n",
      "Episode 54 , Reward: -200.0\n",
      "Episode: 55 starts\n",
      "Episode 55 , Reward: -200.0\n",
      "Episode: 56 starts\n",
      "Episode 56 , Reward: -200.0\n",
      "Episode: 57 starts\n",
      "Episode 57 , Reward: -200.0\n",
      "Episode: 58 starts\n",
      "Episode 58 , Reward: -200.0\n",
      "Episode: 59 starts\n",
      "Episode 59 , Reward: -200.0\n",
      "Episode: 60 starts\n",
      "Episode 60 , Reward: -200.0\n",
      "Episode: 61 starts\n",
      "Episode 61 , Reward: -200.0\n",
      "Episode: 62 starts\n",
      "Episode 62 , Reward: -200.0\n",
      "Episode: 63 starts\n",
      "Episode 63 , Reward: -200.0\n",
      "Episode: 64 starts\n",
      "Episode 64 , Reward: -200.0\n",
      "Episode: 65 starts\n",
      "Episode 65 , Reward: -200.0\n",
      "Episode: 66 starts\n",
      "Episode 66 , Reward: -200.0\n",
      "Episode: 67 starts\n",
      "Episode 67 , Reward: -200.0\n",
      "Episode: 68 starts\n",
      "Episode 68 , Reward: -200.0\n",
      "Episode: 69 starts\n",
      "Episode 69 , Reward: -200.0\n",
      "Episode: 70 starts\n",
      "Episode 70 , Reward: -200.0\n",
      "Episode: 71 starts\n",
      "Episode 71 , Reward: -200.0\n",
      "Episode: 72 starts\n",
      "Episode 72 , Reward: -200.0\n",
      "Episode: 73 starts\n",
      "Episode 73 , Reward: -200.0\n",
      "Episode: 74 starts\n",
      "Episode 74 , Reward: -200.0\n",
      "Episode: 75 starts\n",
      "Episode 75 , Reward: -200.0\n",
      "Episode: 76 starts\n",
      "Episode 76 , Reward: -200.0\n",
      "Episode: 77 starts\n",
      "Episode 77 , Reward: -200.0\n",
      "Episode: 78 starts\n",
      "Episode 78 , Reward: -200.0\n",
      "Episode: 79 starts\n",
      "Episode 79 , Reward: -200.0\n",
      "Episode: 80 starts\n",
      "Episode 80 , Reward: -200.0\n",
      "Episode: 81 starts\n",
      "Episode 81 , Reward: -200.0\n",
      "Episode: 82 starts\n",
      "Episode 82 , Reward: -200.0\n",
      "Episode: 83 starts\n",
      "Episode 83 , Reward: -200.0\n",
      "Episode: 84 starts\n",
      "Episode 84 , Reward: -200.0\n",
      "Episode: 85 starts\n",
      "Episode 85 , Reward: -200.0\n",
      "Episode: 86 starts\n",
      "Episode 86 , Reward: -200.0\n",
      "Episode: 87 starts\n",
      "Episode 87 , Reward: -200.0\n",
      "Episode: 88 starts\n",
      "Episode 88 , Reward: -200.0\n",
      "Episode: 89 starts\n",
      "Episode 89 , Reward: -200.0\n",
      "Episode: 90 starts\n",
      "Episode 90 , Reward: -200.0\n",
      "Episode: 91 starts\n",
      "Episode 91 , Reward: -200.0\n",
      "Episode: 92 starts\n",
      "Episode 92 , Reward: -200.0\n",
      "Episode: 93 starts\n",
      "Episode 93 , Reward: -200.0\n",
      "Episode: 94 starts\n",
      "Episode 94 , Reward: -200.0\n",
      "Episode: 95 starts\n",
      "Episode 95 , Reward: -200.0\n",
      "Episode: 96 starts\n",
      "Episode 96 , Reward: -200.0\n",
      "Episode: 97 starts\n",
      "Episode 97 , Reward: -200.0\n",
      "Episode: 98 starts\n",
      "Episode 98 , Reward: -200.0\n",
      "Episode: 99 starts\n",
      "Episode 99 , Reward: -200.0\n",
      "Episode: 100 starts\n",
      "Episode 100 , Reward: -200.0\n",
      "Episode: 101 starts\n",
      "Episode 101 , Reward: -200.0\n",
      "Episode: 102 starts\n",
      "Episode 102 , Reward: -200.0\n",
      "Episode: 103 starts\n",
      "Episode 103 , Reward: -200.0\n",
      "Episode: 104 starts\n",
      "Episode 104 , Reward: -200.0\n",
      "Episode: 105 starts\n",
      "Episode 105 , Reward: -200.0\n",
      "Episode: 106 starts\n",
      "Episode 106 , Reward: -200.0\n",
      "Episode: 107 starts\n",
      "Episode 107 , Reward: -200.0\n",
      "Episode: 108 starts\n",
      "Episode 108 , Reward: -200.0\n",
      "Episode: 109 starts\n",
      "Episode 109 , Reward: -200.0\n",
      "Episode: 110 starts\n",
      "Episode 110 , Reward: -200.0\n",
      "Episode: 111 starts\n",
      "Episode 111 , Reward: -200.0\n",
      "Episode: 112 starts\n",
      "Episode 112 , Reward: -200.0\n",
      "Episode: 113 starts\n",
      "Episode 113 , Reward: -200.0\n",
      "Episode: 114 starts\n",
      "Episode 114 , Reward: -200.0\n",
      "Episode: 115 starts\n",
      "Episode 115 , Reward: -200.0\n",
      "Episode: 116 starts\n",
      "Episode 116 , Reward: -200.0\n",
      "Episode: 117 starts\n",
      "Episode 117 , Reward: -200.0\n",
      "Episode: 118 starts\n",
      "Episode 118 , Reward: -200.0\n",
      "Episode: 119 starts\n",
      "Episode 119 , Reward: -200.0\n",
      "Episode: 120 starts\n",
      "Episode 120 , Reward: -200.0\n",
      "Episode: 121 starts\n",
      "Episode 121 , Reward: -200.0\n",
      "Episode: 122 starts\n",
      "Episode 122 , Reward: -200.0\n",
      "Episode: 123 starts\n",
      "Episode 123 , Reward: -200.0\n",
      "Episode: 124 starts\n",
      "Episode 124 , Reward: -200.0\n",
      "Episode: 125 starts\n",
      "Episode 125 , Reward: -200.0\n",
      "Episode: 126 starts\n",
      "Episode 126 , Reward: -200.0\n",
      "Episode: 127 starts\n",
      "Episode 127 , Reward: -200.0\n",
      "Episode: 128 starts\n",
      "Episode 128 , Reward: -200.0\n",
      "Episode: 129 starts\n",
      "Episode 129 , Reward: -200.0\n",
      "Episode: 130 starts\n",
      "Episode 130 , Reward: -200.0\n",
      "Episode: 131 starts\n",
      "Episode 131 , Reward: -200.0\n",
      "Episode: 132 starts\n",
      "Episode 132 , Reward: -200.0\n",
      "Episode: 133 starts\n",
      "Episode 133 , Reward: -200.0\n",
      "Episode: 134 starts\n",
      "Episode 134 , Reward: -200.0\n",
      "Episode: 135 starts\n",
      "Episode 135 , Reward: -200.0\n",
      "Episode: 136 starts\n",
      "Episode 136 , Reward: -200.0\n",
      "Episode: 137 starts\n",
      "Episode 137 , Reward: -200.0\n",
      "Episode: 138 starts\n",
      "Episode 138 , Reward: -200.0\n",
      "Episode: 139 starts\n",
      "Episode 139 , Reward: -200.0\n",
      "Episode: 140 starts\n",
      "Episode 140 , Reward: -200.0\n",
      "Episode: 141 starts\n",
      "Episode 141 , Reward: -200.0\n",
      "Episode: 142 starts\n",
      "Episode 142 , Reward: -200.0\n",
      "Episode: 143 starts\n",
      "Episode 143 , Reward: -200.0\n",
      "Episode: 144 starts\n",
      "Episode 144 , Reward: -200.0\n",
      "Episode: 145 starts\n",
      "Episode 145 , Reward: -200.0\n",
      "Episode: 146 starts\n",
      "Episode 146 , Reward: -200.0\n",
      "Episode: 147 starts\n",
      "Episode 147 , Reward: -200.0\n",
      "Episode: 148 starts\n",
      "Episode 148 , Reward: -200.0\n",
      "Episode: 149 starts\n",
      "Episode 149 , Reward: -200.0\n",
      "Episode: 150 starts\n",
      "Episode 150 , Reward: -200.0\n",
      "Episode: 151 starts\n",
      "Episode 151 , Reward: -200.0\n",
      "Episode: 152 starts\n",
      "Episode 152 , Reward: -200.0\n",
      "Episode: 153 starts\n",
      "Episode 153 , Reward: -200.0\n",
      "Episode: 154 starts\n",
      "Episode 154 , Reward: -200.0\n",
      "Episode: 155 starts\n",
      "Episode 155 , Reward: -200.0\n",
      "Episode: 156 starts\n",
      "Episode 156 , Reward: -200.0\n",
      "Episode: 157 starts\n",
      "Episode 157 , Reward: -200.0\n",
      "Episode: 158 starts\n",
      "Episode 158 , Reward: -200.0\n",
      "Episode: 159 starts\n",
      "Episode 159 , Reward: -200.0\n",
      "Episode: 160 starts\n",
      "Episode 160 , Reward: -200.0\n",
      "Episode: 161 starts\n",
      "Episode 161 , Reward: -200.0\n",
      "Episode: 162 starts\n",
      "Episode 162 , Reward: -200.0\n",
      "Episode: 163 starts\n",
      "Episode 163 , Reward: -200.0\n",
      "Episode: 164 starts\n",
      "Episode 164 , Reward: -200.0\n",
      "Episode: 165 starts\n",
      "Episode 165 , Reward: -200.0\n",
      "Episode: 166 starts\n",
      "Episode 166 , Reward: -200.0\n",
      "Episode: 167 starts\n",
      "Episode 167 , Reward: -200.0\n",
      "Episode: 168 starts\n",
      "Episode 168 , Reward: -200.0\n",
      "Episode: 169 starts\n",
      "Episode 169 , Reward: -200.0\n",
      "Episode: 170 starts\n",
      "Episode 170 , Reward: -200.0\n",
      "Episode: 171 starts\n",
      "Episode 171 , Reward: -200.0\n",
      "Episode: 172 starts\n",
      "Episode 172 , Reward: -200.0\n",
      "Episode: 173 starts\n",
      "Episode 173 , Reward: -200.0\n",
      "Episode: 174 starts\n",
      "Episode 174 , Reward: -200.0\n",
      "Episode: 175 starts\n",
      "Episode 175 , Reward: -200.0\n",
      "Episode: 176 starts\n",
      "Episode 176 , Reward: -200.0\n",
      "Episode: 177 starts\n",
      "Episode 177 , Reward: -200.0\n",
      "Episode: 178 starts\n",
      "Episode 178 , Reward: -200.0\n",
      "Episode: 179 starts\n",
      "Episode 179 , Reward: -200.0\n",
      "Episode: 180 starts\n",
      "Episode 180 , Reward: -200.0\n",
      "Episode: 181 starts\n",
      "Episode 181 , Reward: -200.0\n",
      "Episode: 182 starts\n",
      "Episode 182 , Reward: -200.0\n",
      "Episode: 183 starts\n",
      "Episode 183 , Reward: -200.0\n",
      "Episode: 184 starts\n",
      "Episode 184 , Reward: -200.0\n",
      "Episode: 185 starts\n",
      "Episode 185 , Reward: -200.0\n",
      "Episode: 186 starts\n",
      "Episode 186 , Reward: -200.0\n",
      "Episode: 187 starts\n",
      "Episode 187 , Reward: -200.0\n",
      "Episode: 188 starts\n",
      "Episode 188 , Reward: -200.0\n",
      "Episode: 189 starts\n",
      "Episode 189 , Reward: -200.0\n",
      "Episode: 190 starts\n",
      "Episode 190 , Reward: -200.0\n",
      "Episode: 191 starts\n",
      "Episode 191 , Reward: -200.0\n",
      "Episode: 192 starts\n",
      "Episode 192 , Reward: -200.0\n",
      "Episode: 193 starts\n",
      "Episode 193 , Reward: -200.0\n",
      "Episode: 194 starts\n",
      "Episode 194 , Reward: -200.0\n",
      "Episode: 195 starts\n",
      "Episode 195 , Reward: -200.0\n",
      "Episode: 196 starts\n",
      "Episode 196 , Reward: -200.0\n",
      "Episode: 197 starts\n",
      "Episode 197 , Reward: -200.0\n",
      "Episode: 198 starts\n",
      "Episode 198 , Reward: -200.0\n",
      "Episode: 199 starts\n",
      "Episode 199 , Reward: -200.0\n",
      "Episode: 200 starts\n",
      "Episode 200 , Reward: -200.0\n",
      "Episode: 201 starts\n",
      "Episode 201 , Reward: -200.0\n",
      "Episode: 202 starts\n",
      "Episode 202 , Reward: -200.0\n",
      "Episode: 203 starts\n",
      "Episode 203 , Reward: -200.0\n",
      "Episode: 204 starts\n",
      "Episode 204 , Reward: -200.0\n",
      "Episode: 205 starts\n",
      "Episode 205 , Reward: -200.0\n",
      "Episode: 206 starts\n",
      "Episode 206 , Reward: -200.0\n",
      "Episode: 207 starts\n",
      "Episode 207 , Reward: -200.0\n",
      "Episode: 208 starts\n",
      "Episode 208 , Reward: -200.0\n",
      "Episode: 209 starts\n",
      "Episode 209 , Reward: -200.0\n",
      "Episode: 210 starts\n",
      "Episode 210 , Reward: -200.0\n",
      "Episode: 211 starts\n",
      "Episode 211 , Reward: -200.0\n",
      "Episode: 212 starts\n",
      "Episode 212 , Reward: -200.0\n",
      "Episode: 213 starts\n",
      "Episode 213 , Reward: -200.0\n",
      "Episode: 214 starts\n",
      "Episode 214 , Reward: -200.0\n",
      "Episode: 215 starts\n",
      "Episode 215 , Reward: -200.0\n",
      "Episode: 216 starts\n",
      "Episode 216 , Reward: -200.0\n",
      "Episode: 217 starts\n",
      "Episode 217 , Reward: -200.0\n",
      "Episode: 218 starts\n",
      "Episode 218 , Reward: -200.0\n",
      "Episode: 219 starts\n",
      "Episode 219 , Reward: -200.0\n",
      "Episode: 220 starts\n",
      "Episode 220 , Reward: -200.0\n",
      "Episode: 221 starts\n",
      "Episode 221 , Reward: -200.0\n",
      "Episode: 222 starts\n",
      "Episode 222 , Reward: -200.0\n",
      "Episode: 223 starts\n",
      "Episode 223 , Reward: -200.0\n",
      "Episode: 224 starts\n",
      "Episode 224 , Reward: -200.0\n",
      "Episode: 225 starts\n",
      "Episode 225 , Reward: -200.0\n",
      "Episode: 226 starts\n",
      "Episode 226 , Reward: -200.0\n",
      "Episode: 227 starts\n",
      "Episode 227 , Reward: -200.0\n",
      "Episode: 228 starts\n",
      "Episode 228 , Reward: -200.0\n",
      "Episode: 229 starts\n",
      "Episode 229 , Reward: -200.0\n",
      "Episode: 230 starts\n",
      "Episode 230 , Reward: -200.0\n",
      "Episode: 231 starts\n",
      "Episode 231 , Reward: -200.0\n",
      "Episode: 232 starts\n",
      "Episode 232 , Reward: -200.0\n",
      "Episode: 233 starts\n",
      "Episode 233 , Reward: -200.0\n",
      "Episode: 234 starts\n",
      "Episode 234 , Reward: -200.0\n",
      "Episode: 235 starts\n",
      "Episode 235 , Reward: -200.0\n",
      "Episode: 236 starts\n",
      "Episode 236 , Reward: -200.0\n",
      "Episode: 237 starts\n",
      "Episode 237 , Reward: -200.0\n",
      "Episode: 238 starts\n",
      "Episode 238 , Reward: -200.0\n",
      "Episode: 239 starts\n",
      "Episode 239 , Reward: -200.0\n",
      "Episode: 240 starts\n",
      "Episode 240 , Reward: -200.0\n",
      "Episode: 241 starts\n",
      "Episode 241 , Reward: -200.0\n",
      "Episode: 242 starts\n",
      "Episode 242 , Reward: -200.0\n",
      "Episode: 243 starts\n",
      "Episode 243 , Reward: -200.0\n",
      "Episode: 244 starts\n",
      "Episode 244 , Reward: -200.0\n",
      "Episode: 245 starts\n",
      "Episode 245 , Reward: -200.0\n",
      "Episode: 246 starts\n",
      "Episode 246 , Reward: -200.0\n",
      "Episode: 247 starts\n",
      "Episode 247 , Reward: -200.0\n",
      "Episode: 248 starts\n",
      "Episode 248 , Reward: -200.0\n",
      "Episode: 249 starts\n",
      "Episode 249 , Reward: -200.0\n",
      "Episode: 250 starts\n",
      "Episode 250 , Reward: -200.0\n",
      "Episode: 251 starts\n",
      "Episode 251 , Reward: -200.0\n",
      "Episode: 252 starts\n",
      "Episode 252 , Reward: -200.0\n",
      "Episode: 253 starts\n",
      "Episode 253 , Reward: -200.0\n",
      "Episode: 254 starts\n",
      "Episode 254 , Reward: -200.0\n",
      "Episode: 255 starts\n",
      "Episode 255 , Reward: -200.0\n",
      "Episode: 256 starts\n",
      "Episode 256 , Reward: -200.0\n",
      "Episode: 257 starts\n",
      "Episode 257 , Reward: -200.0\n",
      "Episode: 258 starts\n",
      "Episode 258 , Reward: -200.0\n",
      "Episode: 259 starts\n",
      "Episode 259 , Reward: -200.0\n",
      "Episode: 260 starts\n",
      "Episode 260 , Reward: -200.0\n",
      "Episode: 261 starts\n",
      "Episode 261 , Reward: -200.0\n",
      "Episode: 262 starts\n",
      "Episode 262 , Reward: -200.0\n",
      "Episode: 263 starts\n",
      "Episode 263 , Reward: -200.0\n",
      "Episode: 264 starts\n",
      "Episode 264 , Reward: -200.0\n",
      "Episode: 265 starts\n",
      "Episode 265 , Reward: -200.0\n",
      "Episode: 266 starts\n",
      "Episode 266 , Reward: -200.0\n",
      "Episode: 267 starts\n",
      "Episode 267 , Reward: -200.0\n",
      "Episode: 268 starts\n",
      "Episode 268 , Reward: -200.0\n",
      "Episode: 269 starts\n",
      "Episode 269 , Reward: -200.0\n",
      "Episode: 270 starts\n",
      "Episode 270 , Reward: -200.0\n",
      "Episode: 271 starts\n",
      "Episode 271 , Reward: -200.0\n",
      "Episode: 272 starts\n",
      "Episode 272 , Reward: -200.0\n",
      "Episode: 273 starts\n",
      "Episode 273 , Reward: -200.0\n",
      "Episode: 274 starts\n",
      "Episode 274 , Reward: -200.0\n",
      "Episode: 275 starts\n",
      "Episode 275 , Reward: -200.0\n",
      "Episode: 276 starts\n",
      "Episode 276 , Reward: -200.0\n",
      "Episode: 277 starts\n",
      "Episode 277 , Reward: -200.0\n",
      "Episode: 278 starts\n",
      "Episode 278 , Reward: -200.0\n",
      "Episode: 279 starts\n",
      "Episode 279 , Reward: -200.0\n",
      "Episode: 280 starts\n",
      "Episode 280 , Reward: -200.0\n",
      "Episode: 281 starts\n",
      "Episode 281 , Reward: -200.0\n",
      "Episode: 282 starts\n",
      "Episode 282 , Reward: -200.0\n",
      "Episode: 283 starts\n",
      "Episode 283 , Reward: -200.0\n",
      "Episode: 284 starts\n",
      "Episode 284 , Reward: -200.0\n",
      "Episode: 285 starts\n",
      "Episode 285 , Reward: -200.0\n",
      "Episode: 286 starts\n",
      "Episode 286 , Reward: -200.0\n",
      "Episode: 287 starts\n",
      "Episode 287 , Reward: -200.0\n",
      "Episode: 288 starts\n",
      "Episode 288 , Reward: -200.0\n",
      "Episode: 289 starts\n",
      "Episode 289 , Reward: -200.0\n",
      "Episode: 290 starts\n",
      "Episode 290 , Reward: -200.0\n",
      "Episode: 291 starts\n",
      "Episode 291 , Reward: -200.0\n",
      "Episode: 292 starts\n",
      "Episode 292 , Reward: -200.0\n",
      "Episode: 293 starts\n",
      "Episode 293 , Reward: -200.0\n",
      "Episode: 294 starts\n",
      "Episode 294 , Reward: -200.0\n",
      "Episode: 295 starts\n",
      "Episode 295 , Reward: -200.0\n",
      "Episode: 296 starts\n",
      "Episode 296 , Reward: -200.0\n",
      "Episode: 297 starts\n",
      "Episode 297 , Reward: -200.0\n",
      "Episode: 298 starts\n",
      "Episode 298 , Reward: -200.0\n",
      "Episode: 299 starts\n",
      "Episode 299 , Reward: -200.0\n",
      "Episode: 300 starts\n",
      "Episode 300 , Reward: -200.0\n",
      "Episode: 301 starts\n",
      "Episode 301 , Reward: -200.0\n",
      "Episode: 302 starts\n",
      "Episode 302 , Reward: -200.0\n",
      "Episode: 303 starts\n",
      "Episode 303 , Reward: -200.0\n",
      "Episode: 304 starts\n",
      "Episode 304 , Reward: -200.0\n",
      "Episode: 305 starts\n",
      "Episode 305 , Reward: -200.0\n",
      "Episode: 306 starts\n",
      "Episode 306 , Reward: -200.0\n",
      "Episode: 307 starts\n",
      "Episode 307 , Reward: -200.0\n",
      "Episode: 308 starts\n",
      "Episode 308 , Reward: -200.0\n",
      "Episode: 309 starts\n",
      "Episode 309 , Reward: -200.0\n",
      "Episode: 310 starts\n",
      "Episode 310 , Reward: -200.0\n",
      "Episode: 311 starts\n",
      "Episode 311 , Reward: -200.0\n",
      "Episode: 312 starts\n",
      "Episode 312 , Reward: -200.0\n",
      "Episode: 313 starts\n",
      "Episode 313 , Reward: -200.0\n",
      "Episode: 314 starts\n",
      "Episode 314 , Reward: -200.0\n",
      "Episode: 315 starts\n",
      "Episode 315 , Reward: -200.0\n",
      "Episode: 316 starts\n",
      "Episode 316 , Reward: -200.0\n",
      "Episode: 317 starts\n",
      "Episode 317 , Reward: -200.0\n",
      "Episode: 318 starts\n",
      "Episode 318 , Reward: -200.0\n",
      "Episode: 319 starts\n",
      "Episode 319 , Reward: -200.0\n",
      "Episode: 320 starts\n",
      "Episode 320 , Reward: -200.0\n",
      "Episode: 321 starts\n",
      "Episode 321 , Reward: -200.0\n",
      "Episode: 322 starts\n",
      "Episode 322 , Reward: -200.0\n",
      "Episode: 323 starts\n",
      "Episode 323 , Reward: -200.0\n",
      "Episode: 324 starts\n",
      "Episode 324 , Reward: -200.0\n",
      "Episode: 325 starts\n",
      "Episode 325 , Reward: -200.0\n",
      "Episode: 326 starts\n",
      "Episode 326 , Reward: -200.0\n",
      "Episode: 327 starts\n",
      "Episode 327 , Reward: -200.0\n",
      "Episode: 328 starts\n",
      "Episode 328 , Reward: -200.0\n",
      "Episode: 329 starts\n",
      "Episode 329 , Reward: -200.0\n",
      "Episode: 330 starts\n",
      "Episode 330 , Reward: -200.0\n",
      "Episode: 331 starts\n",
      "Episode 331 , Reward: -200.0\n",
      "Episode: 332 starts\n",
      "Episode 332 , Reward: -200.0\n",
      "Episode: 333 starts\n",
      "Episode 333 , Reward: -200.0\n",
      "Episode: 334 starts\n",
      "Episode 334 , Reward: -200.0\n",
      "Episode: 335 starts\n",
      "Episode 335 , Reward: -200.0\n",
      "Episode: 336 starts\n",
      "Episode 336 , Reward: -200.0\n",
      "Episode: 337 starts\n",
      "Episode 337 , Reward: -200.0\n",
      "Episode: 338 starts\n",
      "Episode 338 , Reward: -200.0\n",
      "Episode: 339 starts\n",
      "Episode 339 , Reward: -200.0\n",
      "Episode: 340 starts\n",
      "Episode 340 , Reward: -200.0\n",
      "Episode: 341 starts\n",
      "Episode 341 , Reward: -200.0\n",
      "Episode: 342 starts\n",
      "Episode 342 , Reward: -200.0\n",
      "Episode: 343 starts\n",
      "Episode 343 , Reward: -200.0\n",
      "Episode: 344 starts\n",
      "Episode 344 , Reward: -200.0\n",
      "Episode: 345 starts\n",
      "Episode 345 , Reward: -200.0\n",
      "Episode: 346 starts\n",
      "Episode 346 , Reward: -200.0\n",
      "Episode: 347 starts\n",
      "Episode 347 , Reward: -200.0\n",
      "Episode: 348 starts\n",
      "Episode 348 , Reward: -200.0\n",
      "Episode: 349 starts\n",
      "Episode 349 , Reward: -200.0\n",
      "Episode: 350 starts\n",
      "Episode 350 , Reward: -200.0\n",
      "Episode: 351 starts\n",
      "Episode 351 , Reward: -200.0\n",
      "Episode: 352 starts\n",
      "Episode 352 , Reward: -200.0\n",
      "Episode: 353 starts\n",
      "Episode 353 , Reward: -200.0\n",
      "Episode: 354 starts\n",
      "Episode 354 , Reward: -200.0\n",
      "Episode: 355 starts\n",
      "Episode 355 , Reward: -200.0\n",
      "Episode: 356 starts\n",
      "Episode 356 , Reward: -200.0\n",
      "Episode: 357 starts\n",
      "Episode 357 , Reward: -200.0\n",
      "Episode: 358 starts\n",
      "Episode 358 , Reward: -200.0\n",
      "Episode: 359 starts\n",
      "Episode 359 , Reward: -200.0\n",
      "Episode: 360 starts\n",
      "Episode 360 , Reward: -200.0\n",
      "Episode: 361 starts\n",
      "Episode 361 , Reward: -200.0\n",
      "Episode: 362 starts\n",
      "Episode 362 , Reward: -200.0\n",
      "Episode: 363 starts\n",
      "Episode 363 , Reward: -200.0\n",
      "Episode: 364 starts\n",
      "Episode 364 , Reward: -200.0\n",
      "Episode: 365 starts\n",
      "Episode 365 , Reward: -200.0\n",
      "Episode: 366 starts\n",
      "Episode 366 , Reward: -200.0\n",
      "Episode: 367 starts\n",
      "Episode 367 , Reward: -200.0\n",
      "Episode: 368 starts\n",
      "Episode 368 , Reward: -200.0\n",
      "Episode: 369 starts\n",
      "Episode 369 , Reward: -200.0\n",
      "Episode: 370 starts\n",
      "Episode 370 , Reward: -200.0\n",
      "Episode: 371 starts\n",
      "Episode 371 , Reward: -200.0\n",
      "Episode: 372 starts\n",
      "Episode 372 , Reward: -200.0\n",
      "Episode: 373 starts\n",
      "Episode 373 , Reward: -200.0\n",
      "Episode: 374 starts\n",
      "Episode 374 , Reward: -200.0\n",
      "Episode: 375 starts\n",
      "Episode 375 , Reward: -200.0\n",
      "Episode: 376 starts\n",
      "Episode 376 , Reward: -200.0\n",
      "Episode: 377 starts\n",
      "Episode 377 , Reward: -200.0\n",
      "Episode: 378 starts\n",
      "Episode 378 , Reward: -200.0\n",
      "Episode: 379 starts\n",
      "Episode 379 , Reward: -200.0\n",
      "Episode: 380 starts\n",
      "Episode 380 , Reward: -200.0\n",
      "Episode: 381 starts\n",
      "Episode 381 , Reward: -200.0\n",
      "Episode: 382 starts\n",
      "Episode 382 , Reward: -200.0\n",
      "Episode: 383 starts\n",
      "Episode 383 , Reward: -200.0\n",
      "Episode: 384 starts\n",
      "Episode 384 , Reward: -200.0\n",
      "Episode: 385 starts\n",
      "Episode 385 , Reward: -200.0\n",
      "Episode: 386 starts\n",
      "Episode 386 , Reward: -200.0\n",
      "Episode: 387 starts\n",
      "Episode 387 , Reward: -200.0\n",
      "Episode: 388 starts\n",
      "Episode 388 , Reward: -200.0\n",
      "Episode: 389 starts\n",
      "Episode 389 , Reward: -200.0\n",
      "Episode: 390 starts\n",
      "Episode 390 , Reward: -200.0\n",
      "Episode: 391 starts\n",
      "Episode 391 , Reward: -200.0\n",
      "Episode: 392 starts\n",
      "Episode 392 , Reward: -200.0\n",
      "Episode: 393 starts\n",
      "Episode 393 , Reward: -200.0\n",
      "Episode: 394 starts\n",
      "Episode 394 , Reward: -200.0\n",
      "Episode: 395 starts\n",
      "Episode 395 , Reward: -200.0\n",
      "Episode: 396 starts\n",
      "Episode 396 , Reward: -200.0\n",
      "Episode: 397 starts\n",
      "Episode 397 , Reward: -200.0\n",
      "Episode: 398 starts\n",
      "Episode 398 , Reward: -200.0\n",
      "Episode: 399 starts\n",
      "Episode 399 , Reward: -200.0\n",
      "Episode: 400 starts\n",
      "Episode 400 , Reward: -200.0\n",
      "Episode: 401 starts\n",
      "Episode 401 , Reward: -200.0\n",
      "Episode: 402 starts\n",
      "Episode 402 , Reward: -200.0\n",
      "Episode: 403 starts\n",
      "Episode 403 , Reward: -200.0\n",
      "Episode: 404 starts\n",
      "Episode 404 , Reward: -200.0\n",
      "Episode: 405 starts\n",
      "Episode 405 , Reward: -200.0\n",
      "Episode: 406 starts\n",
      "Episode 406 , Reward: -200.0\n",
      "Episode: 407 starts\n",
      "Episode 407 , Reward: -200.0\n",
      "Episode: 408 starts\n",
      "Episode 408 , Reward: -200.0\n",
      "Episode: 409 starts\n",
      "Episode 409 , Reward: -200.0\n",
      "Episode: 410 starts\n",
      "Episode 410 , Reward: -200.0\n",
      "Episode: 411 starts\n",
      "Episode 411 , Reward: -200.0\n",
      "Episode: 412 starts\n",
      "Episode 412 , Reward: -200.0\n",
      "Episode: 413 starts\n",
      "Episode 413 , Reward: -200.0\n",
      "Episode: 414 starts\n",
      "Episode 414 , Reward: -200.0\n",
      "Episode: 415 starts\n",
      "Episode 415 , Reward: -200.0\n",
      "Episode: 416 starts\n",
      "Episode 416 , Reward: -200.0\n",
      "Episode: 417 starts\n",
      "Episode 417 , Reward: -200.0\n",
      "Episode: 418 starts\n",
      "Episode 418 , Reward: -200.0\n",
      "Episode: 419 starts\n",
      "Episode 419 , Reward: -200.0\n",
      "Episode: 420 starts\n",
      "Episode 420 , Reward: -200.0\n",
      "Episode: 421 starts\n",
      "Episode 421 , Reward: -200.0\n",
      "Episode: 422 starts\n",
      "Episode 422 , Reward: -200.0\n",
      "Episode: 423 starts\n",
      "Episode 423 , Reward: -200.0\n",
      "Episode: 424 starts\n",
      "Episode 424 , Reward: -200.0\n",
      "Episode: 425 starts\n",
      "Episode 425 , Reward: -200.0\n",
      "Episode: 426 starts\n",
      "Episode 426 , Reward: -200.0\n",
      "Episode: 427 starts\n",
      "Episode 427 , Reward: -200.0\n",
      "Episode: 428 starts\n",
      "Episode 428 , Reward: -200.0\n",
      "Episode: 429 starts\n",
      "Episode 429 , Reward: -200.0\n",
      "Episode: 430 starts\n",
      "Episode 430 , Reward: -200.0\n",
      "Episode: 431 starts\n",
      "Episode 431 , Reward: -200.0\n",
      "Episode: 432 starts\n",
      "Episode 432 , Reward: -200.0\n",
      "Episode: 433 starts\n",
      "Episode 433 , Reward: -200.0\n",
      "Episode: 434 starts\n",
      "Episode 434 , Reward: -200.0\n",
      "Episode: 435 starts\n",
      "Episode 435 , Reward: -200.0\n",
      "Episode: 436 starts\n",
      "Episode 436 , Reward: -200.0\n",
      "Episode: 437 starts\n",
      "Episode 437 , Reward: -200.0\n",
      "Episode: 438 starts\n",
      "Episode 438 , Reward: -200.0\n",
      "Episode: 439 starts\n",
      "Episode 439 , Reward: -200.0\n",
      "Episode: 440 starts\n",
      "Episode 440 , Reward: -200.0\n",
      "Episode: 441 starts\n",
      "Episode 441 , Reward: -200.0\n",
      "Episode: 442 starts\n",
      "Episode 442 , Reward: -200.0\n",
      "Episode: 443 starts\n",
      "Episode 443 , Reward: -200.0\n",
      "Episode: 444 starts\n",
      "Episode 444 , Reward: -200.0\n",
      "Episode: 445 starts\n",
      "Episode 445 , Reward: -200.0\n",
      "Episode: 446 starts\n",
      "Episode 446 , Reward: -200.0\n",
      "Episode: 447 starts\n",
      "Episode 447 , Reward: -200.0\n",
      "Episode: 448 starts\n",
      "Episode 448 , Reward: -200.0\n",
      "Episode: 449 starts\n",
      "Episode 449 , Reward: -200.0\n",
      "Episode: 450 starts\n",
      "Episode 450 , Reward: -200.0\n",
      "Episode: 451 starts\n",
      "Episode 451 , Reward: -200.0\n",
      "Episode: 452 starts\n",
      "Episode 452 , Reward: -200.0\n",
      "Episode: 453 starts\n",
      "Episode 453 , Reward: -200.0\n",
      "Episode: 454 starts\n",
      "Episode 454 , Reward: -200.0\n",
      "Episode: 455 starts\n"
     ]
    }
   ],
   "source": [
    "A.train(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bd389a-7f50-449b-85aa-00527071c7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a59d81-bb3c-4b7d-8a07-62b8c25dcbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Prof\n",
    "\n",
    "def value_iteration(self, theta=0.001):\n",
    "    \"\"\"\n",
    "    P : 3D array representing transition probabilities, P[s,a,s'] is the probability of transitioning from s to s' under action a.\n",
    "    R : 2D array representing rewards for each state-action pair\n",
    "    gamma : discount factor\n",
    "    theta : stopping criterion\n",
    "    \"\"\"\n",
    "    V = np.zeros(self.n_states)\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(self.n_states):\n",
    "            v = V[s]\n",
    "            # Calculate the value of each action in the current state\n",
    "            action_values = np.zeros(self.n_actions)\n",
    "            for a in range(self.n_actions):\n",
    "                action_values[a] = np.sum(self.P[s, a] * (self.R[s, a] + self.gamma * v))\n",
    "            # Update the value function\n",
    "            V[s] = np.max(action_values)\n",
    "            # Update the change in value function\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "\n",
    "        # If the change in value function is smaller than theta, stop\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    policy = np.zeros(self.n_states, dtype=int)\n",
    "    for s in range(self.n_states):\n",
    "        # Calculate the value of each action in the current state\n",
    "        action_values = np.zeros(self.n_actions)\n",
    "        for a in range(self.n_actions):\n",
    "            action_values[a] = np.sum(self.P[s, a] * (self.R[s, a] + self.gamma * V))\n",
    "        # Choose the action with the maximum value\n",
    "        policy[s] = np.argmax(action_values)\n",
    "    \n",
    "    return policy, V"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
