{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9968fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "78a654c6-f5ed-4da6-96e9-6a415baa0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MLP\n",
    "class Multi_Layer_Perceptron(nn.Sequential):\n",
    "    def __init__(self, input_dim, intern_dim, output_dim, depth = 2, isBiased = False):\n",
    "        \n",
    "        dict = OrderedDict([(\"input\",nn.Linear(input_dim,intern_dim, bias=isBiased))])\n",
    "        for i in range(depth):\n",
    "            dict.update({str(i) : nn.Linear(intern_dim,intern_dim,bias=isBiased)})\n",
    "        dict.update({\"output\" : nn.Linear(intern_dim,output_dim,bias=isBiased)})\n",
    "\n",
    "        super().__init__(dict)\n",
    "\n",
    "        self.reset_init_weights_biases(0) # so that we do not use a default initialization\n",
    "\n",
    "    def reset_init_weights_biases(self, norm = None):\n",
    "        for layer in self.children():\n",
    "            if norm == None:\n",
    "                stdv = 1. / math.sqrt(layer.weight.size(1))\n",
    "            else :\n",
    "                stdv = norm\n",
    "            \n",
    "            layer.weight.data.uniform_(-stdv, stdv)\n",
    "            if layer.bias is not None:\n",
    "                layer.bias.data.uniform_(-stdv, stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "388bf0ae-b403-4a5f-a87e-1431b33b1405",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CustomLoss DQN\n",
    "class DQN_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN_Loss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        loss = (target - input)**2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fdb52ffe-7290-4c94-8850-db3c480348f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def select_action(self, state):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, episodes, debug_mode=False):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, id, env):\n",
    "        self.id = id\n",
    "        self.env = env\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "48ce199a-cc03-418a-bcf2-c205d954304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        pass\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        return random.randint(0,1)\n",
    "        \n",
    "        \n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, episodes, debug_mode=False):\n",
    "        episodesHistory = np.zeros((episodes))\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            if(debug_mode) : print(\"Episode: \"+str(i+1)+\" starts\")\n",
    "            newSeed = random.randint(0,100000)\n",
    "            state,_ = self.env.reset(seed = newSeed)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                                        \n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                self.observe(state,action,next_state,reward)\n",
    "                self.update()\n",
    "\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "\n",
    "            episodesHistory[i] = episode_reward\n",
    "            if(debug_mode) : print(\"Episode \"+str(i+1)+ \" , Reward: \"+str(episode_reward))\n",
    "        print(episodesHistory[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "228a8b98-8bed-4b33-9097-fa858886d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(Agent):\n",
    "\n",
    "    def __init__(self, id, epsilonMax, epsilonMin = 0.05, Q = Multi_Layer_Perceptron(input_dim = 3,intern_dim = 64, output_dim = 1, depth = 2, isBiased = True), env = gym.make('MountainCar-v0'), arrayNewReward = None, gamma = 0.99, replay_buffer_SIZE = 10000, batch_size = 64, observation_SIZE = 6, optimizer = torch.optim.AdamW):\n",
    "        Agent.__init__(self,id,env)\n",
    "        self.Q = Q\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilonMax\n",
    "        self.epsilonMax = epsilonMax\n",
    "        self.epsilonMin = epsilonMin\n",
    "        self.replay_buffer = np.zeros((replay_buffer_SIZE,observation_SIZE))\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer(self.Q.parameters())\n",
    "        self.replay_buffer_SIZE = replay_buffer_SIZE\n",
    "        self.observation_SIZE = observation_SIZE\n",
    "        self.arrayNewReward = arrayNewReward\n",
    "        \n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        state = np.array(state)\n",
    "        action = np.array([action])\n",
    "        next_state = np.array(next_state)\n",
    "        reward = np.array([reward])\n",
    "\n",
    "        concatenatation = np.concatenate((state, action, next_state, reward))\n",
    "        return concatenatation\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        P = random.uniform(0,1)\n",
    "        a=0\n",
    "        if P < 1-self.epsilon :\n",
    "            A=np.zeros(3)\n",
    "            for k in range(2):\n",
    "                A[k] = self.Q(torch.from_numpy(np.concatenate((np.array(state), np.array([k])))).to(torch.float32))\n",
    "            a = np.argmax(A)\n",
    "        else:\n",
    "            a = random.randint(0,2)\n",
    "            \n",
    "        return a\n",
    "        \n",
    "        \n",
    "    def update(self):\n",
    "\n",
    "        LossFct = DQN_Loss()\n",
    "        \n",
    "        batch = np.random.choice(self.replay_buffer.shape[0], self.batch_size)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "\n",
    "            A0 = torch.from_numpy(np.concatenate((self.replay_buffer[batch[i],3:5], np.array([0])))).to(torch.float32)\n",
    "            A1 = torch.from_numpy(np.concatenate((self.replay_buffer[batch[i],3:5], np.array([1])))).to(torch.float32)\n",
    "            target = self.replay_buffer[batch[i],5] + self.gamma*max(self.Q(A0),self.Q(A1))\n",
    "            input = self.Q(torch.from_numpy(self.replay_buffer[batch[i],:3]).to(torch.float32))\n",
    "\n",
    "            loss = LossFct(input, target)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            #print(loss.item())\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def customReward(self, action, currentReward, next_state, uniqueReward, excludePassivity):\n",
    "        reward = 0\n",
    "\n",
    "        if excludePassivity and action == 1:\n",
    "            return -1\n",
    "        \n",
    "        if self.arrayNewReward.any() == None:\n",
    "            return reward\n",
    "            \n",
    "        for k in range(self.arrayNewReward.shape[0]):\n",
    "            #print(i)\n",
    "            i = self.arrayNewReward[k,0]\n",
    "            if (i <= next_state[0] and i +0.4 >=0) or (i >= next_state[0] and i +0.6 <=0):\n",
    "                if self.arrayNewReward[k,2] == 0:\n",
    "                    reward = self.arrayNewReward[k,1]\n",
    "                if uniqueReward: self.arrayNewReward[k,2] = 1\n",
    "        return reward\n",
    "\n",
    "    def play(self):\n",
    "        self.epsilon = 0\n",
    "        newSeed = random.randint(0,100000)\n",
    "        state,_ = self.env.reset(seed = newSeed)\n",
    "        done = False\n",
    "                    \n",
    "        while done == False:\n",
    "                                        \n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "        self.env.close()\n",
    "    \n",
    "    def train(self, episodes, debug_mode=False, recap_mode=False, reset_init = False, epsilon_decrease = True, uniqueReward = False, excludePassivity = True):\n",
    "        episodesHistory = np.zeros((episodes))\n",
    "        if reset_init: self.Q.reset_init_weights_biases(0)\n",
    "        \n",
    "        for e in range(episodes):\n",
    "            if debug_mode: print(\"Episode: \"+str(e+1)+\" starts\")\n",
    "                \n",
    "            if epsilon_decrease: \n",
    "                if self.epsilon > self.epsilonMin:\n",
    "                    self.epsilon = self.epsilonMax*math.exp(-e/10)\n",
    "            else:\n",
    "                self.epsilon = self.epsilonMax\n",
    "                \n",
    "            newSeed = random.randint(0,100000)\n",
    "            state,_ = self.env.reset(seed = newSeed)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            self.replay_buffer = np.zeros((self.replay_buffer_SIZE, self.observation_SIZE))\n",
    "            if self.arrayNewReward.any() != None:\n",
    "                for i in range(self.arrayNewReward.shape[0]):\n",
    "                    self.arrayNewReward[i,2] == 0\n",
    "            k=0\n",
    "            \n",
    "            while done == False:\n",
    "                                        \n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                reward = self.customReward(action,reward, next_state, uniqueReward, excludePassivity)\n",
    "                if debug_mode: print(\"Action \"+str(k)+\" selected: \"+str(action)+\" Reward: \"+ str(reward))\n",
    "                observe = self.observe(state,action,next_state,reward)\n",
    "                self.replay_buffer[k] = observe\n",
    "                k+=1\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "\n",
    "            if debug_mode or recap_mode: print(\"Episode \"+str(e+1)+ \" , Reward: \"+str(episode_reward)+\" Epsilon: \"+str(self.epsilon))\n",
    "            self.update()\n",
    "            episodesHistory[e] = episode_reward\n",
    "        return episodesHistory[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "0a93ea22-7ebd-4ce1-bcd3-077dbe1a2991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 , Reward: -866.0 Epsilon: 1.0\n",
      "Episode 2 , Reward: -830.0 Epsilon: 0.9048374180359595\n",
      "Episode 3 , Reward: -722.0 Epsilon: 0.8187307530779818\n",
      "Episode 4 , Reward: -614.0 Epsilon: 0.7408182206817179\n",
      "Episode 5 , Reward: -551.0 Epsilon: 0.6703200460356393\n",
      "Episode 6 , Reward: -542.0 Epsilon: 0.6065306597126334\n",
      "Episode 7 , Reward: -596.0 Epsilon: 0.5488116360940265\n",
      "Episode 8 , Reward: -533.0 Epsilon: 0.4965853037914095\n",
      "Episode 9 , Reward: -477.0 Epsilon: 0.44932896411722156\n",
      "Episode 10 , Reward: -506.0 Epsilon: 0.4065696597405991\n",
      "Episode 11 , Reward: -461.0 Epsilon: 0.36787944117144233\n",
      "Episode 12 , Reward: -461.0 Epsilon: 0.33287108369807955\n",
      "Episode 13 , Reward: -443.0 Epsilon: 0.30119421191220214\n",
      "Episode 14 , Reward: -380.0 Epsilon: 0.2725317930340126\n",
      "Episode 15 , Reward: -353.0 Epsilon: 0.2465969639416065\n",
      "Episode 16 , Reward: -272.0 Epsilon: 0.22313016014842982\n",
      "Episode 17 , Reward: -353.0 Epsilon: 0.20189651799465538\n",
      "Episode 18 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 19 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 20 , Reward: -371.0 Epsilon: 0.18268352405273466\n",
      "Episode 21 , Reward: -254.0 Epsilon: 0.18268352405273466\n",
      "Episode 22 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 23 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 24 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 25 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 26 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 27 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 28 , Reward: -389.0 Epsilon: 0.18268352405273466\n",
      "Episode 29 , Reward: -353.0 Epsilon: 0.18268352405273466\n",
      "Episode 30 , Reward: -254.0 Epsilon: 0.18268352405273466\n",
      "Episode 31 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 32 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 33 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 34 , Reward: -263.0 Epsilon: 0.18268352405273466\n",
      "Episode 35 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 36 , Reward: -254.0 Epsilon: 0.18268352405273466\n",
      "Episode 37 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 38 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 39 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 40 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 41 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 42 , Reward: -362.0 Epsilon: 0.18268352405273466\n",
      "Episode 43 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 44 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 45 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 46 , Reward: -227.0 Epsilon: 0.18268352405273466\n",
      "Episode 47 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 48 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 49 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 50 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 51 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 52 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 53 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 54 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 55 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 56 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 57 , Reward: -380.0 Epsilon: 0.18268352405273466\n",
      "Episode 58 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 59 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 60 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 61 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 62 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 63 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 64 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 65 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 66 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 67 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 68 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 69 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 70 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 71 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 72 , Reward: -380.0 Epsilon: 0.18268352405273466\n",
      "Episode 73 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 74 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 75 , Reward: -353.0 Epsilon: 0.18268352405273466\n",
      "Episode 76 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 77 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 78 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 79 , Reward: -245.0 Epsilon: 0.18268352405273466\n",
      "Episode 80 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 81 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 82 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 83 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 84 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 85 , Reward: -353.0 Epsilon: 0.18268352405273466\n",
      "Episode 86 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 87 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 88 , Reward: -353.0 Epsilon: 0.18268352405273466\n",
      "Episode 89 , Reward: -353.0 Epsilon: 0.18268352405273466\n",
      "Episode 90 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 91 , Reward: -389.0 Epsilon: 0.18268352405273466\n",
      "Episode 92 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 93 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 94 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 95 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 96 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 97 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 98 , Reward: -254.0 Epsilon: 0.18268352405273466\n",
      "Episode 99 , Reward: -254.0 Epsilon: 0.18268352405273466\n",
      "Episode 100 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 101 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 102 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 103 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 104 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 105 , Reward: -254.0 Epsilon: 0.18268352405273466\n",
      "Episode 106 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 107 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 108 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 109 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 110 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 111 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 112 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 113 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 114 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 115 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 116 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 117 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 118 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 119 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 120 , Reward: -353.0 Epsilon: 0.18268352405273466\n",
      "Episode 121 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 122 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 123 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 124 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 125 , Reward: -353.0 Epsilon: 0.18268352405273466\n",
      "Episode 126 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 127 , Reward: -362.0 Epsilon: 0.18268352405273466\n",
      "Episode 128 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 129 , Reward: -263.0 Epsilon: 0.18268352405273466\n",
      "Episode 130 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 131 , Reward: -353.0 Epsilon: 0.18268352405273466\n",
      "Episode 132 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 133 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 134 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 135 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 136 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 137 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 138 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 139 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 140 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 141 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 142 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 143 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 144 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 145 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 146 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 147 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 148 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 149 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 150 , Reward: -245.0 Epsilon: 0.18268352405273466\n",
      "Episode 151 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 152 , Reward: -389.0 Epsilon: 0.18268352405273466\n",
      "Episode 153 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 154 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 155 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 156 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 157 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 158 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 159 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 160 , Reward: -353.0 Epsilon: 0.18268352405273466\n",
      "Episode 161 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 162 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 163 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 164 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 165 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 166 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 167 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 168 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 169 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 170 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 171 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 172 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 173 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 174 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 175 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 176 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 177 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 178 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 179 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 180 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 181 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 182 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 183 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 184 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 185 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 186 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 187 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 188 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 189 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 190 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 191 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 192 , Reward: -263.0 Epsilon: 0.18268352405273466\n",
      "Episode 193 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 194 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 195 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 196 , Reward: -263.0 Epsilon: 0.18268352405273466\n",
      "Episode 197 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 198 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 199 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 200 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 201 , Reward: -353.0 Epsilon: 0.18268352405273466\n",
      "Episode 202 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 203 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 204 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 205 , Reward: -362.0 Epsilon: 0.18268352405273466\n",
      "Episode 206 , Reward: -362.0 Epsilon: 0.18268352405273466\n",
      "Episode 207 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 208 , Reward: -263.0 Epsilon: 0.18268352405273466\n",
      "Episode 209 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 210 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 211 , Reward: -254.0 Epsilon: 0.18268352405273466\n",
      "Episode 212 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 213 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 214 , Reward: -263.0 Epsilon: 0.18268352405273466\n",
      "Episode 215 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 216 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 217 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 218 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 219 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 220 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 221 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 222 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 223 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 224 , Reward: -389.0 Epsilon: 0.18268352405273466\n",
      "Episode 225 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 226 , Reward: -353.0 Epsilon: 0.18268352405273466\n",
      "Episode 227 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 228 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 229 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 230 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 231 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 232 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 233 , Reward: -245.0 Epsilon: 0.18268352405273466\n",
      "Episode 234 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 235 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 236 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 237 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 238 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 239 , Reward: -346.0 Epsilon: 0.18268352405273466\n",
      "Episode 240 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 241 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 242 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 243 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 244 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 245 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 246 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 247 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 248 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 249 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 250 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 251 , Reward: -389.0 Epsilon: 0.18268352405273466\n",
      "Episode 252 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 253 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 254 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 255 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 256 , Reward: -245.0 Epsilon: 0.18268352405273466\n",
      "Episode 257 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 258 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 259 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 260 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 261 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 262 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 263 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 264 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 265 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 266 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 267 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 268 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 269 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 270 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 271 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 272 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 273 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 274 , Reward: -353.0 Epsilon: 0.18268352405273466\n",
      "Episode 275 , Reward: -362.0 Epsilon: 0.18268352405273466\n",
      "Episode 276 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 277 , Reward: -398.0 Epsilon: 0.18268352405273466\n",
      "Episode 278 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 279 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 280 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 281 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 282 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 283 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 284 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 285 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 286 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 287 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 288 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 289 , Reward: -236.0 Epsilon: 0.18268352405273466\n",
      "Episode 290 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 291 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 292 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 293 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 294 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 295 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 296 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 297 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 298 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 299 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 300 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 301 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 302 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 303 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 304 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 305 , Reward: -371.0 Epsilon: 0.18268352405273466\n",
      "Episode 306 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 307 , Reward: -263.0 Epsilon: 0.18268352405273466\n",
      "Episode 308 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 309 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 310 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 311 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 312 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 313 , Reward: -362.0 Epsilon: 0.18268352405273466\n",
      "Episode 314 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 315 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 316 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 317 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 318 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 319 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 320 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 321 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 322 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 323 , Reward: -362.0 Epsilon: 0.18268352405273466\n",
      "Episode 324 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 325 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 326 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 327 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 328 , Reward: -263.0 Epsilon: 0.18268352405273466\n",
      "Episode 329 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 330 , Reward: -371.0 Epsilon: 0.18268352405273466\n",
      "Episode 331 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 332 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 333 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 334 , Reward: -254.0 Epsilon: 0.18268352405273466\n",
      "Episode 335 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 336 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 337 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 338 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 339 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 340 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 341 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 342 , Reward: -371.0 Epsilon: 0.18268352405273466\n",
      "Episode 343 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 344 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 345 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 346 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 347 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 348 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 349 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 350 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 351 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 352 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 353 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 354 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 355 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 356 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 357 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 358 , Reward: -362.0 Epsilon: 0.18268352405273466\n",
      "Episode 359 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 360 , Reward: -389.0 Epsilon: 0.18268352405273466\n",
      "Episode 361 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 362 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 363 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 364 , Reward: -353.0 Epsilon: 0.18268352405273466\n",
      "Episode 365 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 366 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 367 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 368 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 369 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 370 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 371 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 372 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 373 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 374 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 375 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 376 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 377 , Reward: -380.0 Epsilon: 0.18268352405273466\n",
      "Episode 378 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 379 , Reward: -263.0 Epsilon: 0.18268352405273466\n",
      "Episode 380 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 381 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 382 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 383 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 384 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 385 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 386 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 387 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 388 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 389 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 390 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 391 , Reward: -254.0 Epsilon: 0.18268352405273466\n",
      "Episode 392 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 393 , Reward: -263.0 Epsilon: 0.18268352405273466\n",
      "Episode 394 , Reward: -263.0 Epsilon: 0.18268352405273466\n",
      "Episode 395 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 396 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 397 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 398 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 399 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 400 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 401 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 402 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 403 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 404 , Reward: -263.0 Epsilon: 0.18268352405273466\n",
      "Episode 405 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 406 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 407 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 408 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 409 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 410 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 411 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 412 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 413 , Reward: -263.0 Epsilon: 0.18268352405273466\n",
      "Episode 414 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 415 , Reward: -371.0 Epsilon: 0.18268352405273466\n",
      "Episode 416 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 417 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 418 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 419 , Reward: -380.0 Epsilon: 0.18268352405273466\n",
      "Episode 420 , Reward: -263.0 Epsilon: 0.18268352405273466\n",
      "Episode 421 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 422 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 423 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 424 , Reward: -254.0 Epsilon: 0.18268352405273466\n",
      "Episode 425 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 426 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 427 , Reward: -353.0 Epsilon: 0.18268352405273466\n",
      "Episode 428 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 429 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 430 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 431 , Reward: -389.0 Epsilon: 0.18268352405273466\n",
      "Episode 432 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 433 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 434 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 435 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 436 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 437 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 438 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 439 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 440 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 441 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 442 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 443 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 444 , Reward: -263.0 Epsilon: 0.18268352405273466\n",
      "Episode 445 , Reward: -263.0 Epsilon: 0.18268352405273466\n",
      "Episode 446 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 447 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 448 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 449 , Reward: -254.0 Epsilon: 0.18268352405273466\n",
      "Episode 450 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 451 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 452 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 453 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 454 , Reward: -353.0 Epsilon: 0.18268352405273466\n",
      "Episode 455 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 456 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 457 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 458 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 459 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 460 , Reward: -353.0 Epsilon: 0.18268352405273466\n",
      "Episode 461 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 462 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 463 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 464 , Reward: -371.0 Epsilon: 0.18268352405273466\n",
      "Episode 465 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 466 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 467 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 468 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 469 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 470 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 471 , Reward: -371.0 Epsilon: 0.18268352405273466\n",
      "Episode 472 , Reward: -371.0 Epsilon: 0.18268352405273466\n",
      "Episode 473 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 474 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 475 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 476 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 477 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 478 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 479 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 480 , Reward: -389.0 Epsilon: 0.18268352405273466\n",
      "Episode 481 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 482 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 483 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 484 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 485 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 486 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 487 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 488 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 489 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 490 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 491 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 492 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 493 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 494 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 495 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 496 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 497 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 498 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 499 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 500 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 501 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 502 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 503 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 504 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 505 , Reward: -236.0 Epsilon: 0.18268352405273466\n",
      "Episode 506 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 507 , Reward: -371.0 Epsilon: 0.18268352405273466\n",
      "Episode 508 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 509 , Reward: -353.0 Epsilon: 0.18268352405273466\n",
      "Episode 510 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 511 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 512 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 513 , Reward: -254.0 Epsilon: 0.18268352405273466\n",
      "Episode 514 , Reward: -344.0 Epsilon: 0.18268352405273466\n",
      "Episode 515 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 516 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 517 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 518 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 519 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 520 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 521 , Reward: -254.0 Epsilon: 0.18268352405273466\n",
      "Episode 522 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 523 , Reward: -362.0 Epsilon: 0.18268352405273466\n",
      "Episode 524 , Reward: -254.0 Epsilon: 0.18268352405273466\n",
      "Episode 525 , Reward: -362.0 Epsilon: 0.18268352405273466\n",
      "Episode 526 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 527 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 528 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 529 , Reward: -263.0 Epsilon: 0.18268352405273466\n",
      "Episode 530 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 531 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 532 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 533 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 534 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 535 , Reward: -353.0 Epsilon: 0.18268352405273466\n",
      "Episode 536 , Reward: -263.0 Epsilon: 0.18268352405273466\n",
      "Episode 537 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 538 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 539 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 540 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 541 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 542 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 543 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 544 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 545 , Reward: -263.0 Epsilon: 0.18268352405273466\n",
      "Episode 546 , Reward: -272.0 Epsilon: 0.18268352405273466\n",
      "Episode 547 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 548 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 549 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 550 , Reward: -326.0 Epsilon: 0.18268352405273466\n",
      "Episode 551 , Reward: -263.0 Epsilon: 0.18268352405273466\n",
      "Episode 552 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 553 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 554 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 555 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 556 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 557 , Reward: -362.0 Epsilon: 0.18268352405273466\n",
      "Episode 558 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 559 , Reward: -281.0 Epsilon: 0.18268352405273466\n",
      "Episode 560 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 561 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 562 , Reward: -335.0 Epsilon: 0.18268352405273466\n",
      "Episode 563 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 564 , Reward: -362.0 Epsilon: 0.18268352405273466\n",
      "Episode 565 , Reward: -308.0 Epsilon: 0.18268352405273466\n",
      "Episode 566 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 567 , Reward: -362.0 Epsilon: 0.18268352405273466\n",
      "Episode 568 , Reward: -290.0 Epsilon: 0.18268352405273466\n",
      "Episode 569 , Reward: -245.0 Epsilon: 0.18268352405273466\n",
      "Episode 570 , Reward: -299.0 Epsilon: 0.18268352405273466\n",
      "Episode 571 , Reward: -362.0 Epsilon: 0.18268352405273466\n",
      "Episode 572 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 573 , Reward: -317.0 Epsilon: 0.18268352405273466\n",
      "Episode 574 , Reward: -272.0 Epsilon: 0.18268352405273466\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[178], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m NewReward\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.85\u001b[39m,\u001b[38;5;241m25\u001b[39m,\u001b[38;5;241m0\u001b[39m),(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m0\u001b[39m),(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m0\u001b[39m),(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m0\u001b[39m),(\u001b[38;5;241m0.40\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m0.5\u001b[39m,\u001b[38;5;241m200\u001b[39m,\u001b[38;5;241m0\u001b[39m)])\n\u001b[0;32m      2\u001b[0m DQN \u001b[38;5;241m=\u001b[39m DQNAgent(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m, epsilonMin \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m, arrayNewReward \u001b[38;5;241m=\u001b[39m NewReward)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mDQN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecap_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_init\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniqueReward\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexcludePassivity\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[173], line 128\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[1;34m(self, episodes, debug_mode, recap_mode, reset_init, epsilon_decrease, uniqueReward, excludePassivity)\u001b[0m\n\u001b[0;32m    125\u001b[0m         done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m debug_mode \u001b[38;5;129;01mor\u001b[39;00m recap_mode: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(e\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m , Reward: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(episode_reward)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Epsilon: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon))\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m     episodesHistory[e] \u001b[38;5;241m=\u001b[39m episode_reward\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m episodesHistory[:]\n",
      "Cell \u001b[1;32mIn[173], line 50\u001b[0m, in \u001b[0;36mDQNAgent.update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     48\u001b[0m A0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mconcatenate((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer[batch[i],\u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m5\u001b[39m], np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m]))))\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     49\u001b[0m A1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mconcatenate((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer[batch[i],\u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m5\u001b[39m], np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m]))))\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 50\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer[batch[i],\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer[batch[i],:\u001b[38;5;241m3\u001b[39m])\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m     53\u001b[0m loss \u001b[38;5;241m=\u001b[39m LossFct(\u001b[38;5;28minput\u001b[39m, target)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NewReward=np.array([(-0.85,25,0),(-0.7,15,0),(-0.2,10,0),(0,20,0),(0.40,3,0), (0.5,200,0)])\n",
    "DQN = DQNAgent(0,1, epsilonMin = 0.2, arrayNewReward = NewReward)\n",
    "DQN.train(5000, debug_mode = False, recap_mode=True, reset_init = True, uniqueReward = True, excludePassivity = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c4cd15d7-4c99-4ea6-8dc0-a6b57044168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN.env = gym.make('MountainCar-v0', render_mode='human')\n",
    "DQN.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3606f5-081e-461b-bdda-0684f0acaf37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
