{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9968fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78a654c6-f5ed-4da6-96e9-6a415baa0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MLP\n",
    "class Multi_Layer_Perceptron(nn.Sequential):\n",
    "    def __init__(self, input_dim, intern_dim, output_dim, depth = 2, isBiased = False):\n",
    "        \n",
    "        dict = OrderedDict([(\"input\",nn.Linear(input_dim,intern_dim, bias=isBiased))])\n",
    "        for i in range(depth):\n",
    "            dict.update({str(i) : nn.Linear(intern_dim,intern_dim,bias=isBiased)})\n",
    "        dict.update({\"output\" : nn.Linear(intern_dim,output_dim,bias=isBiased)})\n",
    "\n",
    "        super().__init__(dict)\n",
    "\n",
    "        self.reset_init_weights_biases(0) # so that we do not use a default initialization\n",
    "\n",
    "    def reset_init_weights_biases(self, norm = None):\n",
    "        for layer in self.children():\n",
    "            if norm == None:\n",
    "                stdv = 1. / math.sqrt(layer.weight.size(1))\n",
    "            else :\n",
    "                stdv = norm\n",
    "            \n",
    "            layer.weight.data.uniform_(-stdv, stdv)\n",
    "            if layer.bias is not None:\n",
    "                layer.bias.data.uniform_(-stdv, stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "388bf0ae-b403-4a5f-a87e-1431b33b1405",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CustomLoss DQN\n",
    "class DQN_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN_Loss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        loss = (target - input)**2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fdb52ffe-7290-4c94-8850-db3c480348f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def select_action(self, state):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, episodes, debug_mode=False):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, id, env):\n",
    "        self.id = id\n",
    "        self.env = env\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48ce199a-cc03-418a-bcf2-c205d954304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        pass\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        return random.randint(0,1)\n",
    "        \n",
    "        \n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, episodes, debug_mode=False):\n",
    "        episodesHistory = np.zeros((episodes))\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            if(debug_mode) : print(\"Episode: \"+str(i+1)+\" starts\")\n",
    "            newSeed = random.randint(0,100000)\n",
    "            state,_ = self.env.reset(seed = newSeed)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                                        \n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                self.observe(state,action,next_state,reward)\n",
    "                self.update()\n",
    "\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "\n",
    "            episodesHistory[i] = episode_reward\n",
    "            if(debug_mode) : print(\"Episode \"+str(i+1)+ \" , Reward: \"+str(episode_reward))\n",
    "        print(episodesHistory[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "228a8b98-8bed-4b33-9097-fa858886d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(Agent):\n",
    "\n",
    "    def __init__(self, id, epsilon, Q = Multi_Layer_Perceptron(input_dim = 3,intern_dim = 64, output_dim = 1, depth = 2, isBiased = True), env = gym.make('MountainCar-v0'), gamma = 0.99, replay_buffer_SIZE = 10000, batch_size = 64, observation_SIZE = 6, optimizer = torch.optim.AdamW):\n",
    "        Agent.__init__(self,id,env)\n",
    "        self.Q = Q\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.replay_buffer = np.zeros((replay_buffer_SIZE,observation_SIZE))\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer(self.Q.parameters())\n",
    "        self.replay_buffer_SIZE = replay_buffer_SIZE\n",
    "        self.observation_SIZE = observation_SIZE\n",
    "        \n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        state = np.array(state)\n",
    "        action = np.array([action])\n",
    "        next_state = np.array(next_state)\n",
    "        reward = np.array([reward])\n",
    "\n",
    "        concatenatation = np.concatenate((state, action, next_state, reward))\n",
    "        return concatenatation\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        return random.randint(0,1)\n",
    "        \n",
    "        \n",
    "    def update(self):\n",
    "\n",
    "        LossFct = DQN_Loss()\n",
    "        \n",
    "        batch = np.random.choice(self.replay_buffer.shape[0], self.batch_size)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "\n",
    "            A0 = torch.from_numpy(np.concatenate((self.replay_buffer[batch[i]][3:5], np.array([0])))).to(torch.float32)\n",
    "            A1 = torch.from_numpy(np.concatenate((self.replay_buffer[batch[i]][3:5], np.array([1])))).to(torch.float32)\n",
    "            target = self.replay_buffer[batch[i]][5] + self.gamma*max(self.Q(A0),self.Q(A1))\n",
    "            input = self.Q(torch.from_numpy(self.replay_buffer[batch[i]][:3]).to(torch.float32))\n",
    "\n",
    "            loss = LossFct(input, target)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            print(loss.item())\n",
    "            self.optimizer.step()\n",
    "        \n",
    "\n",
    "    def train(self, episodes, debug_mode=False):\n",
    "        episodesHistory = np.zeros((episodes))\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            if debug_mode: print(\"Episode: \"+str(i+1)+\" starts\")\n",
    "            newSeed = random.randint(0,100000)\n",
    "            state,_ = self.env.reset(seed = newSeed)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            self.replay_buffer = np.zeros((self.replay_buffer_SIZE, self.observation_SIZE))\n",
    "            k=0\n",
    "            \n",
    "            while not done:\n",
    "                                        \n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                observe = self.observe(state,action,next_state,reward)\n",
    "                self.replay_buffer[k] = observe\n",
    "                k+=1\n",
    "\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "\n",
    "            if debug_mode: print(\"Episode \"+str(i+1)+ \" , Reward: \"+str(episode_reward))\n",
    "            self.update()\n",
    "            episodesHistory[i] = episode_reward\n",
    "        if debug_mode: print(episodesHistory[:])\n",
    "        return episodesHistory[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "59b87236-f076-4348-b990-d2b0d4999b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 starts\n",
      "Episode 1 , Reward: -200.0\n",
      "Batch:[6758 2163 7584 1619 7861 7418 8055 7707 3407   85 8727 2473 9342 3705\n",
      " 5782  883 5866 8468 9197 7057 3355 3537 7111  726 4010 7490 6326 6276\n",
      " 3479 4344 4704 1196 3826 5705 3716 9555 2983 6896 6714 3797 2469 4322\n",
      " 4485 2236 6330 9248 6769 3678 8997 1716 3440  358 3965 2566 2322 3726\n",
      " 5616 6013 1861 7696 3083 6794 5411 4063]\n",
      "[6758 2163 7584 1619 7861 7418 8055 7707 3407   85 8727 2473 9342 3705\n",
      " 5782  883 5866 8468 9197 7057 3355 3537 7111  726 4010 7490 6326 6276\n",
      " 3479 4344 4704 1196 3826 5705 3716 9555 2983 6896 6714 3797 2469 4322\n",
      " 4485 2236 6330 9248 6769 3678 8997 1716 3440  358 3965 2566 2322 3726\n",
      " 5616 6013 1861 7696 3083 6794 5411 4063]\n",
      "[-200.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-200.])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DQN = DQNAgent(0,0.9)\n",
    "DQN.train(1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a93ea22-7ebd-4ce1-bcd3-077dbe1a2991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
