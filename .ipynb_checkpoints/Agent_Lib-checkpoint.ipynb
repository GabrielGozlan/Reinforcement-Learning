{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9968fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 2,
=======
   "execution_count": 89,
   "id": "78a654c6-f5ed-4da6-96e9-6a415baa0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MLP\n",
    "class Multi_Layer_Perceptron(nn.Sequential):\n",
    "    def __init__(self, input_dim, intern_dim, output_dim, depth = 2, isBiased = False):\n",
    "        \n",
    "        dict = OrderedDict([(\"input\",nn.Linear(input_dim,intern_dim, bias=isBiased))])\n",
    "        for i in range(depth):\n",
    "            dict.update({str(i) : nn.Linear(intern_dim,intern_dim,bias=isBiased)})\n",
    "        dict.update({\"output\" : nn.Linear(intern_dim,output_dim,bias=isBiased)})\n",
    "\n",
    "        super().__init__(dict)\n",
    "\n",
    "        self.reset_init_weights_biases(0) # so that we do not use a default initialization\n",
    "\n",
    "    def reset_init_weights_biases(self, norm = None):\n",
    "        for layer in self.children():\n",
    "            if norm == None:\n",
    "                stdv = 1. / math.sqrt(layer.weight.size(1))\n",
    "            else :\n",
    "                stdv = norm\n",
    "            \n",
    "            layer.weight.data.uniform_(-stdv, stdv)\n",
    "            if layer.bias is not None:\n",
    "                layer.bias.data.uniform_(-stdv, stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "388bf0ae-b403-4a5f-a87e-1431b33b1405",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CustomLoss DQN\n",
    "class DQN_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN_Loss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        loss = (target - input)**2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
>>>>>>> Stashed changes
   "id": "fdb52ffe-7290-4c94-8850-db3c480348f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def select_action(self, state):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, episodes, debug_mode=False):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, id, env):\n",
    "        self.id = id\n",
    "        self.env = env\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 3,
=======
   "execution_count": 92,
>>>>>>> Stashed changes
   "id": "48ce199a-cc03-418a-bcf2-c205d954304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        pass\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        return random.randint(0,1)\n",
    "        \n",
    "        \n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, episodes, debug_mode=False):\n",
    "        episodesHistory = np.zeros((episodes))\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            if(debug_mode) : print(\"Episode: \"+str(i+1)+\" starts\")\n",
    "            newSeed = random.randint(0,100000)\n",
    "            state,_ = self.env.reset(seed = newSeed)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                                        \n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                self.observe(state,action,next_state,reward)\n",
    "                self.update()\n",
    "\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "\n",
    "            episodesHistory[i] = episode_reward\n",
    "            if(debug_mode) : print(\"Episode \"+str(i+1)+ \" , Reward: \"+str(episode_reward))\n",
    "        print(episodesHistory[:])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 7,
=======
   "execution_count": 93,
>>>>>>> Stashed changes
   "id": "228a8b98-8bed-4b33-9097-fa858886d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(Agent):\n",
    "\n",
    "    def __init__(self, id, Q, epsilon, env = gym.make('MountainCar-v0'), gamma = 0.99, replay_buffer_SIZE = 100000, batch_size = 64, optimizer = torch.optim.AdamW):\n",
    "        Agent.__init__(self,id,env)\n",
    "        self.Q = Q\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.replay_buffer = np.zero((replay_buffer_SIZE))\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        pass\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        return random.randint(0,1)\n",
    "        \n",
    "        \n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, episodes, debug_mode=False):\n",
    "        episodesHistory = np.zeros((episodes))\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            print(\"Episode: \"+str(i+1)+\" starts\")\n",
    "            newSeed = random.randint(0,100000)\n",
    "            state,_ = self.env.reset(seed = newSeed)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                                        \n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                self.observe(state,action,next_state,reward)\n",
    "                self.update()\n",
    "\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "\n",
    "            episodesHistory[i] = episode_reward\n",
<<<<<<< Updated upstream
    "            print(\"Episode \"+str(i+1)+ \" , Reward: \"+str(episode_reward))\n",
    "        print(episodesHistory[:])"
=======
    "        return episodesHistory[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b8b9b53e-568d-42f7-90a7-3003d3866ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, arrayNewReward, render = None, **kwargs):\n",
    "        super(CustomEnv, self).__init__(**kwargs)\n",
    "        self.basicEnv = gym.make('MountainCar-v0', render_mode=render)\n",
    "        self.arrayNewReward = arrayNewReward\n",
    "\n",
    "    def reset(self,**kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        for i in self.arrayNewReward[:][0]:\n",
    "            if i == next_state[0]:\n",
    "                reward = self.arrayNewReward[i][1]\n",
    "                break\n",
    "        return next_state, reward, terminated, truncated, info \n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "59b87236-f076-4348-b990-d2b0d4999b4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CustomEnv' object has no attribute 'env'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m DQN \u001b[38;5;241m=\u001b[39m DQNAgent(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0.1\u001b[39m, env\u001b[38;5;241m=\u001b[39m CustomEnv(arrayNewReward\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.3\u001b[39m,\u001b[38;5;241m10\u001b[39m),(\u001b[38;5;241m0.3\u001b[39m,\u001b[38;5;241m5\u001b[39m)])))\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(DQN\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m1000\u001b[39m, debug_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "Cell \u001b[1;32mIn[93], line 77\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[1;34m(self, episodes, debug_mode)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m debug_mode: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m starts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     76\u001b[0m newSeed \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m100000\u001b[39m)\n\u001b[1;32m---> 77\u001b[0m state,_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(seed \u001b[38;5;241m=\u001b[39m newSeed)\n\u001b[0;32m     78\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     79\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[94], line 8\u001b[0m, in \u001b[0;36mCustomEnv.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CustomEnv' object has no attribute 'env'"
     ]
    }
   ],
   "source": [
    "DQN = DQNAgent(0,0.1, env= CustomEnv(arrayNewReward=np.array([(-0.3,10),(0.3,5)])))\n",
    "print(DQN.train(1000, debug_mode = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0a93ea22-7ebd-4ce1-bcd3-077dbe1a2991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 starts\n",
      "()\n",
      "[0. 0. 0.]\n",
      "Action 0 selected: 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DQNAgent.customReward() got multiple values for argument 'currentReward'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m NewReward\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.3\u001b[39m,\u001b[38;5;241m10\u001b[39m),(\u001b[38;5;241m0.3\u001b[39m,\u001b[38;5;241m5\u001b[39m),(\u001b[38;5;241m0.4\u001b[39m,\u001b[38;5;241m20\u001b[39m),(\u001b[38;5;241m0.6\u001b[39m,\u001b[38;5;241m200\u001b[39m)])\n\u001b[0;32m      2\u001b[0m DQN \u001b[38;5;241m=\u001b[39m DQNAgent(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0.1\u001b[39m, arrayNewReward \u001b[38;5;241m=\u001b[39m NewReward)\n\u001b[1;32m----> 3\u001b[0m DQN\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m1\u001b[39m, debug_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m DQN\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[93], line 88\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[1;34m(self, episodes, debug_mode)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m debug_mode: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(k)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m selected: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(action))\n\u001b[0;32m     87\u001b[0m next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m---> 88\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustomReward(currentReward \u001b[38;5;241m=\u001b[39m reward)\n\u001b[0;32m     89\u001b[0m observe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobserve(state,action,next_state,reward)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer[k] \u001b[38;5;241m=\u001b[39m observe\n",
      "\u001b[1;31mTypeError\u001b[0m: DQNAgent.customReward() got multiple values for argument 'currentReward'"
     ]
    }
   ],
   "source": [
    "NewReward=np.array([(-0.3,10),(0.3,5),(0.4,20),(0.6,200)])\n",
    "DQN = DQNAgent(0,0.1, arrayNewReward = NewReward)\n",
    "DQN.train(1, debug_mode = True)\n",
    "DQN.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c4cd15d7-4c99-4ea6-8dc0-a6b57044168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN.env.close()"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b87236-f076-4348-b990-d2b0d4999b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "05ed1213-6b94-42d5-a690-b9773bf1ddd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function zeros in module numpy:\n",
      "\n",
      "zeros(...)\n",
      "    zeros(shape, dtype=float, order='C', *, like=None)\n",
      "\n",
      "    Return a new array of given shape and type, filled with zeros.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    shape : int or tuple of ints\n",
      "        Shape of the new array, e.g., ``(2, 3)`` or ``2``.\n",
      "    dtype : data-type, optional\n",
      "        The desired data-type for the array, e.g., `numpy.int8`.  Default is\n",
      "        `numpy.float64`.\n",
      "    order : {'C', 'F'}, optional, default: 'C'\n",
      "        Whether to store multi-dimensional data in row-major\n",
      "        (C-style) or column-major (Fortran-style) order in\n",
      "        memory.\n",
      "    like : array_like, optional\n",
      "        Reference object to allow the creation of arrays which are not\n",
      "        NumPy arrays. If an array-like passed in as ``like`` supports\n",
      "        the ``__array_function__`` protocol, the result will be defined\n",
      "        by it. In this case, it ensures the creation of an array object\n",
      "        compatible with that passed in via this argument.\n",
      "\n",
      "        .. versionadded:: 1.20.0\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    out : ndarray\n",
      "        Array of zeros with the given shape, dtype, and order.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    zeros_like : Return an array of zeros with shape and type of input.\n",
      "    empty : Return a new uninitialized array.\n",
      "    ones : Return a new array setting values to one.\n",
      "    full : Return a new array of given shape filled with value.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> np.zeros(5)\n",
      "    array([ 0.,  0.,  0.,  0.,  0.])\n",
      "\n",
      "    >>> np.zeros((5,), dtype=int)\n",
      "    array([0, 0, 0, 0, 0])\n",
      "\n",
      "    >>> np.zeros((2, 1))\n",
      "    array([[ 0.],\n",
      "           [ 0.]])\n",
      "\n",
      "    >>> s = (2,2)\n",
      "    >>> np.zeros(s)\n",
      "    array([[ 0.,  0.],\n",
      "           [ 0.,  0.]])\n",
      "\n",
      "    >>> np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtype\n",
      "    array([(0, 0), (0, 0)],\n",
      "          dtype=[('x', '<i4'), ('y', '<i4')])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a9d5320b-10f7-4db9-bd34-1778a26e622e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ufunc:\n",
      "\n",
      "ceil = <ufunc 'ceil'>\n",
      "    ceil(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "\n",
      "    Return the ceiling of the input, element-wise.\n",
      "\n",
      "    The ceil of the scalar `x` is the smallest integer `i`, such that\n",
      "    ``i >= x``.  It is often denoted as :math:`\\lceil x \\rceil`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    x : array_like\n",
      "        Input data.\n",
      "    out : ndarray, None, or tuple of ndarray and None, optional\n",
      "        A location into which the result is stored. If provided, it must have\n",
      "        a shape that the inputs broadcast to. If not provided or None,\n",
      "        a freshly-allocated array is returned. A tuple (possible only as a\n",
      "        keyword argument) must have length equal to the number of outputs.\n",
      "    where : array_like, optional\n",
      "        This condition is broadcast over the input. At locations where the\n",
      "        condition is True, the `out` array will be set to the ufunc result.\n",
      "        Elsewhere, the `out` array will retain its original value.\n",
      "        Note that if an uninitialized `out` array is created via the default\n",
      "        ``out=None``, locations within it where the condition is False will\n",
      "        remain uninitialized.\n",
      "    **kwargs\n",
      "        For other keyword-only arguments, see the\n",
      "        :ref:`ufunc docs <ufuncs.kwargs>`.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    y : ndarray or scalar\n",
      "        The ceiling of each element in `x`, with `float` dtype.\n",
      "        This is a scalar if `x` is a scalar.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    floor, trunc, rint, fix\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> a = np.array([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0])\n",
      "    >>> np.ceil(a)\n",
      "    array([-1., -1., -0.,  1.,  2.,  2.,  2.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.ceil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4a06435e-96e4-4caf-8eb0-73aefc48168a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([72., 28.])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discr_step = np.array([0.025, 0.005])\n",
    "env = gym.make('MountainCar-v0')\n",
    "(env.observation_space.high - env.observation_space.low)//discr_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b2890ed6-ef38-493e-ae9e-49369fa24c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1905"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply.reduce(((env.observation_space.high-np.array([0.1, 0])-env.observation_space.low)//discr_step)).astype(np.int32) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5de2bd-d9c9-45de-90c0-c371eb5d55ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cf code cours, après avoir obtenu un modèle de l'environnement, résoudre le porblème d'optimisation avec le dynamic programming\n",
    "\n",
    "class DynaAgent(Agent):\n",
    "\n",
    "    # comment choisir k ? \n",
    "    \n",
    "    def __init__(self, id, env = gym.make('MountainCar-v0'), discr_step = np.array([0.025, 0.005]), gamma = 0.99, epsilon = 0.9, k=10):\n",
    "        Agent.__init__(self,id,env)\n",
    "        self.n_states = np.multiply.reduce(((env.observation_space.high - env.observation_space.low)//self.discr_step)).astype(np.int32)\n",
    "        self.n_actions = 3\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.k = k\n",
    "\n",
    "    # On obtient les s du (s, a, s') en discrétisant (cf plus haut), puis pour les s' on utilise une loi uniforme pour chaque paire (s, a)\n",
    "    \n",
    "    @property\n",
    "    def P(self):\n",
    "        shape = (self.n_states, self.n_actions, self.n_states) \n",
    "        P = np.zeros(shape=shape)\n",
    "        for i in range(self.n_states):\n",
    "            for j in range(self.n_actions):\n",
    "                random = torch.rand(size=(self.n_states,))\n",
    "                P[i, j, :] = random/random.sum()\n",
    "        return P\n",
    "\n",
    "    @property\n",
    "    def N(self):\n",
    "        shape = (self.n_states, self.n_actions, self.n_states) \n",
    "        return np.zeros(shape=shape)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def R(self):\n",
    "        shape = (self.n_states, self.n_actions) \n",
    "        return np.zeros(shape=shape)\n",
    "\n",
    "                     \n",
    "        \n",
    "    @property\n",
    "    def W(self):\n",
    "        shape = (self.n_states, self.n_actions) \n",
    "        W = - np.ones(shape=shape)\n",
    "        first_terminal_state = np.multiply.reduce(((env.observation_space.high-np.array([0.1, 0])-env.observation_space.low)//discr_step)).astype(np.int32) + 1\n",
    "\n",
    "        for state in range(first_terminal_state, self.n_states):\n",
    "            W[state, :] = 0\n",
    "\n",
    "        return W\n",
    "                                                 \n",
    "\n",
    "    @property\n",
    "    def Q(self):\n",
    "        shape = (self.n_states, self.n_actions) \n",
    "        return np.zeros(shape=shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def discretize(self, state):\n",
    "        bin_state = np.multiply.reduce(np.round(((state - env.observation_space.low)//self.discr_step))).astype(np.int32)\n",
    "        return bin_state\n",
    "\n",
    "    \n",
    "    def update(self, state, action, next_state, reward):\n",
    "        discr_state, discr_next_state = self.discretize(state), self.discretize(next_state)\n",
    "        self.N[discr_state, action, discr_next_state] += 1\n",
    "        self.P[discr_state, action, discr_next_state] = self.N[discr_state, action, discr_next_state]/(self.N[discr_state, action, :].sum())\n",
    "        self.W[discr_state, action] += reward\n",
    "        self.R[discr_state, action] = self.W[discr_state, action]/(self.N[discr_state, action, :].sum())\n",
    "\n",
    "        self.Q[discr_state, action] = self.R[discr_state, action] + (self.gamma)*np.array([(self.P[discr_state, action, discr_next_local])*max(self.Q[discr_next_local, :]) for discr_next_local in range(self.n_states)]).sum()\n",
    "\n",
    "        nb=1\n",
    "        while nb<self.k:\n",
    "            random_state = torch.randint(0, (self.n_states - 1), size=(1,))\n",
    "            random_action = torch.randint(0, (self.n_actions - 1), size=(1,))\n",
    "            if self.N[random_state, random_action, :].sum() > 0:\n",
    "                self.Q[random_state, random_action] = self.R[random_state, random_action] + (self.gamma)*np.array([(self.P[random_state, random_action, discr_next_local])*max(self.Q[discr_next_local, :]) for discr_next_local in range(self.n_states)]).sum()\n",
    "                nb+=1\n",
    "\n",
    "    def observe(self):\n",
    "        pass\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state_bin = self.discretize(state)\n",
    "        P = random.uniform(0,1)\n",
    "        a=0\n",
    "        if P < 1-self.epsilon :\n",
    "            a = np.argmax(Q[state_bin, :])\n",
    "        else:\n",
    "            a = random.randint(0,3)\n",
    "            \n",
    "        return a\n",
    "\n",
    "        \n",
    "    def train(self, episodes, debug_mode=True):\n",
    "        episodesHistory = np.zeros((episodes))\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            if debug_mode: print(\"Episode: \"+str(i+1)+\" starts\")\n",
    "            newSeed = random.randint(0,100000)\n",
    "            state,_ = self.env.reset(seed = newSeed)\n",
    "            \n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "\n",
    "                k=0\n",
    "                action = self.select_action(state)\n",
    "                if debug_mode: print(\"Action \"+str(k)+\" selected: \"+str(action))\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "\n",
    "                self.update(state, action, next_state, reward)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "\n",
    "            if debug_mode: print(\"Episode \"+str(i+1)+ \" , Reward: \"+str(episode_reward))\n",
    "           \n",
    "            episodesHistory[i] = episode_reward\n",
    "        return episodesHistory[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5693221-cf32-48f0-b7e2-ab3e79fb5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = DynaAgent(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca504496-ac20-4783-b9f3-b76505e5513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.train(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc9db4f-d27e-4dc3-9938-0fe2d715c035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
