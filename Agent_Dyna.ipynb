{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9968fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb52ffe-7290-4c94-8850-db3c480348f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def select_action(self, state):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, episodes, debug_mode=False):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, id, env):\n",
    "        self.id = id\n",
    "        self.env = env\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da8dc4c5-d0f8-4e79-b8ed-217f095a6532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([72., 28.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discr_step = np.array([0.025, 0.005])\n",
    "env = gym.make('MountainCar-v0')\n",
    "(env.observation_space.high - env.observation_space.low)//discr_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "345c3e60-bf01-42b8-968f-e8cf05543aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cf code cours, après avoir obtenu un modèle de l'environnement, résoudre le porblème d'optimisation avec le dynamic programming\n",
    "\n",
    "class DynaAgent(Agent):\n",
    "    \n",
    "    def __init__(self, id, env=gym.make('MountainCar-v0'), epsilonMax = 0.9, epsilonMin = 0.05, discr_step = np.array([0.05, 0.01]), gamma = 0.99, k=0, k_fixed = True, alpha=0.2, observation_SIZE = 6, replay_buffer_SIZE = 10000):\n",
    "        Agent.__init__(self,id,env)\n",
    "        self.discr_step = discr_step\n",
    "        self.n_xbins = np.round(((self.env.observation_space.high - self.env.observation_space.low)/self.discr_step)[0]).astype(np.int32)\n",
    "        print(self.n_xbins)\n",
    "        self.n_vbins = np.round(((self.env.observation_space.high - self.env.observation_space.low)/self.discr_step)[1]).astype(np.int32)\n",
    "        print(self.n_vbins)\n",
    "        self.n_states = self.n_xbins*self.n_vbins\n",
    "        self.n_actions = 3\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilonMax\n",
    "        self.epsilonMax = epsilonMax\n",
    "        self.epsilonMin = epsilonMin\n",
    "        self.k = k\n",
    "        self.k_fixed = k_fixed\n",
    "        self.alpha = alpha\n",
    "        '''\n",
    "        Definition of the replay buffer\n",
    "        '''\n",
    "        self.replay_buffer_SIZE = replay_buffer_SIZE\n",
    "        self.observation_SIZE = observation_SIZE\n",
    "        self.visited_state_action_Array = np.zeros((replay_buffer_SIZE,observation_SIZE))\n",
    "        self.visited_state_action = set()\n",
    "        \n",
    "        self.N = np.zeros((self.n_states, self.n_actions, self.n_states))\n",
    "        \n",
    "        self.P = np.zeros(shape=(self.n_states, self.n_actions, self.n_states))\n",
    "        for i in range(self.n_states):\n",
    "            for j in range(self.n_actions):\n",
    "                random = np.random.rand(self.n_states)\n",
    "                self.P[i, j, :] = random/random.sum()\n",
    "        \n",
    "        self.R = - np.ones(shape=(self.n_states, self.n_actions))\n",
    "        self.terminal_x_bin = self.discretize_x(0.5)*self.n_vbins\n",
    "        print(self.terminal_x_bin)\n",
    "        \n",
    "        self.Q = np.zeros(shape=(self.n_states, self.n_actions))\n",
    "    \n",
    "\n",
    "    # On obtient les s du (s, a, s') en discrétisant (cf plus haut), puis pour les s' on utilise une loi uniforme pour chaque paire (s, a)\n",
    "\n",
    "    def discretize_x(self, x):\n",
    "        x_bin = np.round(((x - self.env.observation_space.low)/self.discr_step)[0]).astype(np.int32)\n",
    "        return x_bin\n",
    "\n",
    "    def discretize_v(self, v):\n",
    "        v_bin = np.round(((v - self.env.observation_space.low)/self.discr_step)[1]).astype(np.int32)\n",
    "        return v_bin \n",
    "\n",
    "    def discretize(self, state):\n",
    "        x_bin = self.discretize_x(state[0])\n",
    "        v_bin = self.discretize_v(state[1])\n",
    "        return x_bin*self.n_vbins + v_bin\n",
    "\n",
    "    \n",
    "   \n",
    "    def update(self, state, action, next_state, reward):\n",
    "        discr_state, discr_next_state = self.discretize(state), self.discretize(next_state)\n",
    "\n",
    "        self.visited_state_action.add((discr_state,action))\n",
    "        self.N[discr_state,action, discr_next_state] += 1\n",
    "\n",
    "        \n",
    "        total_visited = self.N[discr_state,action,:].sum()\n",
    "\n",
    "        if total_visited > 0:\n",
    "            self.P[discr_state, action, :] = self.N[discr_state, action,  :] / total_visited\n",
    "            self.R[discr_state, action] = (self.R[discr_state, action]*(total_visited-1) + reward) / total_visited\n",
    "            \n",
    "        start = time.time()\n",
    "\n",
    "        if discr_state < self.terminal_x_bin:\n",
    "            self.Q[discr_state, action] = self.R[discr_state, action] + (self.gamma)*(self.P[discr_state, action,:]*np.max(self.Q, axis = 1)[:]).sum()\n",
    "        else:\n",
    "            self.Q[discr_state, action] = self.R[discr_state, action]\n",
    "            \n",
    "        if not self.k_fixed:\n",
    "            self.k = len(self.visited_state_action) // 10\n",
    "            print(\"K changes\")\n",
    "            \n",
    "        sampled_states = []\n",
    "        if self.k >= 1:\n",
    "            sampled_states = random.sample(self.visited_state_action, min(self.k, len(self.visited_state_action)))\n",
    "\n",
    "            for (random_state, random_action) in sampled_states:\n",
    "                if random_state < self.terminal_x_bin:\n",
    "                    self.Q[random_state, random_action] = self.R[random_state, random_action] + (self.gamma)*(self.P[random_state, random_action,:]*np.max(self.Q, axis = 1)[:]).sum()\n",
    "                else:\n",
    "                    self.Q[random_state, random_action] = self.R[random_state, random_action]\n",
    "        #print(time.time() - start)\n",
    "\n",
    "    def observe(self):\n",
    "        pass\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state_bin = self.discretize(state)\n",
    "        p = random.uniform(0,1)\n",
    "        a=0\n",
    "        if p < 1-self.epsilon :\n",
    "            a = np.argmax(self.Q[state_bin,:])\n",
    "        else:\n",
    "            a = random.randint(0,2)\n",
    "            \n",
    "        return a\n",
    "    '''\n",
    "    Select actions without exploration (for the tests)\n",
    "    '''  \n",
    "    def select_best_action(self, state):\n",
    "        state_bin = self.discretize(state)\n",
    "        return np.argmax(self.Q[state_bin,:])\n",
    "    '''\n",
    "    Test the agent on a seed (random or not) after the training\n",
    "    '''  \n",
    "    def play(self, seed = False):\n",
    "        newSeed = random.randint(0,100000)\n",
    "        \n",
    "        if seed != False:\n",
    "            newSeed = seed\n",
    "            \n",
    "        state,_ = self.env.reset(seed = newSeed)\n",
    "        done = False\n",
    "                    \n",
    "        while done == False:\n",
    "                                        \n",
    "            action = self.select_best_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "        self.env.close()\n",
    "\n",
    "    \"\"\"\n",
    "    # Reinitialisation de R nécessaire ?\n",
    "    def reset_for_episode(self):\n",
    "        self.N_episode = np.zeros(shape=(self.n_states, self.n_actions, self.n_states))\n",
    "        \n",
    "        self.W_episode = - np.ones(shape=(self.n_states, self.n_actions))\n",
    "        terminal_x_bin = self.discretize_x(0.5)\n",
    "        for state in range(terminal_x_bin, self.n_states):\n",
    "            self.W_episode[state, :] = 0\n",
    "\n",
    "    \"\"\"  \n",
    "\n",
    "    \n",
    "    def train(self, episodes, debug_mode=True, epsilon_decrease=True, epsilonDecreasing=100):\n",
    "        episodesHistory = np.zeros((episodes))\n",
    "        \n",
    "        for i in tqdm(range(episodes)):\n",
    "\n",
    "            if epsilon_decrease: \n",
    "                if self.epsilon > self.epsilonMin:\n",
    "                    self.epsilon = self.epsilonMax*math.exp(-np.e/epsilonDecreasing)\n",
    "            else:\n",
    "                self.epsilon = self.epsilonMax\n",
    "                \n",
    "            newSeed = random.randint(0,100000)\n",
    "            state,_ = self.env.reset(seed = newSeed)\n",
    "            \n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "            \n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "\n",
    "                self.update(state, action, next_state, reward)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "                if terminated: print(\"Terminated\")\n",
    "\n",
    "            #self.reset_for_episode()\n",
    "\n",
    "             \n",
    "            if debug_mode: print(\"Episode \"+str(i+1)+ \" , Reward: \"+str(episode_reward))\n",
    "            episodesHistory[i] = episode_reward\n",
    "        return episodesHistory[:].mean(), self.Q.mean(), self.R.mean(), self.P.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab3a1265-b585-4fea-bd39-bc8652333bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "14\n",
      "476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]C:\\Users\\triox\\AppData\\Local\\Temp\\ipykernel_164\\2159349105.py:87: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  sampled_states = random.sample(self.visited_state_action, min(self.k, len(self.visited_state_action)))\n",
      "100%|██████████| 1000/1000 [13:17<00:00,  1.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-200.0, -11.642944552566458, -1.0, 0.001984126984126983)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = DynaAgent(\"id0\", k = 200)\n",
    "A.train(1000, debug_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08fd16f2-9498-4806-873f-c0ca4969bc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "14\n",
      "476\n"
     ]
    }
   ],
   "source": [
    "Dyna_Test = DynaAgent(\"idTest\", env=gym.make('MountainCar-v0', render_mode='human'))\n",
    "Dyna_Test.Q = A.Q\n",
    "Dyna_Test.play(seed = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e45d38c-0435-4325-861a-f4b61ca841c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00393107 0.00396309 0.00401147]\n",
      " [0.00396915 0.00385213 0.00385197]\n",
      " [0.00398829 0.00411001 0.00390609]\n",
      " ...\n",
      " [0.00403816 0.00393271 0.00393767]\n",
      " [0.00377775 0.00394541 0.00406515]\n",
      " [0.00395459 0.00406054 0.00390906]]\n"
     ]
    }
   ],
   "source": [
    "print(np.max(A.P, axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bd389a-7f50-449b-85aa-00527071c7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a59d81-bb3c-4b7d-8a07-62b8c25dcbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Prof\n",
    "\n",
    "def value_iteration(self, theta=0.001):\n",
    "    \"\"\"\n",
    "    P : 3D array representing transition probabilities, P[s,a,s'] is the probability of transitioning from s to s' under action a.\n",
    "    R : 2D array representing rewards for each state-action pair\n",
    "    gamma : discount factor\n",
    "    theta : stopping criterion\n",
    "    \"\"\"\n",
    "    V = np.zeros(self.n_states)\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(self.n_states):\n",
    "            v = V[s]\n",
    "            # Calculate the value of each action in the current state\n",
    "            action_values = np.zeros(self.n_actions)\n",
    "            for a in range(self.n_actions):\n",
    "                action_values[a] = np.sum(self.P[s, a] * (self.R[s, a] + self.gamma * v))\n",
    "            # Update the value function\n",
    "            V[s] = np.max(action_values)\n",
    "            # Update the change in value function\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "\n",
    "        # If the change in value function is smaller than theta, stop\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    policy = np.zeros(self.n_states, dtype=int)\n",
    "    for s in range(self.n_states):\n",
    "        # Calculate the value of each action in the current state\n",
    "        action_values = np.zeros(self.n_actions)\n",
    "        for a in range(self.n_actions):\n",
    "            action_values[a] = np.sum(self.P[s, a] * (self.R[s, a] + self.gamma * V))\n",
    "        # Choose the action with the maximum value\n",
    "        policy[s] = np.argmax(action_values)\n",
    "    \n",
    "    return policy, V"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
