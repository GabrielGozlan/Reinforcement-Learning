{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9968fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78a654c6-f5ed-4da6-96e9-6a415baa0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MLP\n",
    "class Multi_Layer_Perceptron(nn.Sequential):\n",
    "    def __init__(self, input_dim, intern_dim, output_dim, depth = 2, isBiased = False):\n",
    "        \n",
    "        dict = OrderedDict([(\"input\",nn.Linear(input_dim,intern_dim, bias=isBiased))])\n",
    "        for i in range(depth):\n",
    "            dict.update({str(i) : nn.Linear(intern_dim,intern_dim,bias=isBiased)})\n",
    "        dict.update({\"output\" : nn.Linear(intern_dim,output_dim,bias=isBiased)})\n",
    "\n",
    "        super().__init__(dict)\n",
    "\n",
    "        self.reset_init_weights_biases(0) # so that we do not use a default initialization\n",
    "\n",
    "    def reset_init_weights_biases(self, norm = -1):\n",
    "        for layer in self.children():\n",
    "            if norm == -1:\n",
    "                stdv = 1. / math.sqrt(layer.weight.size(1))\n",
    "            else :\n",
    "                stdv = norm\n",
    "            layer.weight.data.fill_(stdv)\n",
    "            if layer.bias is not None:\n",
    "                layer.bias.data.fill_(stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdb52ffe-7290-4c94-8850-db3c480348f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def select_action(self, state):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, episodes, debug_mode=False):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, id, env):\n",
    "        self.id = id\n",
    "        self.env = env\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48ce199a-cc03-418a-bcf2-c205d954304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        pass\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        return random.randint(0,1)\n",
    "        \n",
    "        \n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, episodes, debug_mode=False):\n",
    "        episodesHistory = np.zeros((episodes))\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            if(debug_mode) : print(\"Episode: \"+str(i+1)+\" starts\")\n",
    "            newSeed = random.randint(0,100000)\n",
    "            state,_ = self.env.reset(seed = newSeed)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                                        \n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                self.observe(state,action,next_state,reward)\n",
    "                self.update()\n",
    "\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "\n",
    "            episodesHistory[i] = episode_reward\n",
    "            if(debug_mode) : print(\"Episode \"+str(i+1)+ \" , Reward: \"+str(episode_reward))\n",
    "        print(episodesHistory[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "228a8b98-8bed-4b33-9097-fa858886d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(Agent):\n",
    "\n",
    "    def __init__(self, id, epsilonMax, epsilonMin = 0.05, Q = Multi_Layer_Perceptron(input_dim = 3,intern_dim = 64, output_dim = 1, depth = 2, isBiased = False), env = gym.make('MountainCar-v0'), arrayNewPosReward = None, arrayNewVelReward = None, contReward = False, gamma = 0.99, replay_buffer_SIZE = 10000, batch_size = 64, observation_SIZE = 6, optimizer = torch.optim.AdamW):\n",
    "        Agent.__init__(self,id,env)\n",
    "        self.Q = Q\n",
    "        self.QTarget = Q\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilonMax\n",
    "        self.epsilonMax = epsilonMax\n",
    "        self.epsilonMin = epsilonMin\n",
    "        self.replay_buffer = np.zeros((replay_buffer_SIZE,observation_SIZE))\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer(self.Q.parameters())\n",
    "        self.replay_buffer_SIZE = replay_buffer_SIZE\n",
    "        self.observation_SIZE = observation_SIZE\n",
    "        self.arrayNewPosReward = arrayNewPosReward\n",
    "        self.arrayNewVelReward = arrayNewVelReward\n",
    "        self.contReward = contReward\n",
    "        \n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        state = np.array(state)\n",
    "        action = np.array([action])\n",
    "        next_state = np.array(next_state)\n",
    "        reward = np.array([reward])\n",
    "\n",
    "        concatenatation = np.concatenate((state, action, next_state, reward))\n",
    "        return concatenatation\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        P = random.uniform(0,1)\n",
    "        a=0\n",
    "        if P <= 1-self.epsilon :\n",
    "            A=np.zeros(3)\n",
    "            o = []\n",
    "            ArgQmax = 0\n",
    "            for k in range(3):\n",
    "                A[k] = self.Q(torch.from_numpy(np.concatenate((np.array(state), np.array([k])))).to(torch.float32))\n",
    "            a = np.argmax(A)\n",
    "            for k in range(3):\n",
    "                if A[k] == A[a]:\n",
    "                    o.append(k)\n",
    "            a =random.choice(o)\n",
    "                \n",
    "        else:\n",
    "            a = random.randint(0,2)\n",
    "        return a\n",
    "        \n",
    "        \n",
    "    def update(self,j):\n",
    "\n",
    "        LossFct = torch.nn.MSELoss(reduction='mean')\n",
    "        #print(min(j,self.replay_buffer.shape[0]))\n",
    "        batch = np.random.choice(min(j,self.replay_buffer.shape[0]), self.batch_size)\n",
    "        target = torch.zeros((self.batch_size))\n",
    "        input = torch.zeros((self.batch_size))\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "\n",
    "            if self.replay_buffer[batch[i],3] <0.5:\n",
    "                A0 = torch.from_numpy(np.concatenate((self.replay_buffer[batch[i],3:5], np.array([0])))).to(torch.float32)\n",
    "                A1 = torch.from_numpy(np.concatenate((self.replay_buffer[batch[i],3:5], np.array([1])))).to(torch.float32)\n",
    "                A2 = torch.from_numpy(np.concatenate((self.replay_buffer[batch[i],3:5], np.array([2])))).to(torch.float32)\n",
    "                target[i] = self.replay_buffer[batch[i],5] + self.gamma*max(self.QTarget(A0),self.QTarget(A1),self.QTarget(A2))\n",
    "            else:\n",
    "                target[i] = self.replay_buffer[batch[i],5]\n",
    "                \n",
    "            input[i] = self.Q(torch.from_numpy(self.replay_buffer[batch[i],:3]).to(torch.float32))\n",
    "\n",
    "        loss = LossFct(input, target)\n",
    "        for param in self.Q.parameters():\n",
    "            param.grad = None\n",
    "            \n",
    "        loss.backward()\n",
    "\n",
    "        grad = 0\n",
    "        for layer in self.Q.children():\n",
    "            grad += layer.weight.grad.mean()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        return loss.item(), abs(grad)\n",
    "\n",
    "    def customReward(self, state, action, currentReward, next_state, uniqueReward, excludePassivity):\n",
    "        reward = -1\n",
    "\n",
    "        if excludePassivity and action == 1:\n",
    "            reward -= 2\n",
    "\n",
    "        if self.contReward:\n",
    "            reward = (abs(next_state[0] + 0.5)**2)/200\n",
    "        \n",
    "        if self.arrayNewPosReward.all() != None:\n",
    "            for k in range(self.arrayNewPosReward.shape[0]):\n",
    "                #print(i)\n",
    "                i = self.arrayNewPosReward[k,0]\n",
    "                if (i <= next_state[0] and i +0.5 >=0) or (i >= next_state[0] and i +0.5 <=0):\n",
    "                    if self.arrayNewPosReward[k,2] == 0:\n",
    "                        if self.arrayNewPosReward[k,1] > reward:\n",
    "                            reward = self.arrayNewPosReward[k,1]\n",
    "                            if uniqueReward: self.arrayNewPosReward[k,2] = 1\n",
    "                        \n",
    "        if self.arrayNewVelReward.all() != None:\n",
    "            for k in range(self.arrayNewVelReward.shape[0]):\n",
    "                i = self.arrayNewVelReward[k,0]\n",
    "                if (i < state[1] and i>0 and action == 2) or (i > state[1] and i<0 and action == 0):\n",
    "                    reward += self.arrayNewVelReward[k,1]\n",
    "                    #print(state[1])\n",
    "                    #print(action)\n",
    "                    \n",
    "        return reward\n",
    "\n",
    "    def play(self):\n",
    "        self.epsilon = 0\n",
    "        newSeed = random.randint(0,100000)\n",
    "        state,_ = self.env.reset(seed = newSeed)\n",
    "        done = False\n",
    "                    \n",
    "        while done == False:\n",
    "                                        \n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "        self.env.close()\n",
    "    \n",
    "    def train(self, episodes,lossLim = 0, limitStep = 200, refreshTarget = 200, refreshQ = 1, buffer_fill = True, epsilonDecreasing =100, debug_mode=False, recap_mode=False, reset_init = False, epsilon_decrease = True, uniqueReward = False, excludePassivity = True):\n",
    "        episodesHistory = np.zeros((episodes))\n",
    "        rewardHistory = np.zeros((episodes*limitStep))\n",
    "        lossHistory = np.zeros((int(episodes*limitStep/refreshQ)))\n",
    "        gradHistory = np.zeros((int(episodes*limitStep/refreshQ)))\n",
    "        cumulativeHistory = np.zeros((episodes))\n",
    "        self.QTarget = self.Q\n",
    "        if reset_init != False: self.Q.reset_init_weights_biases(reset_init)\n",
    "        j=0\n",
    "        self.replay_buffer = np.zeros((self.replay_buffer_SIZE, self.observation_SIZE))\n",
    "        k=0\n",
    "        for e in range(episodes):\n",
    "            l=0\n",
    "            terminated = False\n",
    "            if debug_mode: print(\"Episode: \"+str(e+1)+\" starts\")\n",
    "                \n",
    "            if epsilon_decrease: \n",
    "                if self.epsilon > self.epsilonMin:\n",
    "                    self.epsilon = self.epsilonMax*math.exp(-e/epsilonDecreasing)\n",
    "            else:\n",
    "                self.epsilon = self.epsilonMax\n",
    "                \n",
    "            newSeed = random.randint(0,100000)\n",
    "            state,_ = self.env.reset(seed = newSeed)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            if self.arrayNewPosReward.all() != None:\n",
    "                for i in range(self.arrayNewPosReward.shape[0]):\n",
    "                    self.arrayNewPosReward[i,2] = 0\n",
    "\n",
    "            s=1\n",
    "            while done == False:\n",
    "                j+=1               \n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                \n",
    "                if s >= limitStep:\n",
    "                    truncated = True\n",
    "                else:\n",
    "                    truncated = False\n",
    "\n",
    "                s+=1\n",
    "                reward = self.customReward(state, action,reward, next_state, uniqueReward, excludePassivity)\n",
    "                if debug_mode: print(\"Action \"+str(k)+\" selected: \"+str(action)+\" Reward: \"+ str(reward))\n",
    "                observe = self.observe(state,action,next_state,reward)\n",
    "                \n",
    "                if k < self.replay_buffer_SIZE: self.replay_buffer[k] = observe\n",
    "                elif buffer_fill: k=-1\n",
    "                k+=1\n",
    "                \n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "                if j%refreshQ == 0:\n",
    "                    l,g = self.update(j)\n",
    "                    lossHistory[int(j/refreshQ)-1] = l\n",
    "                    gradHistory[int(j/refreshQ)-1] = g\n",
    "                if j%refreshTarget == 0:\n",
    "                    print(\"Loss: \"+str(l))\n",
    "                    self.QTarget = self.Q\n",
    "                    #for param in self.Q.parameters():\n",
    "                        #print(param.data)\n",
    "\n",
    "                rewardHistory[j-1] = reward\n",
    "                \n",
    "            if terminated:\n",
    "                if e > 0:\n",
    "                    cumulativeHistory[e] = cumulativeHistory[e-1] +1\n",
    "                else:\n",
    "                    cumulativeHistory[e] = 1\n",
    "            else:\n",
    "                if e > 0:\n",
    "                    cumulativeHistory[e] = cumulativeHistory[e-1]\n",
    "                else:\n",
    "                    cumulativeHistory[e] = 0\n",
    "                                    \n",
    "            if debug_mode or recap_mode: print(\"Episode \"+str(e+1)+ \" , Reward: \"+str(episode_reward)+\" Epsilon: \"+str(self.epsilon))\n",
    "            episodesHistory[e] = episode_reward\n",
    "            \n",
    "            if l <= lossLim:\n",
    "                print(\"Loss reaches limit\")\n",
    "                break\n",
    "        return episodesHistory, rewardHistory, lossHistory, gradHistory, cumulativeHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a93ea22-7ebd-4ce1-bcd3-077dbe1a2991",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 , Reward: -154.5 Epsilon: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m No \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[0;32m     14\u001b[0m DQN \u001b[38;5;241m=\u001b[39m DQNAgent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid0\u001b[39m\u001b[38;5;124m\"\u001b[39m,epsilonMax \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, epsilonMin \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m, batch_size \u001b[38;5;241m=\u001b[39m batch_size, contReward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, arrayNewPosReward \u001b[38;5;241m=\u001b[39m NewPosReward, arrayNewVelReward \u001b[38;5;241m=\u001b[39m NewVelReward, replay_buffer_SIZE \u001b[38;5;241m=\u001b[39m replay_buffer_SIZE)\n\u001b[1;32m---> 15\u001b[0m episodesHistory, rewardHistory, lossHistory, gradHistory, cumulativeHistory\u001b[38;5;241m=\u001b[39m DQN\u001b[38;5;241m.\u001b[39mtrain(episodes,lossLim\u001b[38;5;241m=\u001b[39mlossLimTrain,limitStep \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m,buffer_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, refreshTarget \u001b[38;5;241m=\u001b[39m refreshTarget, refreshQ \u001b[38;5;241m=\u001b[39m refreshQ, debug_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, recap_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, reset_init \u001b[38;5;241m=\u001b[39m reset_init, uniqueReward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, excludePassivity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, epsilonDecreasing \u001b[38;5;241m=\u001b[39mepsilonDecreasing)\n",
      "Cell \u001b[1;32mIn[5], line 178\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[1;34m(self, episodes, lossLim, limitStep, refreshTarget, refreshQ, buffer_fill, epsilonDecreasing, debug_mode, recap_mode, reset_init, epsilon_decrease, uniqueReward, excludePassivity)\u001b[0m\n\u001b[0;32m    176\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m j\u001b[38;5;241m%\u001b[39mrefreshQ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 178\u001b[0m     l,g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(j)\n\u001b[0;32m    179\u001b[0m     lossHistory[\u001b[38;5;28mint\u001b[39m(j\u001b[38;5;241m/\u001b[39mrefreshQ)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m l\n\u001b[0;32m    180\u001b[0m     gradHistory[\u001b[38;5;28mint\u001b[39m(j\u001b[38;5;241m/\u001b[39mrefreshQ)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m g\n",
      "Cell \u001b[1;32mIn[5], line 63\u001b[0m, in \u001b[0;36mDQNAgent.update\u001b[1;34m(self, j)\u001b[0m\n\u001b[0;32m     61\u001b[0m     A1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mconcatenate((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer[batch[i],\u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m5\u001b[39m], np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m]))))\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     62\u001b[0m     A2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mconcatenate((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer[batch[i],\u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m5\u001b[39m], np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m2\u001b[39m]))))\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 63\u001b[0m     target[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer[batch[i],\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQTarget(A0),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQTarget(A1),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQTarget(A2))\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     target[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer[batch[i],\u001b[38;5;241m5\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Project_Reinforcement_Learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Project_Reinforcement_Learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Project_Reinforcement_Learning\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Project_Reinforcement_Learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Project_Reinforcement_Learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Project_Reinforcement_Learning\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "u=0.25\n",
    "episodes = 1000\n",
    "lossLimTrain = -1\n",
    "batch_size = 500\n",
    "refreshQ = 100\n",
    "refreshTarget = 1000\n",
    "reset_init = 0.1\n",
    "replay_buffer_SIZE = 20000\n",
    "epsilonDecreasing = 200\n",
    "\n",
    "NewPosReward=np.array([(-1.1,u*7,0),(-1,u*6,0),(-0.9,u*5,0),(-0.8,u*4,0),(-0.7,u*3,0),(-0.6,u*2,0),(-0.4,u*2,0),(-0.3,u*3,0),(-0.2,u*4,0),(-0.1,u*5,0),(0,u*6,0),(0.1,u*7,0),(0.2,u*8,0),(0.3,u*9,0),(0.4,u*10,0),(0.5,20*u,0)])\n",
    "NewVelReward=np.array([(0.001,1),(-0.001,1)])\n",
    "No = np.array([None])\n",
    "DQN = DQNAgent(\"id0\",epsilonMax = 1, epsilonMin = 0.05, batch_size = batch_size, contReward = False, arrayNewPosReward = NewPosReward, arrayNewVelReward = NewVelReward, replay_buffer_SIZE = replay_buffer_SIZE)\n",
    "episodesHistory, rewardHistory, lossHistory, gradHistory, cumulativeHistory= DQN.train(episodes,lossLim=lossLimTrain,limitStep = 200,buffer_fill = True, refreshTarget = refreshTarget, refreshQ = refreshQ, debug_mode = False, recap_mode=True, reset_init = reset_init, uniqueReward = True, excludePassivity = False, epsilonDecreasing =epsilonDecreasing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dde4285-4eca-4908-80d1-b635bfc70b49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig1,ax1 = plt.subplots(1,1)\n",
    "ax1.plot(range(episodesHistory.shape[0]), episodesHistory, marker='.')\n",
    "ax1.set_xlabel(r'episodes $t$')\n",
    "ax1.set_ylabel(r'$Total Reward$')\n",
    "ax1.set_xscale('linear')\n",
    "ax1.set_yscale('linear')\n",
    "plt.grid(color='black', which=\"both\", linestyle='-', linewidth=0.2)\n",
    "plt.savefig(f'figures/TotalReward_epochs{str(episodes)}_u{str(u)}_bS{str(batch_size)}_rQ{str(refreshQ)}_rT{str(refreshTarget)}_rI{str(reset_init)}_rB{str(replay_buffer_SIZE)}_ep{str(epsilonDecreasing)}.png', format='png')\n",
    "\n",
    "fig2,ax2 = plt.subplots(1,1)\n",
    "ax2.plot(range(rewardHistory.shape[0]), rewardHistory, marker='.')\n",
    "ax2.set_xlabel(r'Step $t$')\n",
    "ax2.set_ylabel(r'$Reward$')\n",
    "ax2.set_xscale('linear')\n",
    "ax2.set_yscale('linear')\n",
    "plt.grid(color='black', which=\"both\", linestyle='-', linewidth=0.2)\n",
    "plt.savefig(f'figures/Reward_epochs{episodes}_u{u}_bS{batch_size}_rQ{refreshQ}_rT{refreshTarget}_rI{reset_init}_rB{replay_buffer_SIZE}_ep{epsilonDecreasing}.png', format='png')\n",
    "\n",
    "\n",
    "fig3,ax3 = plt.subplots(1,1)\n",
    "ax3.plot(range(lossHistory.shape[0]), lossHistory, marker='.')\n",
    "ax3.set_xlabel(r'Update $t$')\n",
    "ax3.set_ylabel(r'$Loss$')\n",
    "ax3.set_xscale('linear')\n",
    "ax3.set_yscale('log')\n",
    "#plt.ylim(top=0.15)\n",
    "plt.grid(color='black', which=\"both\", linestyle='-', linewidth=0.2)\n",
    "plt.savefig(f'figures/Loss_epochs{episodes}_u{u}_bS{batch_size}_rQ{refreshQ}_rT{refreshTarget}_rI{reset_init}_rB{replay_buffer_SIZE}_ep{epsilonDecreasing}.png', format='png')\n",
    "\n",
    "fig4,ax4 = plt.subplots(1,1)\n",
    "ax4.plot(range(gradHistory.shape[0]), gradHistory, marker='.')\n",
    "ax4.set_xlabel(r'Update $t$')\n",
    "ax4.set_ylabel(r'$Grad$')\n",
    "ax4.set_xscale('linear')\n",
    "ax4.set_yscale('log')\n",
    "plt.grid(color='black', which=\"both\", linestyle='-', linewidth=0.2)\n",
    "plt.savefig(f'figures/Grad_epochs{episodes}_u{u}_bS{batch_size}_rQ{refreshQ}_rT{refreshTarget}_rI{reset_init}_rB{replay_buffer_SIZE}_ep{epsilonDecreasing}.png', format='png')\n",
    "\n",
    "\n",
    "s=0\n",
    "for e in range(cumulativeHistory.shape[0]):\n",
    "    if cumulativeHistory[e] == 1:\n",
    "        s = e\n",
    "        break\n",
    "fig5,ax5 = plt.subplots(1,1)\n",
    "ax5.plot(range(cumulativeHistory.shape[0] - s), cumulativeHistory[s:], marker='.')\n",
    "ax5.set_xlabel(r'episodes: first_success: ep '+str(s))\n",
    "ax5.set_ylabel(r'$CumulativeSucess$')\n",
    "ax5.set_xscale('linear')\n",
    "ax5.set_yscale('linear')\n",
    "plt.grid(color='black', which=\"both\", linestyle='-', linewidth=0.2)\n",
    "plt.savefig(f'figures/CumulativeSucess_epochs{episodes}_u{u}_bS{batch_size}_rQ{refreshQ}_rT{refreshTarget}_rI{reset_init}_rB{replay_buffer_SIZE}_ep{epsilonDecreasing}.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4cd15d7-4c99-4ea6-8dc0-a6b57044168f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m DQN\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMountainCar-v0\u001b[39m\u001b[38;5;124m'\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m DQN\u001b[38;5;241m.\u001b[39mplay()\n",
      "Cell \u001b[1;32mIn[5], line 120\u001b[0m, in \u001b[0;36mDQNAgent.play\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m done \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[1;32m--> 120\u001b[0m     next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    121\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Project_Reinforcement_Learning\\Lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Project_Reinforcement_Learning\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Project_Reinforcement_Learning\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Project_Reinforcement_Learning\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\mountain_car.py:148\u001b[0m, in \u001b[0;36mMountainCarEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m (position, velocity)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Project_Reinforcement_Learning\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\mountain_car.py:266\u001b[0m, in \u001b[0;36mMountainCarEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    265\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_fps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    267\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DQN.env = gym.make('MountainCar-v0', render_mode='human')\n",
    "DQN.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac77d547-9e22-4e27-b569-d8835df67752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da8dc4c5-d0f8-4e79-b8ed-217f095a6532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([72., 28.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discr_step = np.array([0.025, 0.005])\n",
    "env = gym.make('MountainCar-v0')\n",
    "(env.observation_space.high - env.observation_space.low)//discr_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7520b9-d6a8-4719-bb1b-e3b674a32bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda6fbdd-728f-44f0-b029-bd07efb6e823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c3e60-bf01-42b8-968f-e8cf05543aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cf code cours, après avoir obtenu un modèle de l'environnement, résoudre le porblème d'optimisation avec le dynamic programming\n",
    "\n",
    "class DynaAgent(Agent):\n",
    "    \n",
    "    def __init__(self, id, epsilonMax = 0.9, epsilonMin = 0.05,env = gym.make('MountainCar-v0'), discr_step = np.array([0.025, 0.005]), gamma = 0.99, k=0, visited_state_action = set()):\n",
    "        Agent.__init__(self,id,env)\n",
    "        self.discr_step = discr_step\n",
    "        self.n_xbins = np.round(((env.observation_space.high - env.observation_space.low)/self.discr_step)[0]).astype(np.int32)\n",
    "        self.n_vbins = np.round(((env.observation_space.high - env.observation_space.low)/self.discr_step)[1]).astype(np.int32)\n",
    "        self.n_states = self.n_xbins*self.n_vbins\n",
    "        self.n_actions = 3\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilonMax\n",
    "        self.epsilonMax = epsilonMax\n",
    "        self.epsilonMin = epsilonMin\n",
    "        self.k = k\n",
    "        self.visited_state_action = visited_state_action\n",
    "        \n",
    "        self.N = np.zeros(shape=(self.n_states, self.n_actions, self.n_states))\n",
    "        \n",
    "        self.P = np.zeros(shape=(self.n_states, self.n_actions, self.n_states))\n",
    "        for i in range(self.n_states):\n",
    "            for j in range(self.n_actions):\n",
    "                random = torch.rand(size=(self.n_states,))\n",
    "                self.P[i, j, :] = random/random.sum()\n",
    "\n",
    "        self.R = np.zeros(shape=(self.n_states, self.n_actions))\n",
    "\n",
    "        self.W = - np.ones(shape=(self.n_states, self.n_actions))\n",
    "        terminal_x_bin = self.discretize_x(0.5)\n",
    "        for state in range(terminal_x_bin, self.n_states):\n",
    "            self.W[state, :] = 0\n",
    "\n",
    "        \n",
    "        self.Q = np.zeros(shape=(self.n_states, self.n_actions))\n",
    "    \n",
    "\n",
    "    # On obtient les s du (s, a, s') en discrétisant (cf plus haut), puis pour les s' on utilise une loi uniforme pour chaque paire (s, a)\n",
    "\n",
    "    def discretize_x(self, x):\n",
    "        x_bin = np.round(((x - env.observation_space.low)/discr_step)[0]).astype(np.int32)\n",
    "        return x_bin*(self.n_vbins) \n",
    "\n",
    "    def discretize_v(self, v):\n",
    "        v_bin = np.round(((v - env.observation_space.low)/discr_step)[1]).astype(np.int32)\n",
    "        return v_bin \n",
    "\n",
    "    def discretize(self, state):\n",
    "        x_bin = self.discretize_x(state[0])\n",
    "        v_bin = self.discretize_v(state[1])\n",
    "        return x_bin + v_bin\n",
    "\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def P(self):\n",
    "        shape = (self.n_states, self.n_actions, self.n_states) \n",
    "        P = np.zeros(shape=shape)\n",
    "        for i in range(self.n_states):\n",
    "            for j in range(self.n_actions):\n",
    "                random = torch.rand(size=(self.n_states,))\n",
    "                P[i, j, :] = random/random.sum()\n",
    "        return P\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def N(self):\n",
    "        shape = (self.n_states, self.n_actions, self.n_states) \n",
    "        return np.zeros(shape=shape)\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def R(self):\n",
    "        shape = (self.n_states, self.n_actions) \n",
    "        return np.zeros(shape=shape)\n",
    "\n",
    "                     \n",
    "        \n",
    "    @property\n",
    "    def W(self):\n",
    "        shape = (self.n_states, self.n_actions) \n",
    "        W = - np.ones(shape=shape)\n",
    "\n",
    "        terminal_x_bin = self.discretize_x(0.5)\n",
    "    \n",
    "        for state in range(terminal_x_bin, self.n_states):\n",
    "            W[state, :] = 0\n",
    "\n",
    "        return W\n",
    "                                                 \n",
    "\n",
    "    @property\n",
    "    def Q(self):\n",
    "        shape = (self.n_states, self.n_actions) \n",
    "        return np.zeros(shape=shape)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def update(self, state, action, next_state, reward):\n",
    "        discr_state, discr_next_state = self.discretize(state), self.discretize(next_state)\n",
    "        \n",
    "        self.N[discr_state, action, discr_next_state] += 1\n",
    "        self.visited_state_action.append((discr_state, action))\n",
    "\n",
    "        \n",
    "        self.P[discr_state, action, discr_next_state] = self.N[discr_state, action, discr_next_state]/(np.sum(self.N[discr_state, action]))\n",
    "        \n",
    "        self.W[discr_state, action] += reward\n",
    "        self.R[discr_state, action] = self.W[discr_state, action]/(np.sum(self.N[discr_state, action]))\n",
    "\n",
    "        self.Q[discr_state, action] = self.R[discr_state, action] + (self.gamma)*np.array([(self.P[discr_state, action, discr_next_local])*max(self.Q[discr_next_local, :]) for discr_next_local in range(self.n_states)]).sum()\n",
    "\n",
    "        self.k = int(len(self.visited_state_action)//10)\n",
    "\n",
    "\n",
    "        sampled_states = random.sample(self.visited_state_action, self.k)\n",
    "\n",
    "        for (random_state, random_action) in sampled_states:\n",
    "            self.Q[random_state, random_action] = self.R[random_state, random_action] + (self.gamma)*np.array([(self.P[random_state, random_action, discr_next_local])*max(self.Q[discr_next_local, :]) for discr_next_local in range(self.n_states)]).sum()\n",
    "            \n",
    "        \n",
    "    def observe(self):\n",
    "        pass\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state_bin = self.discretize(state)\n",
    "        p = random.uniform(0,1)\n",
    "        a=0\n",
    "        if p < 1-self.epsilon :\n",
    "            a = np.argmax(self.Q[state_bin, :])\n",
    "        else:\n",
    "            a = random.randint(0,2)\n",
    "            \n",
    "        return a\n",
    "\n",
    "\n",
    "    \n",
    "    # Reinitialisation de R, et N ?\n",
    "\n",
    "    \n",
    "    def train(self, episodes, debug_mode=True, epsilon_decrease=True, epsilonDecreasing=100):\n",
    "        episodesHistory = np.zeros((episodes))\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            if debug_mode: print(\"Episode: \"+str(i+1)+\" starts\")\n",
    "\n",
    "\n",
    "            if epsilon_decrease: \n",
    "                if self.epsilon > self.epsilonMin:\n",
    "                    self.epsilon = self.epsilonMax*math.exp(-np.e/epsilonDecreasing)\n",
    "            else:\n",
    "                self.epsilon = self.epsilonMax\n",
    "                \n",
    "            newSeed = random.randint(0,100000)\n",
    "            state,_ = self.env.reset(seed = newSeed)\n",
    "            \n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            k=0\n",
    "            \n",
    "            while not done:\n",
    "            \n",
    "                action = self.select_action(state)\n",
    "                if debug_mode: print(\"Action :\"+str(k)+\" selected: \"+str(action))\n",
    "                k+=1\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "\n",
    "                self.update(state, action, next_state, reward)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "\n",
    "            if debug_mode: print(\"Episode \"+str(i+1)+ \" , Reward: \"+str(episode_reward))\n",
    "           \n",
    "            episodesHistory[i] = episode_reward\n",
    "        return episodesHistory[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab3a1265-b585-4fea-bd39-bc8652333bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = DynaAgent(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08fd16f2-9498-4806-873f-c0ca4969bc5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[4.30088694e-04, 9.56921067e-05, 9.18481674e-04, ...,\n",
       "         4.14301205e-04, 3.00056185e-04, 6.76981814e-04],\n",
       "        [7.78097310e-04, 2.89272375e-05, 5.00049617e-04, ...,\n",
       "         7.00163364e-04, 2.72828853e-04, 4.14978742e-04],\n",
       "        [5.47259173e-04, 7.81918061e-05, 3.97732249e-04, ...,\n",
       "         1.33216163e-04, 9.01781314e-04, 8.26434931e-04]],\n",
       "\n",
       "       [[1.94012091e-06, 4.68581187e-04, 6.93704875e-04, ...,\n",
       "         6.02930668e-04, 1.86541933e-04, 1.63529912e-04],\n",
       "        [6.02302025e-04, 5.48511336e-04, 1.69379404e-04, ...,\n",
       "         5.71682285e-05, 6.87830732e-04, 9.94519214e-04],\n",
       "        [6.56312739e-04, 7.77193287e-04, 8.66895716e-04, ...,\n",
       "         1.04135383e-04, 1.18378601e-04, 4.84000018e-04]],\n",
       "\n",
       "       [[1.18075390e-04, 7.30634929e-05, 8.15048697e-04, ...,\n",
       "         3.85990890e-04, 3.59918195e-04, 7.94714782e-04],\n",
       "        [3.06812552e-04, 5.06804325e-04, 5.12513689e-05, ...,\n",
       "         8.07305798e-04, 3.44019616e-04, 3.80475074e-04],\n",
       "        [8.68938514e-04, 8.84373963e-04, 6.33959949e-04, ...,\n",
       "         1.27869702e-04, 6.14409684e-04, 5.02305396e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2.39106725e-04, 3.94786039e-04, 8.21460970e-04, ...,\n",
       "         1.19346059e-05, 8.46923213e-04, 9.56211297e-04],\n",
       "        [9.94846341e-04, 8.91918724e-04, 7.89342506e-04, ...,\n",
       "         7.05118233e-04, 3.94465023e-04, 5.90107404e-04],\n",
       "        [1.87055484e-04, 5.66852977e-04, 8.47968913e-04, ...,\n",
       "         9.35338961e-04, 6.85404288e-04, 4.78353526e-04]],\n",
       "\n",
       "       [[1.39819604e-04, 9.32260125e-04, 5.73336496e-04, ...,\n",
       "         9.39508842e-04, 8.35826853e-04, 6.08949398e-04],\n",
       "        [7.50046980e-04, 8.24455754e-04, 6.09486015e-04, ...,\n",
       "         6.59838770e-05, 9.75055795e-04, 3.26853595e-04],\n",
       "        [9.67705331e-04, 3.24103050e-04, 2.34917534e-04, ...,\n",
       "         6.98630989e-04, 5.88974624e-04, 1.89650455e-04]],\n",
       "\n",
       "       [[8.51164514e-04, 1.77542926e-04, 5.43466711e-04, ...,\n",
       "         8.18728062e-04, 1.35199743e-05, 2.18282701e-04],\n",
       "        [3.95332550e-04, 6.32210111e-04, 9.69136658e-04, ...,\n",
       "         8.96372716e-04, 9.31202900e-04, 5.03811345e-04],\n",
       "        [5.23971859e-04, 7.39622337e-04, 8.34061473e-04, ...,\n",
       "         1.23410618e-05, 3.56136472e-04, 9.79343822e-05]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c80cd3a1-7794-4c20-95fd-923c228c7b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 starts\n",
      "Action :0 selected: 1\n",
      "Action :1 selected: 2\n",
      "Action :2 selected: 0\n",
      "Action :3 selected: 1\n",
      "Action :4 selected: 0\n",
      "Action :5 selected: 0\n",
      "Action :6 selected: 2\n",
      "Action :7 selected: 0\n",
      "Action :8 selected: 2\n",
      "Action :9 selected: 2\n",
      "Action :10 selected: 0\n",
      "Action :11 selected: 1\n",
      "Action :12 selected: 0\n",
      "Action :13 selected: 2\n",
      "Action :14 selected: 2\n",
      "Action :15 selected: 2\n",
      "Action :16 selected: 2\n",
      "Action :17 selected: 1\n",
      "Action :18 selected: 0\n",
      "Action :19 selected: 0\n",
      "Action :20 selected: 0\n",
      "Action :21 selected: 0\n",
      "Action :22 selected: 0\n",
      "Action :23 selected: 0\n",
      "Action :24 selected: 0\n",
      "Action :25 selected: 0\n",
      "Action :26 selected: 1\n",
      "Action :27 selected: 0\n",
      "Action :28 selected: 1\n",
      "Action :29 selected: 0\n",
      "Action :30 selected: 2\n",
      "Action :31 selected: 2\n",
      "Action :32 selected: 2\n",
      "Action :33 selected: 1\n",
      "Action :34 selected: 0\n",
      "Action :35 selected: 1\n",
      "Action :36 selected: 0\n",
      "Action :37 selected: 1\n",
      "Action :38 selected: 2\n",
      "Action :39 selected: 0\n",
      "Action :40 selected: 1\n",
      "Action :41 selected: 1\n",
      "Action :42 selected: 2\n",
      "Action :43 selected: 0\n",
      "Action :44 selected: 2\n",
      "Action :45 selected: 2\n",
      "Action :46 selected: 2\n",
      "Action :47 selected: 1\n",
      "Action :48 selected: 2\n",
      "Action :49 selected: 1\n",
      "Action :50 selected: 2\n",
      "Action :51 selected: 0\n",
      "Action :52 selected: 2\n",
      "Action :53 selected: 0\n",
      "Action :54 selected: 1\n",
      "Action :55 selected: 2\n",
      "Action :56 selected: 2\n",
      "Action :57 selected: 0\n",
      "Action :58 selected: 1\n",
      "Action :59 selected: 2\n",
      "Action :60 selected: 2\n",
      "Action :61 selected: 2\n",
      "Action :62 selected: 2\n",
      "Action :63 selected: 1\n",
      "Action :64 selected: 1\n",
      "Action :65 selected: 0\n",
      "Action :66 selected: 0\n",
      "Action :67 selected: 0\n",
      "Action :68 selected: 1\n",
      "Action :69 selected: 1\n",
      "Action :70 selected: 2\n",
      "Action :71 selected: 2\n",
      "Action :72 selected: 2\n",
      "Action :73 selected: 1\n",
      "Action :74 selected: 1\n",
      "Action :75 selected: 0\n",
      "Action :76 selected: 0\n",
      "Action :77 selected: 1\n",
      "Action :78 selected: 2\n",
      "Action :79 selected: 1\n",
      "Action :80 selected: 2\n",
      "Action :81 selected: 0\n",
      "Action :82 selected: 2\n",
      "Action :83 selected: 1\n",
      "Action :84 selected: 2\n",
      "Action :85 selected: 2\n",
      "Action :86 selected: 0\n",
      "Action :87 selected: 1\n",
      "Action :88 selected: 0\n",
      "Action :89 selected: 2\n",
      "Action :90 selected: 1\n",
      "Action :91 selected: 0\n",
      "Action :92 selected: 0\n",
      "Action :93 selected: 0\n",
      "Action :94 selected: 2\n",
      "Action :95 selected: 1\n",
      "Action :96 selected: 1\n",
      "Action :97 selected: 1\n",
      "Action :98 selected: 1\n",
      "Action :99 selected: 2\n",
      "Action :100 selected: 0\n",
      "Action :101 selected: 2\n",
      "Action :102 selected: 1\n",
      "Action :103 selected: 2\n",
      "Action :104 selected: 1\n",
      "Action :105 selected: 0\n",
      "Action :106 selected: 0\n",
      "Action :107 selected: 0\n",
      "Action :108 selected: 0\n",
      "Action :109 selected: 1\n",
      "Action :110 selected: 1\n",
      "Action :111 selected: 2\n",
      "Action :112 selected: 2\n",
      "Action :113 selected: 2\n",
      "Action :114 selected: 1\n",
      "Action :115 selected: 0\n",
      "Action :116 selected: 2\n",
      "Action :117 selected: 2\n",
      "Action :118 selected: 2\n",
      "Action :119 selected: 1\n",
      "Action :120 selected: 0\n",
      "Action :121 selected: 0\n",
      "Action :122 selected: 1\n",
      "Action :123 selected: 1\n",
      "Action :124 selected: 2\n",
      "Action :125 selected: 0\n",
      "Action :126 selected: 2\n",
      "Action :127 selected: 2\n",
      "Action :128 selected: 2\n",
      "Action :129 selected: 0\n",
      "Action :130 selected: 0\n",
      "Action :131 selected: 0\n",
      "Action :132 selected: 0\n",
      "Action :133 selected: 2\n",
      "Action :134 selected: 1\n",
      "Action :135 selected: 1\n",
      "Action :136 selected: 1\n",
      "Action :137 selected: 0\n",
      "Action :138 selected: 0\n",
      "Action :139 selected: 2\n",
      "Action :140 selected: 2\n",
      "Action :141 selected: 2\n",
      "Action :142 selected: 0\n",
      "Action :143 selected: 2\n",
      "Action :144 selected: 1\n",
      "Action :145 selected: 1\n",
      "Action :146 selected: 0\n",
      "Action :147 selected: 0\n",
      "Action :148 selected: 0\n",
      "Action :149 selected: 0\n",
      "Action :150 selected: 2\n",
      "Action :151 selected: 2\n",
      "Action :152 selected: 1\n",
      "Action :153 selected: 1\n",
      "Action :154 selected: 2\n",
      "Action :155 selected: 0\n",
      "Action :156 selected: 1\n",
      "Action :157 selected: 1\n",
      "Action :158 selected: 2\n",
      "Action :159 selected: 0\n",
      "Action :160 selected: 1\n",
      "Action :161 selected: 1\n",
      "Action :162 selected: 1\n",
      "Action :163 selected: 2\n",
      "Action :164 selected: 1\n",
      "Action :165 selected: 1\n",
      "Action :166 selected: 2\n",
      "Action :167 selected: 1\n",
      "Action :168 selected: 0\n",
      "Action :169 selected: 2\n",
      "Action :170 selected: 0\n",
      "Action :171 selected: 2\n",
      "Action :172 selected: 0\n",
      "Action :173 selected: 1\n",
      "Action :174 selected: 2\n",
      "Action :175 selected: 1\n",
      "Action :176 selected: 2\n",
      "Action :177 selected: 2\n",
      "Action :178 selected: 0\n",
      "Action :179 selected: 0\n",
      "Action :180 selected: 1\n",
      "Action :181 selected: 2\n",
      "Action :182 selected: 2\n",
      "Action :183 selected: 0\n",
      "Action :184 selected: 0\n",
      "Action :185 selected: 0\n",
      "Action :186 selected: 0\n",
      "Action :187 selected: 2\n",
      "Action :188 selected: 0\n",
      "Action :189 selected: 1\n",
      "Action :190 selected: 1\n",
      "Action :191 selected: 2\n",
      "Action :192 selected: 1\n",
      "Action :193 selected: 2\n",
      "Action :194 selected: 2\n",
      "Action :195 selected: 1\n",
      "Action :196 selected: 0\n",
      "Action :197 selected: 2\n",
      "Action :198 selected: 1\n",
      "Action :199 selected: 2\n",
      "Episode 1 , Reward: -200.0\n",
      "Episode: 2 starts\n",
      "Action :0 selected: 0\n",
      "Action :1 selected: 1\n",
      "Action :2 selected: 2\n",
      "Action :3 selected: 0\n",
      "Action :4 selected: 0\n",
      "Action :5 selected: 0\n",
      "Action :6 selected: 0\n",
      "Action :7 selected: 1\n",
      "Action :8 selected: 0\n",
      "Action :9 selected: 1\n",
      "Action :10 selected: 0\n",
      "Action :11 selected: 2\n",
      "Action :12 selected: 1\n",
      "Action :13 selected: 2\n",
      "Action :14 selected: 2\n",
      "Action :15 selected: 1\n",
      "Action :16 selected: 0\n",
      "Action :17 selected: 0\n",
      "Action :18 selected: 1\n",
      "Action :19 selected: 0\n",
      "Action :20 selected: 2\n",
      "Action :21 selected: 0\n",
      "Action :22 selected: 0\n",
      "Action :23 selected: 1\n",
      "Action :24 selected: 1\n",
      "Action :25 selected: 2\n",
      "Action :26 selected: 0\n",
      "Action :27 selected: 2\n",
      "Action :28 selected: 2\n",
      "Action :29 selected: 0\n",
      "Action :30 selected: 2\n",
      "Action :31 selected: 2\n",
      "Action :32 selected: 1\n",
      "Action :33 selected: 1\n",
      "Action :34 selected: 1\n",
      "Action :35 selected: 0\n",
      "Action :36 selected: 2\n",
      "Action :37 selected: 0\n",
      "Action :38 selected: 1\n",
      "Action :39 selected: 2\n",
      "Action :40 selected: 2\n",
      "Action :41 selected: 0\n",
      "Action :42 selected: 0\n",
      "Action :43 selected: 2\n",
      "Action :44 selected: 1\n",
      "Action :45 selected: 0\n",
      "Action :46 selected: 0\n",
      "Action :47 selected: 0\n",
      "Action :48 selected: 1\n",
      "Action :49 selected: 2\n",
      "Action :50 selected: 2\n",
      "Action :51 selected: 0\n",
      "Action :52 selected: 2\n",
      "Action :53 selected: 0\n",
      "Action :54 selected: 1\n",
      "Action :55 selected: 2\n",
      "Action :56 selected: 2\n",
      "Action :57 selected: 2\n",
      "Action :58 selected: 0\n",
      "Action :59 selected: 0\n",
      "Action :60 selected: 2\n",
      "Action :61 selected: 1\n",
      "Action :62 selected: 1\n",
      "Action :63 selected: 2\n",
      "Action :64 selected: 0\n",
      "Action :65 selected: 1\n",
      "Action :66 selected: 2\n",
      "Action :67 selected: 2\n",
      "Action :68 selected: 1\n",
      "Action :69 selected: 1\n",
      "Action :70 selected: 1\n",
      "Action :71 selected: 0\n",
      "Action :72 selected: 2\n",
      "Action :73 selected: 0\n",
      "Action :74 selected: 0\n",
      "Action :75 selected: 2\n",
      "Action :76 selected: 1\n",
      "Action :77 selected: 2\n",
      "Action :78 selected: 2\n",
      "Action :79 selected: 2\n",
      "Action :80 selected: 0\n",
      "Action :81 selected: 2\n",
      "Action :82 selected: 0\n",
      "Action :83 selected: 2\n",
      "Action :84 selected: 2\n",
      "Action :85 selected: 0\n",
      "Action :86 selected: 0\n",
      "Action :87 selected: 2\n",
      "Action :88 selected: 1\n",
      "Action :89 selected: 0\n",
      "Action :90 selected: 0\n",
      "Action :91 selected: 2\n",
      "Action :92 selected: 1\n",
      "Action :93 selected: 0\n",
      "Action :94 selected: 0\n",
      "Action :95 selected: 2\n",
      "Action :96 selected: 2\n",
      "Action :97 selected: 0\n",
      "Action :98 selected: 2\n",
      "Action :99 selected: 2\n",
      "Action :100 selected: 0\n",
      "Action :101 selected: 1\n",
      "Action :102 selected: 0\n",
      "Action :103 selected: 1\n",
      "Action :104 selected: 2\n",
      "Action :105 selected: 2\n",
      "Action :106 selected: 0\n",
      "Action :107 selected: 1\n",
      "Action :108 selected: 2\n",
      "Action :109 selected: 0\n",
      "Action :110 selected: 0\n",
      "Action :111 selected: 0\n",
      "Action :112 selected: 1\n",
      "Action :113 selected: 0\n",
      "Action :114 selected: 1\n",
      "Action :115 selected: 1\n",
      "Action :116 selected: 1\n",
      "Action :117 selected: 0\n",
      "Action :118 selected: 0\n",
      "Action :119 selected: 0\n",
      "Action :120 selected: 0\n",
      "Action :121 selected: 1\n",
      "Action :122 selected: 1\n",
      "Action :123 selected: 2\n",
      "Action :124 selected: 0\n",
      "Action :125 selected: 1\n",
      "Action :126 selected: 1\n",
      "Action :127 selected: 0\n",
      "Action :128 selected: 2\n",
      "Action :129 selected: 2\n",
      "Action :130 selected: 2\n",
      "Action :131 selected: 1\n",
      "Action :132 selected: 1\n",
      "Action :133 selected: 1\n",
      "Action :134 selected: 1\n",
      "Action :135 selected: 2\n",
      "Action :136 selected: 0\n",
      "Action :137 selected: 2\n",
      "Action :138 selected: 1\n",
      "Action :139 selected: 0\n",
      "Action :140 selected: 2\n",
      "Action :141 selected: 0\n",
      "Action :142 selected: 2\n",
      "Action :143 selected: 2\n",
      "Action :144 selected: 1\n",
      "Action :145 selected: 0\n",
      "Action :146 selected: 2\n",
      "Action :147 selected: 2\n",
      "Action :148 selected: 1\n",
      "Action :149 selected: 1\n",
      "Action :150 selected: 2\n",
      "Action :151 selected: 0\n",
      "Action :152 selected: 0\n",
      "Action :153 selected: 2\n",
      "Action :154 selected: 2\n",
      "Action :155 selected: 1\n",
      "Action :156 selected: 1\n",
      "Action :157 selected: 2\n",
      "Action :158 selected: 0\n",
      "Action :159 selected: 2\n",
      "Action :160 selected: 1\n",
      "Action :161 selected: 2\n",
      "Action :162 selected: 2\n",
      "Action :163 selected: 0\n",
      "Action :164 selected: 0\n",
      "Action :165 selected: 1\n",
      "Action :166 selected: 2\n",
      "Action :167 selected: 2\n",
      "Action :168 selected: 1\n",
      "Action :169 selected: 1\n",
      "Action :170 selected: 2\n",
      "Action :171 selected: 1\n",
      "Action :172 selected: 1\n",
      "Action :173 selected: 1\n",
      "Action :174 selected: 1\n",
      "Action :175 selected: 2\n",
      "Action :176 selected: 2\n",
      "Action :177 selected: 1\n",
      "Action :178 selected: 2\n",
      "Action :179 selected: 0\n",
      "Action :180 selected: 2\n",
      "Action :181 selected: 2\n",
      "Action :182 selected: 1\n",
      "Action :183 selected: 2\n",
      "Action :184 selected: 0\n",
      "Action :185 selected: 1\n",
      "Action :186 selected: 1\n",
      "Action :187 selected: 1\n",
      "Action :188 selected: 1\n",
      "Action :189 selected: 1\n",
      "Action :190 selected: 1\n",
      "Action :191 selected: 2\n",
      "Action :192 selected: 0\n",
      "Action :193 selected: 0\n",
      "Action :194 selected: 2\n",
      "Action :195 selected: 2\n",
      "Action :196 selected: 2\n",
      "Action :197 selected: 0\n",
      "Action :198 selected: 1\n",
      "Action :199 selected: 0\n",
      "Episode 2 , Reward: -200.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-200., -200.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.train(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8305f7f-c98e-4850-aa45-7964b56f45c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c58ade7f-b209-47a7-b3fd-e9c9f3ace13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "336"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(A.W>-1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "62a5dea6-f48b-4d86-b947-62407d7b4b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab1 = np.array([0, -0.00069564])\n",
    "tab2 = np.array([-0.54452115, -0.00069564])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2905410a-7afb-4657-94f2-481338a49407",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.update(tab1, 2, tab2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee86be-3541-4ff2-8663-bf2106d92f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.step(1)\n",
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bd389a-7f50-449b-85aa-00527071c7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a59d81-bb3c-4b7d-8a07-62b8c25dcbd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4281b95b-a34f-44da-ba31-1feb3200ff53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
